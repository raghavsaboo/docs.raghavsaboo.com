{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":"<p>Welcome to my digital brain dump! \ud83e\udde0\ud83d\udca1 Dive into evolving notes on various topics\u2014it's a dynamic space where knowledge is always in motion. Feel free to contribute and learn with me! \ud83d\ude80</p>"},{"location":"Non-Technical/Areas%20of%20Interest.html","title":"Areas of Interest","text":"<ul> <li>Story Telling</li> <li>Writing</li> <li>Spoken Delivery</li> <li>Psychology</li> <li>Business</li> <li>History</li> <li>Movies</li> </ul>"},{"location":"Non-Technical/Business/Books/The%20Personal%20MBA%20by%20Josh%20Kaufman.html","title":"The Personal MBA","text":"<p>Link: The Personal MBA</p>"},{"location":"Non-Technical/Business/Books/The%20Personal%20MBA%20by%20Josh%20Kaufman.html#value-creation","title":"Value Creation","text":"<p>The key to value creation is to link a repeatable process that combines</p> <ol> <li>Value Creation - actually discovering and building something that people want</li> <li>Marketing - driving attention to it</li> <li>Sales - convert interest into paying customers</li> <li>Value Delivery - ensure customers are satisfied with what you promised</li> <li>Finance - sustain the process throug profit</li> </ol> <p>with the needs of the market which can fall into one of the following human drives</p> <ol> <li>The Drive to Acquire</li> <li>The Drive to Bond</li> <li>The Drive to Learn</li> <li>The Drive to Defend</li> <li>The Drive to Feel</li> </ol>"},{"location":"Non-Technical/Mental%20Models/reference.html","title":"Reference","text":"<ul> <li>Model Thinkers</li> </ul>"},{"location":"Non-Technical/Psychology/Books/Alchemy%3A%20The%20Dark%20Art%20and%20Curious%20Science%20of%20Creating%20Magin%20in%20Brands%2C%20Business%2C%20and%20Life.html","title":"Alchemy by Rory Sutherland","text":"<p>Link: Good Reads</p>"},{"location":"Non-Technical/Psychology/Books/Alchemy%3A%20The%20Dark%20Art%20and%20Curious%20Science%20of%20Creating%20Magin%20in%20Brands%2C%20Business%2C%20and%20Life.html#overview","title":"Overview","text":"<p>The main premise of the book is that while science and logic has resulted in immensely valuable products, there is a treasure trove still waiting to be discovered by applying apparently illogical solutions to human problems.</p> <ul> <li>Just because the scientific methodology has been so reliable it does not have to be the only tool to apply especially   to a messy field like human behaviour.</li> <li>\"Engineering does not allow for magic. Psychology does.\"</li> <li>Rory defines the way humans make decisions as \"psycho-logic\" - to distinguish it from the concepts of \"logic\" and \"   rationality\"</li> <li>The idea is to become adept at spotting instances where the \"universal laws\" don't apply - when abandoning logic is   sensible</li> </ul> <p>A 4x4 matrix that can be plotted of human advances / ideas that fall on a spectrum between FAILS to WORKS and SEEMS WEIRD to MAKES SENSE.</p> <p>e.g. Bicycles definitely work but are at the same time very weird - humans have learned how to ride bicycles but it is not built in a \"logical\" manner.</p>"},{"location":"Non-Technical/Psychology/Books/Alchemy%3A%20The%20Dark%20Art%20and%20Curious%20Science%20of%20Creating%20Magin%20in%20Brands%2C%20Business%2C%20and%20Life.html#signalling","title":"Signalling","text":""},{"location":"Non-Technical/Psychology/Books/Alchemy%3A%20The%20Dark%20Art%20and%20Curious%20Science%20of%20Creating%20Magin%20in%20Brands%2C%20Business%2C%20and%20Life.html#subconcious-hacking","title":"Subconcious Hacking","text":""},{"location":"Non-Technical/Psychology/Books/Alchemy%3A%20The%20Dark%20Art%20and%20Curious%20Science%20of%20Creating%20Magin%20in%20Brands%2C%20Business%2C%20and%20Life.html#satisficing","title":"Satisficing","text":""},{"location":"Non-Technical/Psychology/Books/Alchemy%3A%20The%20Dark%20Art%20and%20Curious%20Science%20of%20Creating%20Magin%20in%20Brands%2C%20Business%2C%20and%20Life.html#psychophysics","title":"Psychophysics","text":""},{"location":"Non-Technical/Story%20Telling/Books/Storyworthy%20by%20Matthew%20Dicks.html","title":"Storyworthy by Matthew Dicks","text":"<p>Link: Matthew Dicks Official Site</p>"},{"location":"Non-Technical/Story%20Telling/Books/Storyworthy%20by%20Matthew%20Dicks.html#how-to-brainstorm-story-ideas","title":"How to Brainstorm Story Ideas","text":"<ul> <li> <p>Spend 5 minutes each day to write 2-3 sentences - not the entire story - of moments from the day</p> </li> <li> <p>Don't worry of connecting threads of stories from previous days</p> </li> <li> </li> <li> <p>Spend 10 minutes to allow yourself to write your stream of conciousness/thoughts</p> </li> <li>Don't let your pen from stopping - let new ideas keep crashing in without hesitation or judgement</li> <li> <p>Be willing to leave a good idea behind in favour of a new one - even if the new idea is bad</p> </li> <li> </li> <li> <p>A story against each of these prompts e.g. <code>First/Best/Last/Worst Car</code></p> </li> <li>Annotate any as potential story or anecdote</li> <li>But not all listed will be story worthy</li> </ul>"},{"location":"Non-Technical/Story%20Telling/Books/Storyworthy%20by%20Matthew%20Dicks.html#homework-for-life","title":"Homework for Life","text":""},{"location":"Non-Technical/Story%20Telling/Books/Storyworthy%20by%20Matthew%20Dicks.html#crash-burn","title":"Crash &amp; Burn","text":""},{"location":"Non-Technical/Story%20Telling/Books/Storyworthy%20by%20Matthew%20Dicks.html#first-best-last-worst","title":"First, Best, Last, Worst","text":""},{"location":"Technical/Areas%20of%20Interest.html","title":"Areas of Interest","text":"<p>Below are the key areas of technical notes</p> <ul> <li>Algorithms</li> <li>Data Structures</li> <li>Statistics</li> <li>Linear Algebra</li> <li>Calculus</li> <li>Machine Learning</li> <li>Distributed Systems Design</li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/analysis.html","title":"Algorithmic Analysis","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/analysis.html#key-metrics","title":"Key Metrics","text":"<ul> <li>Time Complexity: Measures the time taken by an algorithm to execute as a function of input size.</li> <li>Space Complexity: Measures the amount of memory required by an algorithm as a function of input size.</li> <li>Scalability: Determines how an algorithm's performance scales with increasing input size.</li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/analysis.html#techniques","title":"Techniques","text":"<ul> <li>Big O Notation (\\(O\\)): Provides an upper bound on the growth rate of an algorithm's resource usage.</li> <li>Big Omega Notation (\\(\\Omega\\)): Provides a lower bound on the growth rate of an algorithm's resource usage.</li> <li>Big Theta Notation (\\(\\Theta\\)): Provides tight bounds on the growth rate of an algorithm's resource usage.</li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/analysis.html#common-asymptotic-functions","title":"Common Asymptotic Functions","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/analysis.html#o1-constant-time","title":"\\(O(1)\\) - Constant Time","text":"<ul> <li>Represents algorithms with constant time complexity, independent of input size.</li> <li>Examples include accessing an element in an array or performing basic arithmetic operations.</li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/analysis.html#olog-n-logarithmic-time","title":"\\(O(\\log n)\\) - Logarithmic Time","text":"<ul> <li>Represents algorithms whose time complexity grows logarithmically with the input size.</li> <li>Examples include binary search and certain tree operations like finding elements in balanced binary search trees.</li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/analysis.html#on-linear-time","title":"\\(O(n)\\) - Linear Time","text":"<ul> <li>Represents algorithms whose time complexity grows linearly with the input size.</li> <li>Examples include linear search, traversing an array, and simple array manipulations.</li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/analysis.html#on-log-n-linearithmic-time","title":"\\(O(n \\log n)\\) - Linearithmic Time","text":"<ul> <li>Represents algorithms whose time complexity grows log-linearly with the input size.</li> <li>Examples include efficient sorting algorithms like merge sort, heap sort, and quicksort.</li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/analysis.html#on2-quadratic-time","title":"\\(O(n^2)\\) - Quadratic Time","text":"<ul> <li>Represents algorithms whose time complexity grows quadratically with the input size.</li> <li>Examples include bubble sort, selection sort, and certain matrix operations like matrix multiplication.</li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/analysis.html#o2n-exponential-time","title":"\\(O(2^n)\\) - Exponential Time","text":"<ul> <li>Represents algorithms whose time complexity grows exponentially with the input size.</li> <li>Examples include certain recursive algorithms like generating all subsets of a set.</li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/analysis.html#notations","title":"Notations","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/analysis.html#big-o-notation-o","title":"Big O Notation (\\(O\\))","text":"<ul> <li>Represents the upper bound or worst-case scenario of an algorithm's time or space complexity.</li> <li>Example: \\(O(n)\\) denotes linear time complexity, indicating that the algorithm's runtime grows linearly with the input size.</li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/analysis.html#big-omega-notation-omega","title":"Big Omega Notation (\\(\\Omega\\))","text":"<ul> <li>Represents the lower bound or best-case scenario of an algorithm's time or space complexity.</li> <li>Example: \\(\\Omega(1)\\) denotes constant time complexity, indicating that the algorithm's runtime is constant regardless of input size.</li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/analysis.html#big-theta-notation-theta","title":"Big Theta Notation (\\(\\Theta\\))","text":"<ul> <li>Represents tight bounds on an algorithm's time or space complexity, providing both upper and lower bounds.</li> <li>Example: \\(\\Theta(n^2)\\) denotes quadratic time complexity, indicating that the algorithm's runtime grows quadratically with the input size.</li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/convex%20hull.html","title":"Convex Hull","text":"<p>TODO: Add notes here - leaving blank for now.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/graph%20traversals.html","title":"Graph Traversals","text":"<p>TODO: Add notes here - leaving blank for now.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/graph%20traversals.html#depth-first-search","title":"Depth First Search","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/graph%20traversals.html#breadth-first-search","title":"Breadth First Search","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/graph%20traversals.html#transitive-closure","title":"Transitive Closure","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/graph%20traversals.html#topological-sorting","title":"Topological Sorting","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/graph%20traversals.html#shortest-paths","title":"Shortest Paths","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/graph%20traversals.html#weighted-paths","title":"Weighted Paths","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/graph%20traversals.html#dijkstras","title":"Dijkstra's","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/graph%20traversals.html#bellman-ford","title":"Bellman-Ford","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/graph%20traversals.html#floyd-warshall","title":"Floyd-Warshall","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/greedy.html","title":"Greedy Methods","text":"<p>Usually used to solve optimization problems, where we are asked to optimize some value and calculate it, or see if the optimal path is possible. Refer to Optimization Patter for examples.</p> <p>Some key words: <code>min/max, longest/shortest, largest/smallest</code> etc.</p> <p>Some example problems: 1. Return the minimum number of swaps required to make s1 and s2 equal, or return -1 if it is impossible to do so. 2. Return the minimum number of steps to make s and t anagrams of each other. 3. Given three integers a, b, and c, return the longest possible happy string, if there are multiple happy strings, return any of them, if there is no such string return empty string.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/greedy.html#advantages","title":"Advantages","text":"<ul> <li>Simple to code</li> <li>Doesn't use complex algorithms (BFS or DFS for example)</li> <li>runtime is fast, usually <code>O(n)</code> or <code>O(nlogn)</code></li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/greedy.html#template-to-solve-using-greedy-method","title":"Template to solve using greedy method","text":"<ul> <li>Greedy algorithms follow this format:</li> <li>identify what choices you have at every step<ul> <li>find the best choice and perform it</li> <li>just do what looks best until done</li> </ul> </li> <li>assume best choice at every step leads to final best answer</li> </ul> <p>While you can use a formal math proof to check if greedy approach works, you can use intuition in practical settings (like an interview).</p> <p>Steps to build the intuition of whether greedy would work:</p> <ol> <li>Identify the different steps and choices of a problem: a) choices have to be comparable, and b) must determine if the choice is better than another</li> <li>Check if greedy works: a) try finding small cases where greedy isn't optimal b) if cases pass, try to figure out why greedy is optimal </li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/greedy.html#maximum-number-of-non-overlapping-intervals-on-an-axis","title":"Maximum Number of Non-Overlapping Intervals on an Axis","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/greedy.html#fractional-knapsack","title":"Fractional Knapsack","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/recursion.html","title":"Recursion","text":"<p>Recursion is another way to achieve repetition apart from loops. In recursion a function makes one or more calls to itself during execution, or by which a data structure relies on smaller instances of the same type of structure in its representation.</p> <p>A recursive function has some basic properties:</p> <ol> <li>it contains one or more base cases which are defined non-recursively in terms of fixed quantities</li> <li>it contains one or more recursive cases which are defined by appealing to the definition of the function being defined</li> </ol> <pre><code>def factorial(n):\n\n    # base case\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n</code></pre> <p>In Python, each time a function (recursive or otherwise) is called, a structure known as an activation record or frame is created to store information about the progress of that invocation of the function. This activation record includes a namespace for storing the function call's parameters and local variables, and information about which command in the body of the function is currently executing.</p> <p>When the execution of a function leads to a nested function call, the execution of the former call is suspended and its activation record stores the place in the source code at which the flow of control should continue upon return of the nested call. This process is used both in the standard case of one function calling a different function, or in the recursive case in which a function invokes itself. The key point is that there is a different activation record for each active cell.</p> <pre><code>import os\n\ndef disk_usage(path):\n    \"\"\"\n    Return the number of bytes used by a file/folder and any descendents.\n    \"\"\"\n    total = os.path.getsize(path)\n\n    if os.path.isdir(path):\n        for filename in os.listdir(path):\n            childpath = os.path.join(path, filename)\n            total += disk.usage(childpath)\n\n        print(f'{total}', path)\n        return total\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/recursion.html#maximum-recursive-depth-in-python","title":"Maximum Recursive Depth in Python","text":"<p>If each recursive call makes another recursive call, without ever reaching a base case, then we have an infinite series of such calls.</p> <p>This is called infinite recursion and it is a fatal error as it can quickly swamp computing resources, not only due to rapid use of the CPU, but because each successive call creates an activation record requiring additional memory.</p> <p>To avoid this, we should always ensure that each recursive call is in some way progressing toward a base case. Also in Python there is an intentional design decision to limit the overall number of function activations that can be simultaneously active at ~ 1,000.</p> <p>In Python this can be dynamically reconfigured:</p> <pre><code>import sys\n\nold = sys.getrecursionlimit()\nsys.setrecursionlimit(100000)\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/recursion.html#other-examples-of-recursion","title":"Other Examples of Recursion","text":"<ul> <li>If a recursive call starts at most one other, we call this a linear recursion</li> <li>If a recursive call may start two others, we all this a binary recursion</li> <li>If a recursive call may start three or more ohers, this is multiple recursion</li> </ul> <p>Note that the linear recursion terminology here reflects the structure of the recursion trace, and not the time complexity.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/recursion.html#linear-recursion-examples","title":"Linear Recursion Examples","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/recursion.html#linear-sum","title":"Linear Sum","text":"<p>Calculating the sum of a sequence of numbers by adding the last number to the sum of the first <code>n-1</code>.</p> <pre><code>def linear_sum(S, n):\n    \"\"\"Return the sum of the first n numbers of sequence S.\n    \"\"\"\n    if n == 0:\n        return 0\n    else:\n        return linear_sum(S, n-1) + S[n-1]\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/recursion.html#reversing-a-sequence","title":"Reversing a Sequence","text":"<p>Reversal of a sequence by swapping the first and last elements, and then recursively reversing the remaining elements.</p> <pre><code>def reverse(S, start, stop):\n    \"\"\"Reverse elements in implicit slice S[start:stop]\n    \"\"\"\n    if start &lt; stop - 1:\n        S[start], S[stop-1] = S[stop-1], S[start]\n        reverse(S, start+1, stop-1)\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/recursion.html#computing-powers","title":"Computing Powers","text":"<p>Raising a number \\(x\\) to an arbitrary non-negaive integer, \\(n\\) - i.e. compute the power function, defined as \\(\\text{power}(x,n)=x^n\\).</p> <p>A trivial definition would follow from the fact that \\(x^n=x \\cdot x^{n-1}\\)</p> \\[ \\text{power}(x,n)=     \\begin{cases}     1 &amp; \\text{if n=0} \\\\     x \\cdot \\text{power}(x, n-1) &amp; \\text{otherwise}     \\end{cases} \\] <p>This is <code>O(n)</code></p> <p>A much faster way to compute the power function is the squaring technique. Let \\(k=\\lfloor \\frac{n}{2} \\rfloor\\) denote the floor of the division. When \\(n\\) is even, \\(k=\\lfloor \\frac{n}{2} \\rfloor=\\frac{n}{2}\\) and therefore \\((x^k)^2=(x^{\\frac{n}{2}})^2=x^n\\). When \\(n\\) is odd \\(k=\\lfloor \\frac{n}{2} \\rfloor=\\frac{n-1}{2}\\) and therefore \\((x^k)^2=(x^{\\frac{n-1}{2}})^2=x^{n-1}\\).</p> \\[ \\text{power}(x,n)=     \\begin{cases}     1 &amp; \\text{if n=0} \\\\     x \\cdot (\\text{power}(x, \\lfloor \\frac{n}{2} \\rfloor))^2 &amp; \\text{if n&gt;0 is odd} \\\\     \\text{power}(x, \\lfloor \\frac{n}{2} \\rfloor)^2 &amp; \\text{if n&gt;0 is even} \\\\     \\end{cases} \\] <p>This is <code>O(log n)</code></p> <pre><code>def power(x, n):\n    \"\"\"Compute the value x**n for integer n.\"\"\"\n    if n == 0:\n        return 1\n    else:\n        return x*power(x, n-1)\n\ndef faster_power(x, n):\n    \"\"\"Compute the value x**n for integer n.\"\"\"\n    if n == 0:\n        return 1\n    else:\n        partial = power(x, n//2)\n        result = partial * partial\n        if n % 2 == 1:\n            result *= x\n        return result\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/recursion.html#binary-recursion","title":"Binary Recursion","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/recursion.html#binary-sum","title":"Binary Sum","text":"<p>Summing the \\(n\\) elements of a sequence, \\(S\\), of numbers by computing the sum of the first half and the sum of the second half, and then adding them together.</p> <pre><code>def binary_sum(S, start, stop):\n    \"\"\"Return the sum of the numbers in implicit slic S[start:stop]\"\"\"\n    if start &gt;= stop:\n        return 0\n    elif start == stop - 1:\n        return S[start]\n    else:\n        mid = (start + stop) // 2\n        return binary_sum(S, start, mid) + binary_sum(S, mid, stop)\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/recursion.html#multiple-recursion","title":"Multiple Recursion","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/recursion.html#summation-puzzle","title":"Summation Puzzle","text":"<p>A common example is when we want to enumerate various configurations in order to solve a combinatorial puzzle e.g. a summation puzzle.</p> \\[ \\begin{split} pot + pan = bib \\\\ dog + cat = pig \\\\ boy + girl = baby \\end{split} \\] <p>Here we need to assign a unique digit (0...9) to each letter in the equation in order to make the equation true. So we can use the computer to enumerate all possibilities and test each one.</p> <p>The general algorithm for building sequences is:</p> <ol> <li>Recursively generate the sequences of <code>k - 1</code> elements</li> <li>Append to each such sequence an element not already contained in it</li> <li>Use a set <code>U</code> to keep track of the elements not contained in the current sequence, so that na element <code>e</code> is considered to not have been used yet if and only if <code>e</code> is in <code>U</code></li> </ol> <pre><code>puzzle_solve(k, S, U):\n    Input: An integer k, sequence S, and set U\n    Output: enumeration of all k-length extensions to S using elements in U without repetitions\n\n    for each e in U do\n        Add e to the end of S\n        Remove e from U\n        if k==1 then\n            Test whether S is a configuration that solves the puzzle\n            if S solves the puzzle then\n                return \"Solution found: \" S\n        else\n            puzzle_solve(k-1, S, U)\n        Remove r from the end of S\n        Add e back to U\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/recursion.html#designing-recursive-algorithms","title":"Designing Recursive Algorithms","text":"<p>Think of the different ways to define subproblems that have the same general structure as the original problem.</p> <p>Add necessary parameterization to the function (e.g. passing beginning and end of sub-array).</p> <p>Test for base cases - there should at least be one, and defined so that every possible chain of recursive calls eventually reaches a base case.</p> <p>Recur - for non-base cases, perform one or more recursive calls.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/recursion.html#eliminating-tail-recursion","title":"Eliminating Tail Recursion","text":"<p>The usefulness of recursion comes at a modest cost - in particular the Python interpreter must maintain activation records that keep track of the state of each nested call.</p> <p>Some forms of recursion can be eliminated without the use of axillary memory (like stacks) - and a notable form is tail recursion.</p> <p>A recursion is a tail recursion if any recursive call is made from one context is the very last operation in that context, with the return value of the recursive call immediately returned by the enclosing recursion. In this can by necessity, a tail recursion must be a linear recursion.</p> <p>Examples are binary search and the sequence reversing algorithm.</p> <p>However the factorial function is not a tail recursion as it involves <code>return n * factorial(n-1)</code> which means an additional multiplication is performed after the recursive call.</p> <p>Tail recursions can be reimplemented non-recursively by enclosing the body in a loop for repetition, and replacing a recursive call with new parameters by reassignment of the existing parameters to those values.</p> <pre><code>def binary_search_iterative(data, target):\n    low = 0\n    high = len(data) - 1\n\n    while low &lt;= high:\n        mid = (low + high) // 2\n        if target == data[mid]:\n            return True\n\n        elif target &lt; data[mid]:\n            high = mid - 1\n\n        else:\n            low = mid + 1\n\n    return False\n\ndef reverse_iterative(S):\n\n    start, stop = 0, len(S)\n\n    while start &lt; stop - 1:\n        S[start], S[stop-1] = S[stop-1], S[start]\n        start, stop = start + 1, stop - 1\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/searching.html","title":"Searching","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/searching.html#binary-search","title":"Binary Search","text":"<p>A classic recursive algorithm, binary search, that is used to locate a target value within a sorted sequence of n elements.</p> <p>Time complexity: <code>O(log n)</code></p> <p>When the sequence is unsorted then we can use sequential search algorithms that run in O(n) time (i.e. linear time).</p> <pre><code>def binary_search(data, target, left, right):\n    \"\"\"\n    Return True if target is found in indicated portion of a Python list.\n\n    The search only considers the portion from data[left] to data[right] inclusive.\n    \"\"\"\n\n    if left &gt; right:\n        return False, None # interval is empty, no match\n    else:\n        mid = (left + right) // 2\n        if target == data[mid]: # found a match\n            return True, mid\n        elif target &lt; data[mid]:\n            # recur on the portion left of the middle\n            return binary_search(data, target, left, mid - 1)\n        else:\n            # recur on the portion right of the middle\n            return binary_search(data, target, mid + 1, right)\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/selection.html","title":"Selection","text":"<p>Here we are interested in identifying a single element in terms of its rank relative to the sorted order of the entire set. Examples are: minimum, and maximum elements, median element etc.</p> <p>In general, queries that ask for an element with a given rank are called order statistics.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/selection.html#general-selection-problem","title":"General Selection Problem","text":"<p>The general order-statistc problem of selecting the kth smallest element from an unsorted collection of <code>n</code> comparable elements is the selection problem. This can be solved by sorting the collection and then indexing into the sorted sequence at <code>k-1</code> but this would take <code>O(nlogn)</code>.</p> <p>However <code>O(n)</code> running time can be achieved (worst case <code>k=n</code>) including the interesting case of finding the median where <code>k=floor(n/2)</code>. </p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/selection.html#prune-and-search","title":"Prune and Search","text":"<p>To get <code>O(n)</code> running time for any value fo <code>k</code> we use a design pattern known as prune and search where on a collection of <code>n</code> objects we prune away a fraction of the <code>n</code> objects and recursively solve the smaller problem. When we have finally reduced the probnlem to one defined on a constant-sized collection of objercts, we then solve the problem using some brute-force method. </p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/selection.html#randomized-quick-select","title":"Randomized Quick Select","text":"<p>Runs in <code>O(n)</code> expected time taken over all possible random choices made by the algorithm. Worst case is <code>O(n^2)</code>. </p> <p>Suppose we are given an unordered sequence S of <code>n</code> comparable elements together with an integer <code>k</code> we find the <code>kth</code> smallest element by:</p> <ul> <li>picking a pivot element from S at random</li> <li>subdividing S into three subsequences L, E, and G, storing the elements of S less than, equal to, and great than the pivot respectively</li> <li>determine hich of these subsets contain the desired element based on the value of <code>k</code> and sizes of the subsets</li> <li>recure on the appropriate subset, noting that the desired elements rank in the subset may differ from its rank in the full set.</li> </ul> <pre><code>def quick_select(S, k):\n    \"\"\"Return the kth smallest element of list S&lt; for k from 1 to len(S)\"\"\"\n\n    if len(S) == 1:\n        return S[0]\n\n    pivot = random.choice(S)\n    L = [x for x in S if x &lt; pivot]\n    E = [x for x in S if x == pivot]\n    G = [x for x in S if x &gt; pivot]\n\n    if k &lt;= len(L):\n        return quick_select(L, k) # kth smallest lies in L\n    elif k &lt;= len(L) + len(E):\n        return pivot # kth smallest is equal to pivot\n    else:\n        j = k - len(L) - len(E) # new selection parameter\n        return quick_select(G, j) # kth smallest is jth in G\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/sorting.html","title":"Sorting Algorithms","text":"<p>Sorting algorithms can be stable or unstable. A stable sort algorithm preserves the relative order of records with equal keys in the sorted output as they appear in the unsorted output.</p> <p>On the other hand unstable sorting algorithms do not guarantee that any two or more objects with the same keys will appear in the same order before an after sorting.</p> Sorting Algorithm Best Time Complexity Average Time Complexity Worst Time Complexity Space Complexity Stable Sort Bubble Sort O(n) O(n^2) O(n^2) O(1) Yes Selection Sort O(n^2) O(n^2) O(n^2) O(1) No Insertion Sort O(n) O(n^2) O(n^2) O(1) Yes Merge Sort O(n log n) O(n log n) O(n log n) O(n) Yes Quick Sort O(n log n) O(n log n) O(n^2) O(log n) No Bucket Sort O(n+k) O(n+k) O(n^2) O(n) Yes Radix Sort O(nk) O(nk) O(nk) O(n+k) Yes Counting Sort O(n+k) O(n+k) O(n+k) O(k) Yes Heap Sort O(n log n) O(n log n) O(n log n) O(1) No Cycle Sort O(n^2) O(n^2) O(n^2) O(1) No <ul> <li><code>n</code> is the number of elements being sorted.</li> <li><code>k</code> is the range of the input (for non-comparison based sorts).</li> <li>Space complexity considers auxiliary space.</li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/sorting.html#bubble-sort","title":"Bubble Sort","text":"<p>Bubble Sort repeatedly compares and swaps adjacent elements if they are in the wrong order. - Complexity: Best: O(n), Average and Worst: O(n^2)</p> <pre><code>def bubble_sort(arr):\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n-i-1):\n            if arr[j] &gt; arr[j+1]:\n                arr[j], arr[j+1] = arr[j+1], arr[j]\n    return arr\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/sorting.html#selection-sort","title":"Selection Sort","text":"<p>Divides the input into two parts: sorted and unsorted.</p> <p>First the smallest element in the unsorted sublist is found, and placed at the end of the sorted sublist. This is repeated until the list is fully sorted.</p> <pre><code>def selection_sort(arr):\n    for i in range(len(arr)):\n        # Find the minimum element in the remaining unsorted array\n        min_idx = i # assume current is the index of minimum\n        for j in range(i+1, len(arr)):\n            if arr[j] &lt; arr[min_idx]:\n                min_idx = j\n        # Swap the found minimum element with the first element\n        arr[i], arr[min_idx] = arr[min_idx], arr[i]\n    return arr\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/sorting.html#insertion-sort","title":"Insertion Sort","text":"<p>Segments the list into sorted and unsorted parts like selection sort. It iterates over the unsorted sublist, and then inserts the element being viewed into the correct position of the sorted sublist.</p> <pre><code>def insertion_sort(arr):\n    # Traverse through 1 to len(arr)\n    for i in range(1, len(arr)):\n        key = arr[i]\n        # Move elements of arr[0..i-1], that are\n        # greater than key, to one position ahead\n        # of their current position\n        j = i-1\n        while j &gt;=0 and key &lt; arr[j]:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key\n    return arr\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/sorting.html#merge-sort","title":"Merge Sort","text":"<p>Merge Sort is a divide-and-conquer algorithm that divides the input array into two halves, calls itself for the two halves, and then merges the two sorted halves. The merge function is key to its performance, efficiently combining two sorted arrays into a single sorted array.</p> <pre><code>def merge(left, right):\n    result = []\n    i = j = 0\n\n    while i &lt; len(left) and j &lt; len(right):\n        if left[i] &lt; right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n\n    # Append remaining elements\n    result += left[i:]\n    result += right[j:]\n\n    return result\n\ndef merge_sort(arr):\n    if len(arr) &lt;= 1:\n        return arr\n\n    mid = len(arr) // 2\n    left = merge_sort(arr[:mid])\n    right = merge_sort(arr[mid:])\n\n    return merge(left, right)\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/sorting.html#quick-sort","title":"Quick Sort","text":"<p>Quicksort is a divide-and-conquer algorithm that sorts an array by selecting a 'pivot' element and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. The steps can be summarized as:</p> <ol> <li>Divide: Choose a pivot and partition the array such that elements less than the pivot are on the left, and elements greater than the pivot are on the right.</li> <li>Conquer: Recursively apply the same process to the sub-arrays.</li> <li>Combine: Since the algorithm sorts in place, no explicit merge step is needed like in merge sort.</li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/sorting.html#choice-of-pivot","title":"Choice of Pivot","text":"<p>The choice of the pivot can significantly affect the performance of Quicksort. Common strategies include:</p> <ul> <li>First Element: Choose the first element as the pivot.</li> <li>Last Element: Choose the last element as the pivot.</li> <li>Random Element: Choose a random element as the pivot to reduce the chance of worst-case complexity.</li> <li>Median: Ideally, choose the median of the array as the pivot, though this is computationally expensive to determine.</li> <li>Median of Three: Choose the median of the first, middle, and last elements as the pivot. This is a good compromise, offering better performance without significant overhead.</li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/sorting.html#hoare-partition-scheme","title":"Hoare Partition Scheme","text":"<p>Hoare's partitioning algorithm is an efficient way of partitioning an array into two parts around a pivot. It works by initializing two pointers (one at the start and one at the end of the array) and moving them towards each other until they find an inversion (a pair of elements where one is greater than the pivot and the other is smaller). These elements are then swapped.</p> <pre><code>def partition_hoare(arr, low, high):\n    pivot = arr[low]\n    i = low - 1\n    j = high + 1\n    while True:\n        i += 1\n        while arr[i] &lt; pivot:\n            i += 1\n        j -= 1\n        while arr[j] &gt; pivot:\n            j -= 1\n        if i &gt;= j:\n            return j\n        arr[i], arr[j] = arr[j], arr[i]\n\ndef quicksort_hoare(arr, low, high):\n    if low &lt; high:\n        p = hoare_partition(arr, low, high)\n        quicksort_hoare(arr, low, p)\n        quicksort_hoare(arr, p + 1, high)\n\ndef quicksort(arr):\n    quicksort_hoare(arr, 0, len(arr) - 1)\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/sorting.html#lomuto-partition-scheme","title":"Lomuto Partition Scheme","text":"<p>The Lomuto partition scheme is simpler but less efficient than Hoare's scheme. It works by choosing the last element as the pivot, then moving all elements smaller than the pivot to the left of it and all greater elements to the right.</p> <pre><code>def lomuto_partition(arr, low, high):\n    pivot = arr[high]\n    i = low - 1\n    for j in range(low, high):\n        if arr[j] &lt; pivot:\n            i += 1\n            arr[i], arr[j] = arr[j], arr[i]\n    arr[i + 1], arr[high] = arr[high], arr[i + 1]\n    return i + 1\n\ndef quicksort_lomuto(arr, low, high):\n    if low &lt; high:\n        p = lomuto_partition(arr, low, high)\n        quicksort_lomuto(arr, low, p - 1)\n        quicksort_lomuto(arr, p + 1, high)\n\ndef quicksort(arr):\n    quicksort_lomuto(arr, 0, len(arr) - 1)\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/sorting.html#bucket-sort","title":"Bucket Sort","text":"<p>Bucket Sort is a distribution sort that divides the input into several groups, called \"buckets,\" and then sorts these buckets individually using a suitable sorting algorithm or recursively applying Bucket Sort. </p> <p>It's especially useful for data that is uniformly distributed over a range - i.e. when the range of values is N.</p> <pre><code>def bucket_sort(arr):\n    # 1. Create empty buckets\n    bucket_count = 10\n    buckets = [[] for _ in range(bucket_count)]\n\n    # 2. Insert elements into their respective buckets\n    for i in arr:\n        index = int(bucket_count * i)\n        buckets[index].append(i)\n\n    # 3. Sort the elements of each bucket\n    for i in range(bucket_count):\n        buckets[i] = sorted(buckets[i])\n\n    # 4. Concatenate all buckets into arr\n    k = 0\n    for i in range(bucket_count):\n        for j in range(len(buckets[i])):\n            arr[k] = buckets[i][j]\n            k += 1\n    return arr\n\n# Example usage\narr = [0.42, 0.32, 0.23, 0.52, 0.25, 0.47, 0.51]\nprint(\"Original array:\", arr)\nsorted_arr = bucket_sort(arr)\nprint(\"Sorted array:\", sorted_arr)\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/sorting.html#counting-sort","title":"Counting Sort","text":"<p>Counting Sort is a non-comparative sorting algorithm suitable for sorting items with integer keys. It operates by counting the number of objects that have each distinct key value and using arithmetic on those counts to determine the positions of each key value in the output sequence.</p> <ul> <li>Non-Comparative: Unlike other sorting algorithms, Counting Sort does not compare the elements of the array to sort them.</li> <li>Stable Sort: Preserves the relative order of records with equal keys.</li> <li>Efficient for Small Range of Keys: Best suited when the range of potential items (k) is not significantly greater than the number of items (n).</li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/sorting.html#algorithm-steps","title":"Algorithm Steps","text":"<ol> <li>Count Occurrences: Count the occurrences of each unique element.</li> <li>Cumulative Count: Transform the count array to store the cumulative sum of counts.</li> <li>Place the Elements: Place each element in its correct position based on the cumulative count and decrease the count by one for each placement.</li> </ol> <pre><code>def counting_sort(arr):\n    max_val = max(arr)\n    m = max_val + 1\n    count = [0] * m\n\n    for a in arr:\n        count[a] += 1\n    i = 0\n    for a in range(m):\n        for c in range(count[a]):\n            arr[i] = a\n            i += 1\n    return arr\n\n# Example usage\narr = [4, 2, 2, 8, 3, 3, 1]\nsorted_arr = counting_sort(arr)\nprint(\"Sorted array:\", sorted_arr)\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/sorting.html#radix-sort","title":"Radix Sort","text":"<p>Radix Sort is a non-comparative integer sorting algorithm that sorts data with integer keys by grouping keys by the individual digits which share the same significant position and value. It processes integer keys from least significant digit to most significant digit.Radix Sort is a non-comparative integer sorting algorithm that sorts data with integer keys by grouping keys by the individual digits which share the same significant position and value. It processes integer keys from least significant digit to most significant digit.Radix Sort is a non-comparative integer sorting algorithm that sorts data with integer keys by grouping keys by the individual digits which share the same significant position and value. It processes integer keys from least significant digit to most significant digit.</p> <ul> <li>Non-Comparative: Does not compare elements directly.</li> <li>Stable Sort: Maintains the relative order of records with equal keys.</li> <li>Efficient for Large Keys: Performs well when the keys are large.</li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/sorting.html#algorithm-steps_1","title":"Algorithm Steps","text":"<ol> <li>Get the Maximum Number: To know the number of digits.</li> <li>Counting Sort for Each Digit: Apply counting sort to sort elements based on the current digit.<ul> <li>Repeat for each digit until the largest number.</li> </ul> </li> </ol> <p><pre><code>def counting_sort_for_radix(arr, exp):\n    n = len(arr)\n    output = [0] * n\n    count = [0] * 10\n\n    for i in range(n):\n        index = arr[i] // exp\n        count[index % 10] += 1\n\n    for i in range(1, 10):\n        count[i] += count[i - 1]\n\n    i = n - 1\n    while i &gt;= 0:\n        index = arr[i] // exp\n        output[count[index % 10] - 1] = arr[i]\n        count[index % 10] -= 1\n        i -= 1\n\n    for i in range(n):\n        arr[i] = output[i]\n\ndef radix_sort(arr):\n    max_val = max(arr)\n    exp = 1\n    while max_val // exp &gt; 0:\n        counting_sort_for_radix(arr, exp)\n        exp *= 10\n\n# Example usage\narr = [170, 45, 75, 90, 802, 24, 2, 66]\nradix_sort(arr)\nprint(\"Sorted array:\", arr)\n</code></pre> - Radix Sort uses counting sort as a subroutine to sort. - The efficiency of Radix Sort depends on the digit length of the numbers. - It's excellent for sorting numbers or strings of characters that can be represented as numbers.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/sorting.html#heap-sort","title":"Heap Sort","text":"<p>Heap Sort is a comparison-based sorting algorithm that builds a heap from the input data, then repeatedly extracts the maximum element from the heap and rebuilds the heap until all elements are sorted.</p> <ul> <li>In-Place Sorting: Though it operates on the array itself to sort it, a small constant extra space is used for the heap.</li> <li>Not Stable: Does not maintain the relative order of records with equal keys.</li> <li>Efficient for Large Data Sets: Offers good performance on large data sets.</li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/sorting.html#algorithm-steps_2","title":"Algorithm Steps","text":"<ol> <li>Build a Max Heap: Rearrange the array to form a max heap, where the largest element is at the root.</li> <li>Extract Elements from Heap: Repeatedly move the root of the heap to the end of the array, reduce the size of the heap by one, and then rebuild the heap.</li> <li>Repeat Until Sorted: Continue the extraction process until all elements are sorted.</li> </ol> <pre><code>def heapify(arr, n, i):\n    largest = i\n    l = 2 * i + 1\n    r = 2 * i + 2\n\n    # See if left child of root exists and is greater than root\n    if l &lt; n and arr[l] &gt; arr[largest]:\n        largest = l\n\n    # See if right child of root exists and is greater than the largest so far\n    if r &lt; n and arr[r] &gt; arr[largest]:\n        largest = r\n\n    # Change root, if needed\n    if largest != i:\n        arr[i], arr[largest] = arr[largest], arr[i]\n\n        # Heapify the root\n        heapify(arr, n, largest)\n\ndef heap_sort(arr):\n    n = len(arr)\n\n    # Build a maxheap\n    for i in range(n//2 - 1, -1, -1):\n        heapify(arr, n, i)\n\n    # Extract elements one by one\n    for i in range(n-1, 0, -1):\n        arr[i], arr[0] = arr[0], arr[i]  # swap\n        heapify(arr, i, 0)\n\n# Example usage\narr = [12, 11, 13, 5, 6, 7]\nheap_sort(arr)\nprint(\"Sorted array is:\", arr)\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/sorting.html#cycle-sort","title":"Cycle Sort","text":"<p>Cycle Sort is an in-place, comparison-based sorting algorithm that is theoretically optimal in terms of the total number of writes to the original array. It's particularly useful where memory write or swap operations are costly.</p> <ul> <li>In-Place Sorting: Does not require additional space for sorting, except for variable storage.</li> <li>Not Stable: May change the relative order of equal elements.</li> <li>Write-Efficient: Minimizes the number of memory writes, which is its main advantage.</li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/sorting.html#algorithm-steps_3","title":"Algorithm Steps","text":"<ol> <li>Find Cycles: Start from the first element and consider it as the starting point of a cycle. Then, find the correct position for this element by counting the number of elements that are smaller.</li> <li>Rotate Cycle: Once the correct position is found, place the element there and repeat the process for all elements to complete the cycle.</li> <li>Repeat: Continue until the end of the array is reached.</li> </ol> <pre><code>def cycle_sort(arr):\n    writes = 0\n    for cycle_start in range(0, len(arr) - 1):\n        item = arr[cycle_start]\n        pos = cycle_start\n        for i in range(cycle_start + 1, len(arr)):\n            if arr[i] &lt; item:\n                pos += 1\n        if pos == cycle_start:\n            continue\n        while item == arr[pos]:\n            pos += 1\n        arr[pos], item = item, arr[pos]\n        writes += 1\n        while pos != cycle_start:\n            pos = cycle_start\n            for i in range(cycle_start + 1, len(arr)):\n                if arr[i] &lt; item:\n                    pos += 1\n            while item == arr[pos]:\n                pos += 1\n            arr[pos], item = item, arr[pos]\n            writes += 1\n    return arr\n\n# Example usage\narr = [1, 8, 3, 9, 10, 10, 2, 4 ]\ncycle_sort(arr)\nprint(\"Sorted array is:\", arr)\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/sorting.html#external-memory-sorting","title":"External Memory Sorting:","text":"<p>External memory sorting is a technique used when the size of the input data exceeds the available memory capacity. In such scenarios, traditional in-memory sorting algorithms become impractical, requiring the use of disk-based sorting techniques to process large datasets efficiently.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/sorting.html#challenges-of-large-datasets","title":"Challenges of Large Datasets:","text":"<ol> <li> <p>Limited Memory: The primary challenge is the limited amount of available memory, which prevents loading the entire dataset into RAM for sorting.</p> </li> <li> <p>Disk I/O Overhead: Disk I/O operations are significantly slower than memory operations, leading to performance bottlenecks when reading and writing data from/to disk.</p> </li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/sorting.html#multiway-merge-sort","title":"Multiway Merge-Sort:","text":"<p>Multiway merge-sort is a disk-based sorting algorithm that efficiently sorts large datasets by dividing the input into smaller chunks that can fit into memory, sorting them in-memory, and then merging the sorted chunks to produce the final sorted output.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/sorting.html#algorithm-overview","title":"Algorithm Overview:","text":"<ol> <li> <p>Initial Partitioning: Divide the input dataset into multiple smaller chunks or blocks that can fit into memory.</p> </li> <li> <p>In-Memory Sorting: Load each block into memory and perform an in-memory sorting algorithm (e.g., quicksort or heapsort) to sort the data within each block.</p> </li> <li> <p>Multiway Merge: Merge the sorted blocks using a multiway merge algorithm, such as k-way merge, which merges k sorted sequences efficiently.</p> </li> <li> <p>Output Generation: As the merge proceeds, write the sorted output to disk.</p> </li> <li> <p>Final Merge: If the number of blocks exceeds the memory capacity, perform another merge pass until all blocks are merged into a single sorted output.</p> </li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/sorting.html#complexity","title":"Complexity:","text":"<ul> <li> <p>Time Complexity: The time complexity of multiway merge-sort depends on the number of passes required for merging and the efficiency of the in-memory sorting algorithm. Typically, it is dominated by the disk I/O operations and the number of merge passes needed. The total number of disk transfers \\(T_{\\text{disk}}\\) is often used as a measure of performance. For \\(N\\) elements and \\(M\\) available memory blocks, the total number of disk transfers can be approximated as \\(T_{\\text{disk}} \\approx 2N \\log_M N\\).</p> </li> <li> <p>Space Complexity: The space complexity is determined by the number of blocks that can be loaded into memory at once and the size of each block. It is often proportional to the memory capacity available.</p> </li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/text%20processing.html","title":"Text Processing and Pattern Matching","text":"<p>In classic pattern matching, we are given a text string <code>T</code> of length <code>n</code> and a pattern string <code>P</code> of length <code>m</code>, and want to find whether <code>P</code> is a substring of <code>T</code>. If so, we may want to find the lowest index <code>j</code> within <code>T</code> at which <code>P</code> begins, such that <code>T[j:j+m]</code> equals <code>P</code>, or perhaps find all indices of <code>T</code> at which pattern <code>P</code> begins. </p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/text%20processing.html#brute-force","title":"Brute Force","text":"<p>Time complexity: \\(O(nm)\\)</p> <p>Compare all substrings in <code>T</code> with <code>P</code>. </p> <pre><code>def find_brute(T, P):\n    \"\"\"Return lowst index of T at which substring P begins (or else -1)\"\"\"\n    n, m = len(T), len(P)                      # introduce convenient notations\n    for i in range(n-m+1):                     # try every potential starting index within T\n        k = 0                                    # an index into pattern P\n        while k &lt; m and T[i + k] == P[k]:        # kth character of P matches\n        k += 1\n        if k == m:                               # if we reached the end of pattern,\n        return i                               # substring T[i:i+m] matches P\n    return -1                                  # failed to find a match starting with any i\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/text%20processing.html#boyer-moore-algorithm","title":"Boyer-Moore Algorithm","text":"<p>Introduces two time-saving heuristics:</p> <ol> <li>Looking Glass Heuristic - when testing a possible placement of <code>P</code> against <code>T</code>, begin the comparisons from the of <code>P</code> and move bckward to the front of <code>P</code>.</li> <li>Character Jump Heuristic - during the testing of a possible placement of <code>P</code> within <code>T</code>, a mismatch of text character <code>T[i]=c</code> with the corresponding pattern character <code>P[k]</code> is handled as follows: if <code>c</code> is not contained anywhere in <code>P</code>, then shift <code>P</code> completely past <code>T[i]</code> (for it cannot match any character in <code>P</code>), otherwise shift <code>P</code> until an occurance of character <code>c</code> in <code>P</code> gets aligned with <code>T[i]</code>.</li> </ol> <p>The time complexity of the Boyer-Moore algorithm depends on the characteristics of the input text and pattern. </p> <ul> <li> <p>Best Case: In the best-case scenario, when there are many mismatches and few matches, the algorithm can achieve linear time complexity (\\(O(n/m)\\)), where \\(n\\) is the length of the text and \\(m\\) is the length of the pattern.</p> </li> <li> <p>Worst Case: In the worst-case scenario, the algorithm may exhibit quadratic time complexity (\\(O(mn)\\)). </p> </li> </ul> <p>Despite the worst-case time complexity, Boyer-Moore is often faster than other string searching algorithms in practice, especially for longer patterns and texts, due to its ability to skip large portions of the text efficiently based on precomputed rules.</p> <pre><code>def find_boyer_moore(T, P):\n  \"\"\"Return the lowest index of T at which substring P begins (or else -1).\"\"\"\n  n, m = len(T), len(P)                   # introduce convenient notations\n  if m == 0: return 0                     # trivial search for empty string\n  last = {}                               # build 'last' dictionary\n  for k in range(m):\n    last[ P[k] ] = k                      # later occurrence overwrites\n  # align end of pattern at index m-1 of text\n  i = m-1                                 # an index into T\n  k = m-1                                 # an index into P\n  while i &lt; n:\n    if T[i] == P[k]:                      # a matching character\n      if k == 0:\n        return i                          # pattern begins at index i of text\n      else:\n        i -= 1                            # examine previous character\n        k -= 1                            # of both T and P\n    else:\n      j = last.get(T[i], -1)              # last(T[i]) is -1 if not found\n      i += m - min(k, j + 1)              # case analysis for jump step\n      k = m - 1                           # restart at end of pattern\n  return -1\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/text%20processing.html#knuth-morris-pratt-algorithm","title":"Knuth-Morris-Pratt Algorithm","text":"<p>The Knuth-Morris-Pratt (KMP) algorithm works by utilizing the information about previous matches to avoid unnecessary comparisons.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/text%20processing.html#key-components","title":"Key Components:","text":"<ol> <li> <p>Failure Function (Prefix Function): The algorithm precomputes a \"failure function\" or \"prefix function,\" which stores the length of the longest proper prefix that is also a suffix of the pattern. This information is used to determine the amount to shift the pattern when a mismatch occurs.</p> </li> <li> <p>Partial Match Table (LPS Array): The failure function is often implemented using a partial match table, also known as the \"Longest Proper Prefix which is also Suffix\" (LPS) array. This table helps to efficiently compute the shift amount during the search phase.</p> </li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/text%20processing.html#search-phase","title":"Search Phase:","text":"<ol> <li> <p>Comparison of Pattern and Text: The algorithm compares characters of the pattern with corresponding characters in the text from left to right. When a mismatch occurs, instead of shifting the pattern by one position, KMP uses the failure function to determine the maximum possible shift without missing potential matches.</p> </li> <li> <p>Utilizing LPS Array: The LPS array helps to avoid redundant comparisons by efficiently skipping over previously matched characters. When a mismatch occurs, the algorithm uses the LPS value of the previous character to determine the next possible comparison position in the pattern.</p> </li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/text%20processing.html#knuth-morris-pratt-algorithm-example","title":"Knuth-Morris-Pratt Algorithm: Example","text":"<p>Consider searching for the pattern \"ABABCAB\" in the text \"ABABDABACDABABCABAB\". Here's a brief overview of how the KMP algorithm works:</p> <ol> <li>Precompute the LPS array for the pattern: \"ABABCAB\"</li> <li> <p>LPS array: \\([0, 0, 1, 2, 0, 1, 2]\\)</p> </li> <li> <p>Start the search phase:</p> </li> <li>Begin comparing characters of the pattern and the text from left to right.</li> <li> <p>When a mismatch occurs, use the LPS value of the previous character to determine the next comparison position.</p> </li> <li> <p>Progress through the text:</p> </li> <li>When a match occurs, continue comparing subsequent characters.</li> <li> <p>When a mismatch occurs, use the LPS value to determine the next position to start comparing from.</p> </li> <li> <p>Continue until the end of the text is reached or a complete match of the pattern is found.</p> </li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/text%20processing.html#python-implementation","title":"Python implementation","text":"<pre><code>def find_kmp(T, P):\n  \"\"\"Return the lowest index of T at which substring P begins (or else -1).\"\"\"\n  n, m = len(T), len(P)            # introduce convenient notations\n  if m == 0: return 0              # trivial search for empty string\n  fail = compute_kmp_fail(P)       # rely on utility to precompute\n  j = 0                            # index into text\n  k = 0                            # index into pattern\n  while j &lt; n:\n    if T[j] == P[k]:               # P[0:1+k] matched thus far\n      if k == m - 1:               # match is complete\n        return j - m + 1           \n      j += 1                       # try to extend match\n      k += 1\n    elif k &gt; 0:                    \n      k = fail[k-1]                # reuse suffix of P[0:k]\n    else:\n      j += 1\n  return -1                        # reached end without match\n\ndef compute_kmp_fail(P):\n  \"\"\"Utility that computes and returns KMP 'fail' list.\"\"\"\n  m = len(P)\n  fail = [0] * m                   # by default, presume overlap of 0 everywhere\n  j = 1\n  k = 0\n  while j &lt; m:                     # compute f(j) during this pass, if nonzero\n    if P[j] == P[k]:               # k + 1 characters match thus far\n      fail[j] = k + 1\n      j += 1\n      k += 1\n    elif k &gt; 0:                    # k follows a matching prefix\n      k = fail[k-1]\n    else:                          # no match found starting at j\n      j += 1\n  return fail\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/text%20processing.html#knuth-morris-pratt-algorithm-time-complexity","title":"Knuth-Morris-Pratt Algorithm: Time Complexity","text":"<p>The time complexity of the Knuth-Morris-Pratt algorithm is \\(O(n + m)\\), where \\(n\\) is the length of the text and \\(m\\) is the length of the pattern. This complexity arises from the fact that each character of the text is compared at most once, and the precomputation of the LPS array takes \\(O(m)\\) time.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/text%20processing.html#least-common-substring-algorithm","title":"Least Common Substring Algorithm","text":"<p>Find the longest string <code>S</code> that is a subsequence of both character string <code>X</code> and <code>Y</code>.</p> <p>The dynamic programming solution for the LCS problem involves building a table to store the lengths of the longest common subsequences of prefixes of the given sequences. The table is filled iteratively, starting from the shortest prefixes and gradually building up to the entire sequences.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/text%20processing.html#key-steps","title":"Key Steps:","text":"<ol> <li> <p>Initialization: Initialize a table \\(dp\\) with dimensions \\((n+1) \\times (m+1)\\), where \\(n\\) and \\(m\\) are the lengths of the two sequences, respectively. Initialize the first row and first column of the table to 0.</p> </li> <li> <p>Filling the Table: Iterate over the sequences and fill the table according to the following rules:</p> </li> <li>If the current characters match, increment the value in the table corresponding to the previous characters by 1.</li> <li> <p>If the current characters do not match, take the maximum of the values obtained by either excluding one of the characters and considering the LCS obtained so far or by excluding the other character and considering the LCS obtained so far.</p> </li> <li> <p>Backtracking: Once the table is filled, backtrack from the bottom-right corner to reconstruct the longest common subsequence.</p> </li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/text%20processing.html#example","title":"Example:","text":"<p>Consider two sequences \"ABCBDAB\" and \"BDCAB\". Here's a step-by-step explanation of the dynamic programming solution:</p> <ol> <li>Initialize the table: <pre><code>   |   | A | B | C | B | D | A | B |\n   |---|---|---|---|---|---|---|---|\n   |   | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n B | 0 | 0 | 1 | 1 | 1 | 1 | 1 | 1 |\n D | 0 | 0 | 1 | 1 | 1 | 2 | 2 | 2 |\n C | 0 | 0 | 1 | 2 | 2 | 2 | 2 | 2 |\n A | 0 | 1 | 1 | 2 | 2 | 2 | 3 | 3 |\n B | 0 | 1 | 2 | 2 | 3 | 3 | 3 | 4 |\n</code></pre></li> <li>Backtrack from the bottom-right corner to reconstruct the LCS: \"BCAB\"</li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/text%20processing.html#time-complexity","title":"Time Complexity:","text":"<p>The time complexity of the dynamic programming solution for the LCS problem is \\(O(nm)\\), where \\(n\\) and \\(m\\) are the lengths of the two sequences, respectively. This is because we fill an \\(n \\times m\\) table, and each cell requires constant time to compute based on the values of its neighbors.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/text%20processing.html#python-implementation_1","title":"Python Implementation","text":"<pre><code>def LCS(X, Y):\n  \"\"\"Return table such that L[j][k] is length of LCS for X[0:j] and Y[0:k].\"\"\"\n  n, m = len(X), len(Y)                      # introduce convenient notations\n  L = [[0] * (m+1) for k in range(n+1)]      # (n+1) x (m+1) table\n  for j in range(n):\n    for k in range(m):\n      if X[j] == Y[k]:                       # align this match\n        L[j+1][k+1] = L[j][k] + 1            \n      else:                                  # choose to ignore one character\n        L[j+1][k+1] = max(L[j][k+1], L[j+1][k])\n  return L\n\ndef LCS_solution(X, Y, L):\n  \"\"\"Return the longest common substring of X and Y, given LCS table L.\"\"\"\n  solution = []\n  j,k = len(X), len(Y)\n  while L[j][k] &gt; 0:                   # common characters remain\n    if X[j-1] == Y[k-1]:\n      solution.append(X[j-1])\n      j -= 1\n      k -= 1\n    elif L[j-1][k] &gt;= L[j][k-1]:\n      j -=1\n    else:\n      k -= 1\n  return ''.join(reversed(solution))   # return left-to-right version\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/text%20processing.html#text-compression","title":"Text Compression","text":"<p>The text compression problem involves efficiently encoding a string \\(x\\) into a smaller binary string \\(y\\) using only the characters 0 and 1. The goal is to reduce the size of the encoded string while preserving its essential information.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/text%20processing.html#huffman-coding","title":"Huffman Coding","text":"<p>Huffman coding is a popular algorithm used for lossless data compression. It works by assigning variable-length codes to input characters, with shorter codes assigned to more frequent characters and longer codes assigned to less frequent characters. This ensures that the most common characters are represented by shorter codes, resulting in efficient compression.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/text%20processing.html#key-concepts","title":"Key Concepts:","text":"<ol> <li> <p>Character Frequencies: Huffman coding takes advantage of the frequency distribution of characters in the input text. Characters that occur more frequently are assigned shorter codes, while characters that occur less frequently are assigned longer codes.</p> </li> <li> <p>Prefix Code: Huffman codes are prefix-free codes, also known as prefix codes. This means that no code word is a prefix of another code word. This property allows for uniquely decodable encoding and decoding.</p> </li> <li> <p>Binary Tree Representation: Huffman codes can be represented using binary trees, where each leaf node corresponds to a character and each internal node corresponds to the sum of the frequencies of its children. The tree is constructed in a way that minimizes the total encoding length.</p> </li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/text%20processing.html#huffman-coding-algorithm","title":"Huffman Coding Algorithm","text":"<p>The Huffman coding algorithm follows these steps:</p> <ol> <li>Frequency Counting: Count the frequency of each character in the input text.</li> <li>Build Huffman Tree: Build a Huffman tree using a priority queue (min-heap) or similar data structure. The tree is constructed by repeatedly merging the two nodes with the lowest frequencies until only one node remains, which becomes the root of the Huffman tree.</li> <li>Generate Codes: Traverse the Huffman tree to generate Huffman codes for each character. The code for each character is determined by the path from the root of the tree to the leaf node representing that character.</li> <li>Encode Data: Encode the input text using the generated Huffman codes.</li> <li>Decode Data: Decode the encoded text using the same Huffman tree.</li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/text%20processing.html#pseudocode","title":"Pseudocode","text":"<pre><code>Node:\n    character\n    frequency\n    left_child\n    right_child\n\nHuffmanCoding(X):\n    1. Count the frequency of each character in string X.\n    2. Create an empty min-heap Q.\n    3. for each character c in X:\n         - Insert a node containing c, its frequency, and null left and right children into Q.\n    4. while |Q| &gt; 1:\n         - Remove two nodes with the lowest frequencies, say node1 and node2, from Q.\n         - Create a new internal node with frequency = node1.frequency + node2.frequency.\n         - Set node1 as the left child and node2 as the right child of the new internal node.\n         - Insert the new internal node back into Q.\n    5. root = Q.pop()  // The root of the Huffman tree is the last node remaining in Q.\n    6. GenerateCodes(root, \"\")  // Call a recursive function to generate Huffman codes for each character.\n\nGenerateCodes(node, code):\n    if node is a leaf node:\n        Output node.character and code  // Output the character and its corresponding code\n    else:\n        GenerateCodes(node.left_child, code + \"0\")  // Traverse left and append '0' to the code\n        GenerateCodes(node.right_child, code + \"1\")  // Traverse right and append '1' to the code\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/text%20processing.html#complexity-analysis","title":"Complexity Analysis","text":"<ul> <li>Building Huffman Tree: The time complexity of building the Huffman tree is \\(O(n \\log n)\\), where \\(n\\) is the number of unique characters in the input text. This is because each insertion and deletion operation in the priority queue takes \\(O(\\log n)\\) time, and there are \\(n\\) characters to process.</li> <li>Generating Codes: The time complexity of generating Huffman codes is \\(O(n)\\), where \\(n\\) is the number of unique characters in the input text. This is because each character corresponds to a leaf node in the Huffman tree, and traversing from the root to each leaf node takes constant time.</li> <li>Encoding and Decoding: The time complexity of encoding and decoding the text using Huffman codes is \\(O(m)\\), where \\(m\\) is the length of the input text. This is because each character in the input text is replaced by its corresponding Huffman code, which takes constant time.</li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/tree%20traversals.html","title":"Tree Traversals","text":"<p>Systematic way of accessing all of the positions of a tree. An example of implementing these in an abstree <code>Tree</code> datastructure is presented in Trees</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/tree%20traversals.html#pre-order-post-order-and-in-order","title":"Pre-Order, Post-Order, and In-Order","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/tree%20traversals.html#pre-order","title":"Pre-Order","text":"<p>in pre-order traversal the root of the tree is visited first, and then the subtrees rooted at its children are traversed recursively. If the tree is ordered, then the subtrees are traversed according to the order of the children.</p> <pre><code>pre_ordered_list = []\ndef pre_order(tree, position, pre_ordered_list):\n    pre_ordered_list.append(position)\n    for child in tree.children(position):\n        pre_order(tree, child, visited)\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/tree%20traversals.html#post-order","title":"Post-Order","text":"<p>Opposite of pre-order traversal, because it recursively traverses the subtrees rooted at the children of the root first, and then visits the root.</p> <pre><code>post_ordered_list = []\ndef post_order(tree, position, post_ordered_list):\n    for child in tree.children(position):\n        post_order(tree, child, post_ordered_list)\n    post_ordered_list.add(position)\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/tree%20traversals.html#in-order","title":"In-Order","text":"<p>This traversal is specific to binary trees. We visit a position between the recursive traversal of the left and right subtrees of a binary tree. i.e. visiting nodes from <code>left to right</code>. For every position <code>p</code>, inorder traversal visits <code>p</code> after all of the positions in the left subtree of <code>p</code> and before all the positions in the right subtree of <code>p</code>.</p> <pre><code>in_order_list = []\ndef inorder(tree, position, in_order_list):\n    if tree.left(position):\n        inorder(tree, tree.left(position), in_order_list)\n    in_order_list.append(position)\n    if tree.right(position):\n        inorder(tree, tree.right(position), in_order_list)\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/tree%20traversals.html#breadth-first-search","title":"Breadth First Search","text":"<p>Traverse a tree so that we visit all of the positions at a given depth before we visit the positions at the next depth/level.</p> <p>We use a queue to produce a FIFO sematic for the order in which we visit nodes without making it recursive.</p> <pre><code>def breadth_first_search(tree):\n    queue = deque([tree.root])\n    bfs_order = []\n    while queue:\n        p = queue.popleft()\n        bfs_order.append(p)\n        for child in tree.children(p):\n            queue.enqueue(child)\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/tree%20traversals.html#depth-first-search","title":"Depth First Search","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/tree%20traversals.html#prim-jarnik","title":"Prim-Jarnik","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/tree%20traversals.html#kruskals","title":"Kruskal's","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/tree%20traversals.html#euler-tours","title":"Euler Tours","text":"<p>The Euler tour traversal of a general tree <code>T</code> is defined as a \"walk\" around <code>T</code>, where we start by going from the root towards its leftmost child, viewing the edges of <code>T</code> as being \"walls\" that we always keep to the left.</p> <p>The complexity of the walk is <code>O(n)</code> - it progresses exactly to times along each of the <code>n-1</code> edges of the tree - once going downward along the edge, and later going upward along the edge. </p> <p></p> <p>To unify this with <code>preorder</code> and <code>postorder</code> traversals, we can think of there being two notable \"visits\" to each position <code>p</code>:</p> <ul> <li>a <code>pre visit</code> occurs when first reaching the position, that is, whe the walk passes immediately left of the node in our visualization</li> <li>a <code>post visit</code> occurs when the walk later proceeds upward from that position, that is, when the walk pass to the right of the node in our visualization</li> </ul> <p>Euler tour can be done recursively such that in between the pre-visit and post-visit of a position will be a recursve tour of each of the subtrees. </p> <pre><code>Algorithm eulertour(T, p):\n     perform the \"previsit\" action for position p\n     for each child c in T.children(p):\n        eulertou(T, c)\n\n    perform the \"postvisit\" action for position p\n</code></pre> <p>A more complete implementation is below, where we use the Template Method to define the generic traversal algorithm, and use two hooks (auxiliary functions) - one for the previsit before the subtrees are traversed, and another for the postvisit after the completion of subtree traversals.</p> <p>The hooks can be overridden to provide specialized behavior.</p> <pre><code>class EulerTour:\n\n    def __init__(self, tree):\n        self._tree = tree\n\n    def tree(self):\n        return self._tree\n\n    def execute(self):\n        if len(self._tree) &gt; 0:\n            return self._tour(self._tree.root(), 0, [])\n\n    def _tour(self, p, d, path):\n        \"\"\"\n        Perform the tour of a subtree rooted at Position p.\n\n        p = position of current node being visited\n        d = depth of p in the tree\n        path = list of indices of children on path from root to p\n        \"\"\"\n\n        # previsit p\n        self._hook_previsit(p, d, path)\n\n        results = []\n\n        # add new index to the end of path before recursion\n        path.append(0)\n\n        for c in self._tree.children(p):\n            # recur on child's subtree\n            results.append(self._tour(c, d+1, path))\n            # increment index\n            path[-1] += 1\n        # remove extraneous index from the end of path\n        path.pop()\n        # post visit p\n        answer = self._hook_postvisit(p, d, path, results)\n\n        return answer\n\n    def _hook_previsit(self, p, d, path):\n        pass\n\n    def _hook_postvisit(self, p, d, path):\n        pass \n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/tree%20traversals.html#applications","title":"Applications","text":"<p>TODO: Add notes here - leaving blank for now.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/Pattern%3A%20Dual%20Sequence%20or%20DP%20on%20Subsequences.html","title":"Dual Sequence or DP on Subsequences","text":"<p>The \"Dual Sequence\" dynamic programming pattern is used to solve problems that involve calculating some value related to two sequences. This pattern is particularly useful when the solution depends on both sequences, and we need to consider all possible subsequences of both sequences.</p> <p>In this pattern, we define a 2D array <code>cache[i][j]</code> to store the answer to the subproblem solved on a prefix of sequence 1 with length <code>i</code>, and a prefix of sequence 2 with length <code>j</code>. We then iteratively fill in this array to compute the final answer.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/Pattern%3A%20Dual%20Sequence%20or%20DP%20on%20Subsequences.html#top-down-approach","title":"Top-Down Approach:","text":"<pre><code>def dp_top_down(i, j):\n    if i == 0 or j == 0:\n        return base_case_value\n    if cache[i][j] is already computed:\n        return cache[i][j]\n    # Compute the value based on previous results and current elements\n    cache[i][j] = some_function(dp_top_down(i-1, j), dp_top_down(i, j-1), ...)\n    return cache[i][j]\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/Pattern%3A%20Dual%20Sequence%20or%20DP%20on%20Subsequences.html#bottom-up-approach","title":"Bottom-Up Approach","text":"<pre><code>def dp_bottom_up():\n    # Initialize base cases\n    for i in range(m+1):\n        cache[i][0] = base_case_value\n    for j in range(n+1):\n        cache[0][j] = base_case_value\n    # Fill DP array\n    for i in range(1, m+1):\n        for j in range(1, n+1):\n            cache[i][j] = some_function(cache[i-1][j], cache[i][j-1], ...)\n    return cache[m][n]  # Final answer\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/Pattern%3A%20Dual%20Sequence%20or%20DP%20on%20Subsequences.html#complexity","title":"Complexity:","text":"<ul> <li> <p>Time Complexity: The time complexity of the DP solution depends on the size of the input sequences and the number of subproblems computed. Typically, it is \\(O(mn)\\), where \\(m\\) and \\(n\\) are the lengths of the input sequences.</p> </li> <li> <p>Space Complexity: The space complexity also depends on the size of the DP array, which is typically \\(O(mn)\\).</p> </li> </ul> <p>This dynamic programming pattern is versatile and can be applied to various problems that involve two sequences, such as finding the longest common subsequence, edit distance, or maximum sum of increasing subsequences.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/Pattern%3A%20Grids.html","title":"Pattern: Grids","text":"<p>This dynamic programming pattern is used to solve problems where you have a grid structure, and you need to find an optimal solution by considering subgrids within the original grid. The cached table will have the same dimensions as the grid.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/Pattern%3A%20Grids.html#memoization-top-down-approach","title":"Memoization (Top-Down) Approach","text":"<p>Define the state:</p> <ul> <li>dp[i][j]: [Define the meaning of the state dp[i][j].]</li> </ul> <p>Initialize the memoization array:</p> <ul> <li>Initialize a memoization array <code>memo</code> with dimensions equal to the grid.</li> </ul> <p>Memoization function:</p> <ul> <li>Define a function to recursively compute the state <code>dp[i][j]</code> while memoizing the results.</li> </ul> <p>Pseudocode:</p> <pre><code>def dp(i, j):\n    if (i, j) has already been computed:\n        return memo[i][j]\n    if base case:\n        return base case value\n    dp[i][j] = [Define transition based on previous states, recursively calling dp function]\n    memo[i][j] = dp[i][j]\n    return dp[i][j]\n\ndef solve_grid_problem(grid):\n    initialize memoization array memo\n    return dp(n, m)  # n and m are the dimensions of the grid\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/Pattern%3A%20Grids.html#bottom-up-approach","title":"Bottom-up Approach","text":"<p>Define the state:</p> <ul> <li>dp[i][j]: [Define the meaning of the state dp[i][j].]</li> </ul> <p>Initialize the base case:</p> <ul> <li>Initialize dp[0][0], dp[0][1], ..., dp[n][m] based on the problem.</li> </ul> <p>Transition:</p> <ul> <li>dp[i][j] = [Define the transition based on the problem statement and previous states].</li> </ul> <p>Return:</p> <ul> <li>Return the desired result, e.g., dp[n][m].</li> </ul> <p>Pseudocode:</p> <pre><code>def solve_grid_problem(grid):\n    n, m = dimensions of the grid\n    # Initialize dp array\n    dp = [[0] * m for _ in range(n)]\n\n    # Base cases\n    dp[0][0] = [Define base case]\n\n    # Populate dp array bottom-up\n    for i from 0 to n-1:\n        for j from 0 to m-1:\n            if (i, j) is not in the boundary:\n                dp[i][j] = [Define transition based on dp[i-1][j], dp[i][j-1], and/or dp[i-1][j-1]]\n\n    # Return the desired result\n    return dp[n-1][m-1]\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/Pattern%3A%20Interval.html","title":"Interval Dynamic Programming","text":"<p>The Interval dynamic programming pattern is used to solve problems where the DP subproblem is solved on every single interval (subarray) of the array. This pattern is particularly useful for problems involving contiguous subsequences or subarrays, such as finding the Longest Palindromic Subsequence.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/Pattern%3A%20Interval.html#approach-explanation","title":"Approach Explanation:","text":"<p>In this pattern, we define a 2D array <code>dp[i][j]</code> to store the solution to the subproblem solved on the interval <code>[i, j]</code> of the array. We then iteratively fill in this array to compute the final answer.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/Pattern%3A%20Interval.html#top-down-approach","title":"Top-Down Approach:","text":"<pre><code>def dp_top_down(i, j):\n    if i &gt; j:\n        return 0  # Base case: Empty interval has zero length\n    if i == j:\n        return 1  # Base case: Single element interval has length 1\n    if dp[i][j] is already computed:\n        return dp[i][j]\n    if s[i] == s[j]:\n        dp[i][j] = 2 + dp_top_down(i+1, j-1)\n    else:\n        dp[i][j] = max(dp_top_down(i+1, j), dp_top_down(i, j-1))\n    return dp[i][j]\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/Pattern%3A%20Interval.html#bottom-up-approach","title":"Bottom-Up Approach","text":"<pre><code>def dp_bottom_up():\n    # Initialize base cases\n    for i in range(n):\n        dp[i][i] = 1  # Base case: Single element interval has length 1\n    # Fill DP array\n    for length in range(2, n+1):\n        for i in range(n-length+1):\n            j = i + length - 1\n            if s[i] == s[j]:\n                dp[i][j] = 2 + dp[i+1][j-1]\n            else:\n                dp[i][j] = max(dp[i+1][j], dp[i][j-1])\n    return dp[0][n-1]  # Final answer\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/Pattern%3A%20Interval.html#complexity","title":"Complexity:","text":"<ul> <li> <p>Time Complexity: The time complexity of the DP solution depends on the number of intervals considered and the complexity of comparing elements or computing the function <code>dp_top_down</code> or <code>dp_bottom_up</code>. Typically, it is \\(O(n^2)\\), where \\(n\\) is the length of the array.</p> </li> <li> <p>Space Complexity: The space complexity is determined by the size of the DP array, which is typically \\(O(n^2)\\).</p> </li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/Pattern%3A%20Knapsack.html","title":"Knapsack Dynamic Programming","text":"<p>The Knapsack dynamic programming pattern is used to solve problems where items have certain values and weights, and we aim to maximize the value of items included in a knapsack (or a backpack) with a given capacity.</p> <p>In this pattern, we define a 2D array <code>dp[i][w]</code> to store the maximum value that can be achieved using the first <code>i</code> items and a knapsack capacity of <code>w</code>. We then iteratively fill in this array to compute the final answer.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/Pattern%3A%20Knapsack.html#top-down-approach","title":"Top-Down Approach:","text":"<pre><code>def knapsack_top_down(i, w):\n    if i == 0 or w == 0:\n        return 0  # Base case: No items or knapsack capacity is 0\n    if dp[i][w] is already computed:\n        return dp[i][w]\n    if weights[i] &gt; w:\n        dp[i][w] = knapsack_top_down(i-1, w)\n    else:\n        dp[i][w] = max(values[i] + knapsack_top_down(i-1, w - weights[i]), knapsack_top_down(i-1, w))\n    return dp[i][w]\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/Pattern%3A%20Knapsack.html#bottom-up-approach","title":"Bottom-Up Approach:","text":"<pre><code>def knapsack_bottom_up():\n    # Initialize base cases\n    for i in range(n+1):\n        dp[i][0] = 0\n    for w in range(W+1):\n        dp[0][w] = 0\n    # Fill DP array\n    for i in range(1, n+1):\n        for w in range(1, W+1):\n            if weights[i] &gt; w:\n                dp[i][w] = dp[i-1][w]\n            else:\n                dp[i][w] = max(values[i] + dp[i-1][w - weights[i]], dp[i-1][w])\n    return dp[n][W]  # Final answer\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/Pattern%3A%20Knapsack.html#complexity","title":"Complexity:","text":"<ul> <li> <p>Time Complexity: The time complexity of both top-down and bottom-up approaches is \\(O(nW)\\), where \\(n\\) is the number of items and \\(W\\) is the capacity of the knapsack.</p> </li> <li> <p>Space Complexity: The space complexity is determined by the size of the DP array, which is typically \\(O(nW)\\).</p> </li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/Pattern%3A%20Linear%20Sequences%20with%20Constant%20Transition.html","title":"Pattern: Linear Sequences with Constant Transition","text":"<p>This dynamic programming pattern deals with problems where you have to compute a value for each index in a sequence based on the values of previous indices, and the transition from one index to the next is constant. In other words, the transition from <code>dp[i]</code> to <code>dp[i+1]</code> depends only on a fixed number of previous states and not on the current index i.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/Pattern%3A%20Linear%20Sequences%20with%20Constant%20Transition.html#memoization-top-down-approach","title":"Memoization (Top-Down) Approach","text":"<p>Define the state:</p> <ul> <li>dp[i]: [Define the meaning of the state dp[i].]</li> </ul> <p>Initialize the memoization array:</p> <ul> <li>Initialize a memoization array <code>memo</code> with the size of the problem.</li> </ul> <p>Memoization function:</p> <ul> <li>Define a function to recursively compute the state <code>dp[i]</code> while memoizing the results.</li> </ul> <p>Pseudocode:</p> <pre><code>def dp(i):\n    if i has already been computed:\n        return dp[i]\n    if base case:\n        return base case value\n    dp[i] = [Define transition based on previous states, recursively calling dp function]\n    return dp[i]\n\ndef solve_linear_sequences_with_constant_transition(nums):\n    initialize memoization array memo\n    return dp(n)\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/Pattern%3A%20Linear%20Sequences%20with%20Constant%20Transition.html#bottom-up-approach","title":"Bottom-up Approach","text":"<p>Define the state:</p> <ul> <li>dp[i]: [Define the meaning of the state dp[i].]</li> </ul> <p>Initialize the base case:</p> <ul> <li>Initialize dp[0], dp[1], ... [Depends on the problem.]</li> </ul> <p>Transition:</p> <ul> <li>dp[i] = [Define the transition based on the problem statement and previous states].</li> </ul> <p>Return:</p> <ul> <li>Return the desired result, e.g., dp[n].</li> </ul> <p>Pseudocode:</p> <pre><code>def solve_linear_sequences_with_constant_transition(nums):\n    n = len(nums)\n    # Initialize dp array\n    dp = [0] * (n + 1)\n\n    # Base cases\n    dp[0] = [Define base case]\n    dp[1] = [Define base case]\n\n    # Populate dp array bottom-up\n    for i in range(2, n + 1):\n        dp[i] = [Define transition based on dp[i-1] and/or dp[i-2]]\n\n    # Return the desired result\n    return dp[n]\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/Pattern%3A%20Linear%20Sequences%20with%20Non-Constant%20Transition.html","title":"Linear Sequences with Non-Constant Transition Dynamic Programming","text":"<p>The \"Linear Sequences with Non-Constant Transition\" dynamic programming pattern is used to solve problems where the DP problem is solved on every prefix of the array, similar to the \"Group 2\" pattern. However, the transitions in this pattern may not be simple and may require considering a linear amount of options from indices \\(j &lt; i\\).</p> <p>In this pattern, we still consider solving the DP problem on every prefix of the array. However, the transition from one state to another may not be constant and may depend on a linear number of options from previous indices.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/Pattern%3A%20Linear%20Sequences%20with%20Non-Constant%20Transition.html#top-down-approach","title":"Top-Down Approach:","text":"<pre><code>def dp_top_down(i):\n    if i == 0:\n        return base_case_value\n    if dp[i] is already computed:\n        return dp[i]\n    # Initialize with a value that works for the base case\n    dp[i] = initial_value\n    # Iterate over possible transitions from indices j &lt; i\n    for j in range(i-1, -1, -1):\n        # Update dp[i] based on transitions from previous indices\n        dp[i] = max(dp[i], some_function(dp[j], ...))\n    return dp[i]\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/Pattern%3A%20Linear%20Sequences%20with%20Non-Constant%20Transition.html#bottom-up-approach","title":"Bottom-Up Approach:","text":"<pre><code>def dp_bottom_up():\n    # Initialize base cases\n    dp[0] = base_case_value\n    # Fill DP array\n    for i in range(1, n):\n        # Initialize with a value that works for the base case\n        dp[i] = initial_value\n        # Iterate over possible transitions from indices j &lt; i\n        for j in range(i-1, -1, -1):\n            # Update dp[i] based on transitions from previous indices\n            dp[i] = max(dp[i], some_function(dp[j], ...))\n    return max(dp)  # Final answer\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/Pattern%3A%20Matrix%20Chain-Product%20Problem.html","title":"Matrix Chain-Product Problem: Overview","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/Pattern%3A%20Matrix%20Chain-Product%20Problem.html#problem-statement","title":"Problem Statement:","text":"<p>Given a chain of \\(n\\) matrices \\(A_1, A_2, \\ldots, A_n\\), where the dimensions of matrix \\(A_i\\) are \\(p_{i-1} \\times p_i\\) for \\(i = 1, 2, \\ldots, n\\), determine the optimal order of multiplication that minimizes the total number of scalar multiplications needed to compute the product.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/Pattern%3A%20Matrix%20Chain-Product%20Problem.html#example","title":"Example:","text":"<p>Consider a chain of matrices with dimensions \\(p = [10, 20, 30, 40]\\). The dimensions of the matrices are as follows: - Matrix \\(A_1\\) has dimensions \\(10 \\times 20\\). - Matrix \\(A_2\\) has dimensions \\(20 \\times 30\\). - Matrix \\(A_3\\) has dimensions \\(30 \\times 40\\).</p> <p>The goal is to determine the optimal order of multiplication that minimizes the total number of scalar multiplications needed to compute \\(A_1 \\times A_2 \\times A_3\\).</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/Pattern%3A%20Matrix%20Chain-Product%20Problem.html#dynamic-programming-approach","title":"Dynamic Programming Approach","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/Pattern%3A%20Matrix%20Chain-Product%20Problem.html#approach","title":"Approach:","text":"<p>The dynamic programming approach is used to solve the Matrix Chain-Product Problem efficiently. The key idea is to compute the minimum number of scalar multiplications needed to compute the product of matrices of different lengths, starting from smaller subchains and gradually building up to the entire chain.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/Pattern%3A%20Matrix%20Chain-Product%20Problem.html#steps","title":"Steps:","text":"<ol> <li>Subproblem Definition: Define subproblems by considering all possible ways to split the chain of matrices into two subchains.</li> <li>Recurrence Relation: Define a recurrence relation to compute the minimum number of scalar multiplications needed to compute the product of matrices in each subchain.</li> <li>Fill the Table: Use dynamic programming to fill a table with the results of the subproblems, starting from smaller subchains and gradually building up to the entire chain.</li> <li>Backtracking: Once the table is filled, backtrack to reconstruct the optimal order of multiplication.</li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/Pattern%3A%20Matrix%20Chain-Product%20Problem.html#implementation","title":"Implementation","text":"<pre><code>def matrix_chain_order(p):\n    n = len(p) - 1  # Number of matrices in the chain\n    m = [[0] * n for _ in range(n)]  # Table to store minimum scalar multiplications\n    s = [[0] * n for _ in range(n)]  # Table to store optimal split points\n\n    # Fill the table using bottom-up dynamic programming\n    for l in range(2, n + 1):  # l is the chain length\n        for i in range(n - l + 1):\n            j = i + l - 1  # End index of current chain\n            m[i][j] = float('inf')  # Initialize minimum scalar multiplications to infinity\n            for k in range(i, j):\n                # Compute the number of scalar multiplications for each split point\n                cost = m[i][k] + m[k + 1][j] + p[i] * p[k + 1] * p[j + 1]\n                if cost &lt; m[i][j]:\n                    m[i][j] = cost\n                    s[i][j] = k\n\n    # Backtrack to reconstruct the optimal order of multiplication\n    def print_optimal_order(i, j):\n        if i == j:\n            return \"A\" + str(i + 1)\n        else:\n            return \"(\" + print_optimal_order(i, s[i][j]) + \" \u00d7 \" + print_optimal_order(s[i][j] + 1, j) + \")\"\n\n    optimal_order = print_optimal_order(0, n - 1)\n    return m[0][n - 1], optimal_order\n\n# Example usage\ndimensions = [10, 20, 30, 40]\nmin_scalar_multiplications, optimal_order = matrix_chain_order(dimensions)\nprint(\"Minimum scalar multiplications:\", min_scalar_multiplications)\nprint(\"Optimal order of multiplication:\", optimal_order)\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/Pattern%3A%20Matrix%20Chain-Product%20Problem.html#time-complexity","title":"Time Complexity","text":"<p>The time complexity of the dynamic programming solution for the Matrix Chain-Product Problem is \\(O(n^3)\\), where \\(n\\) is the number of matrices in the chain. This is because we fill an \\(n \\times n\\) table, and each cell requires constant time to compute based on the values of its neighbors.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html","title":"Dynamic Programming","text":"<p>TODO: Add notes here - leaving blank for now.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#basics","title":"Basics","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#climbing-stairs","title":"Climbing Stairs","text":"<p>https://leetcode.com/problems/climbing-stairs/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#n-th-tribonacci-number","title":"N-th Tribonacci Number","text":"<p>https://leetcode.com/problems/n-th-tribonacci-number/description/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#perfect-squares","title":"Perfect Squares","text":"<p>https://leetcode.com/problems/perfect-squares/description/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#matrix-chain-multiplication","title":"Matrix Chain Multiplication","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#linear-sequences-with-constant-transition","title":"Linear Sequences with Constant Transition","text":"<p>Here the solution requires us to solve the sub problem on every prefix of the array. The definition of a prefix of an array is a subarray from 0 to i for some i.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#min-cost-climbing-stairs","title":"Min Cost Climbing Stairs","text":"<p>https://leetcode.com/problems/min-cost-climbing-stairs/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#house-robber","title":"House Robber","text":"<p>https://leetcode.com/problems/house-robber/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#decode-ways","title":"Decode Ways","text":"<p>https://leetcode.com/problems/decode-ways/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#minumum-cost-for-tickets","title":"Minumum Cost for Tickets","text":"<p>https://leetcode.com/problems/minimum-cost-for-tickets/description/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#solving-questions-with-brainpower","title":"Solving Questions with Brainpower","text":"<p>https://leetcode.com/problems/solving-questions-with-brainpower/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#grid-problems","title":"Grid Problems","text":"<p>The cached table will have the same dimensions as the grid. The DP solution will involve solving the sub problem on a bunch of subgrids.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#unique-paths","title":"Unique Paths","text":"<p>https://leetcode.com/problems/unique-paths/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#unique-paths-ii","title":"Unique Paths II","text":"<p>https://leetcode.com/problems/unique-paths-ii/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#minimum-path-sum","title":"Minimum Path Sum","text":"<p>https://leetcode.com/problems/minimum-path-sum/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#count-square-submatrices-with-all-ones","title":"Count Square Submatrices with All Ones","text":"<p>https://leetcode.com/problems/count-square-submatrices-with-all-ones/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#maximal-square","title":"Maximal Square","text":"<p>https://leetcode.com/problems/maximal-square/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#dungeon-game","title":"Dungeon Game","text":"<p>https://leetcode.com/problems/dungeon-game/description/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#dual-sequence-or-dp-on-subsequences","title":"Dual Sequence or DP on Subsequences","text":"<p>The prblem asks about calcuating some value related to two sequences. The <code>cache[i][j]</code> will store the answer to the subproblem solved on a prefix on sequence 1 with length i, and prefix on sequence 2 with length j.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#longest-common-subsequence","title":"Longest Common Subsequence","text":"<p>https://leetcode.com/problems/longest-common-subsequence/description/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#uncrossed-lines","title":"Uncrossed Lines","text":"<p>https://leetcode.com/problems/uncrossed-lines/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#minimum-ascii-delete-sum-for-two-strings","title":"Minimum ASCII Delete Sum for Two Strings","text":"<p>https://leetcode.com/problems/minimum-ascii-delete-sum-for-two-strings/description/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#edit-distance","title":"Edit Distance","text":"<p>https://leetcode.com/problems/edit-distance/description/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#distinct-subsequences","title":"Distinct Subsequences","text":"<p>https://leetcode.com/problems/distinct-subsequences/description/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#shortest-common-supersequence","title":"Shortest Common Supersequence","text":"<p>https://leetcode.com/problems/shortest-common-supersequence/description/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#interval","title":"Interval","text":"<p>The DP sub problem is solved on every single interval (subarray) of the array.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#longest-palindromic-subsequence","title":"Longest Palindromic Subsequence","text":"<p>https://leetcode.com/problems/longest-palindromic-subsequence/description/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#stone-game-vii","title":"Stone Game VII","text":"<p>https://leetcode.com/problems/stone-game-vii/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#palindromic-substrings","title":"Palindromic Substrings","text":"<p>https://leetcode.com/problems/palindromic-substrings/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#minimum-cost-tree-from-leaf-values","title":"Minimum Cost Tree From Leaf Values","text":"<p>https://leetcode.com/problems/minimum-cost-tree-from-leaf-values/description/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#burst-balloons","title":"Burst Balloons","text":"<p>https://leetcode.com/problems/burst-balloons/description/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#strange-printer","title":"Strange Printer","text":"<p>https://leetcode.com/problems/strange-printer/description/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#linear-sequences-with-non-constant-transition","title":"Linear Sequences with Non-Constant Transition","text":"<p>DP problem is solved on every prefix of the array just like group 2. However, transitions may not be simple and require a linear amount of options from indices j &lt; i.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#count-number-of-teams","title":"Count Number of Teams","text":"<p>https://leetcode.com/problems/count-number-of-teams/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#longest-increasing-subsequence","title":"Longest Increasing Subsequence","text":"<p>https://leetcode.com/problems/longest-increasing-subsequence/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#partition-array-for-maximum-sum","title":"Partition Array for Maximum Sum","text":"<p>https://leetcode.com/problems/partition-array-for-maximum-sum/description/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#largest-sum-of-averages","title":"Largest Sum of Averages","text":"<p>https://leetcode.com/problems/largest-sum-of-averages/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#filling-bookcase-shelves","title":"Filling Bookcase Shelves","text":"<p>https://leetcode.com/problems/filling-bookcase-shelves/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#knapsack-problems","title":"Knapsack Problems","text":"<p>Problems with constraints.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#0-1-knapsack","title":"0-1 Knapsack","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#minimum-cost-to-cut-a-stick","title":"Minimum Cost to Cut a Stick","text":"<p>https://leetcode.com/problems/minimum-cost-to-cut-a-stick/description/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#partition-equal-subset-sum","title":"Partition Equal Subset Sum","text":"<p>https://leetcode.com/problems/partition-equal-subset-sum/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#number-of-dice-rolls-with-target-sum","title":"Number of Dice Rolls with Target Sum","text":"<p>https://leetcode.com/problems/number-of-dice-rolls-with-target-sum/description/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#combination-sum-iv","title":"Combination Sum IV","text":"<p>https://leetcode.com/problems/combination-sum-iv/description/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#ones-and-zeros","title":"Ones and Zeros","text":"<p>https://leetcode.com/problems/ones-and-zeroes/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#coin-change","title":"Coin Change","text":"<p>https://leetcode.com/problems/coin-change/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#coin-change-ii","title":"Coin Change II","text":"<p>https://leetcode.com/problems/coin-change-ii/description/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#target-sum","title":"Target Sum","text":"<p>https://leetcode.com/problems/target-sum/description/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#last-stone-weight-ii","title":"Last Stone Weight II","text":"<p>https://leetcode.com/problems/last-stone-weight/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Dynamic%20Programming/dynamic%20programming.html#profitable-schemes","title":"Profitable Schemes","text":"<p>https://leetcode.com/problems/profitable-schemes/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Algorithms/Patterns/optimization.html","title":"Optimization","text":"<p>Whenever there is a component of optimization in the question, i.e. when the following words are used</p> <ol> <li>\"Can we reach this answer\"</li> <li>\"minimize/maximize\"</li> <li>\"longest/shortest\"</li> <li>\"largest/smallest\"</li> </ol> <p>you should consider Greedy or Dynamic Programming approaches.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/arrays.html","title":"Arrays","text":"<p>Python has various \"sequence\" classes, namely the built in list, tuple and str classes. They have significant commonality, most notably that:</p> <ol> <li>each supports indexing to access an individual elements of a sequence using a syntax such as <code>seq[k]</code></li> <li>each uses a low-level concept known as an array to represent the sequence</li> </ol> <p>However they also have significant differences in the abstractions that these classes represent, and in the way instances of these classes are represented internally by Python.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/arrays.html#low-level-arrays","title":"Low-Level Arrays","text":"<p>The primary memory of a computer is composed of bits of information, and those bits are typically grouped into larger units that depend upon the precise system architecture. Such a typical unit is a byte, which is equivalent to 8 bits.</p> <p>To keep track of what information is stored in what byte, the computer uses an abstraction known as a memory address - each byte of memory is associated wih a unique number that serves as its address (more formally, the binary representation of the number servers as the address).</p> <p>Even thought the bytes are physically stored sequentially, Random Access Memory can store and retrieve in <code>O(1)</code> time.</p> <p>A group of related variables can be stored one after another in a contiguous portion of the computer's memory - denoted as an array. An example is a text string where each character is represented using the Unicode Character set with 16 bits (i.e. 2 bytes) for each character. So a siz-character string, such as 'SAMPLE' is stored in 12 consecutive bytes of memory.</p> <p>Hence each cell of an array must use the same number of bytes to allow an arbitrary cell of the array to be accessed in constant time based on its index.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/arrays.html#referential-arrays","title":"Referential Arrays","text":"<p>Imagine an array of 200 strings - these strings can be of any length. So one way to store this would be to reserve enough space for each cell to hold the maximum length string but that would be wasteful. So instead we store the reference of each cell's object - i.e. memory addresses at which the elements of the sequence reside.</p> <p>In such a scenario:</p> <ol> <li>a single list may include multiple references to the same object as elements of the list</li> <li>it is possible for a single object to be an element of two or more lists</li> </ol> <p>When elements are immutable, this is not a huge issue, as neither of the lists can cause a change to the shared object. e.g. <code>temp[2] = 15</code> just changes the reference of the indexed cell to the new object <code>15</code>.</p> <p>More care is needed when we are dealing with mutable objects.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/arrays.html#compact-array","title":"Compact Array","text":"<p>Strings are for example represented using an array of characters (not an array of references) - i.e. a compact array because the array is storing the bits that represent the primary data.</p> <p>These have some advantages over referential structures in terms of computing performance:</p> <ol> <li>overall memory usage will be much lower for a compact structure because there is no overhead devoted to the explicit storage of the sequence of memory references in addition to the primary data</li> <li>the primary data are stored consecutively in memory - which is not the case for a referential structure. Because of the workings of the cache and memory hierarchies of computers, it is often advantageous to have data stored in memory near other data that might be used in the same computations.</li> </ol> <p>Primary support for compact arrays is in a module named <code>array</code>.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/arrays.html#dynamic-arrays-and-amortization","title":"Dynamic Arrays and Amortization","text":"<p>When creating a low-level array in a computer system, the precise size of that array must be explicitly declared in order for the system o properly allocate a consecutive piece of memory for its storage. Because the system may dedicate neighboring memory locations to store other data, the capacity of an array cannot trivially be increased by expanding into subsequent cells.</p> <ul> <li>for <code>tuple</code> or <code>str</code> this is not a problem - they are immutable, so the correct size for an underlying array can be fixed when the object is instantiated</li> </ul> <p>However <code>list</code> class allows us to add elements tot he list with no apparent limit to the overall capacity of the list - this is accomplished through an algorithmic implementation of a dynamic array.</p> <p>The key properties that allow this are:</p> <ol> <li>the list instance maintains an underlying array that is often larger / has greater capacity than the current length of the list - the system may reserve an underlying array capable of storing more object references than what the list was instantiated with</li> <li>as new elements are appended to a list, the class requests new larger arrays from the system and initialize the new array so that its prefix matches that of the existing small array - at which point the old array is reclaimed by the system</li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/arrays.html#implementing-a-dynamic-array","title":"Implementing a Dynamic Array","text":"<pre><code>import ctypes\n\nclass DynamicArray:\n    \"\"\"\n    A dynamic array class akin to a simplified Python list.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Create an empty array\"\"\"\n        # count actual elements\n        self._n = 0\n        # default array capacity\n        self._capacity = 1\n        # low-level array\n        self._A = self._make_array(self._capacity)\n\n    def __len__(self):\n        \"\"\"Return number of elements stored in the array.\"\"\"\n        return self._n\n\n    def __get_item(self, k):\n        \"\"\"Return element at index k.\"\"\"\n        if not 0 &lt;= k &lt; self._n:\n            raise IndexError('invalid index')\n        # retrieve from array\n        return self._A[k]\n\n    def append(self, obj):\n        \"\"\"Add an object to the end of the array\"\"\"\n\n        # if there is not enough room\n        if self._n == self._capacity:\n            # double the capacity\n            self._resize(2 * self._capacity)\n\n        self._A[self._n] = obj\n        self._n += 1\n\n    def insert(self, k, obj):\n        \"\"\"Insert value at index k, shifting subsequent values rightward\"\"\"\n\n        # we assume that k is 0 &lt;= k &lt;= n\n        if self._n == self._capacity:\n            # not enough room\n            self._resize(2 * self._capacity)\n\n        for j in range(self._n, k, -1):\n            # shift rightmost first\n            self._A[j] = self._A[j-1]\n\n        # store newest element\n        self._A[k] = obj\n        self._n += 1\n\n    def remove(self, k, obj):\n        \"\"\"Remove value at index k, shifting subsequent values leftward\"\"\"\n\n        # we don't consider shrinking the dynamic array in this implementation\n        for k in range(self._n):\n            if self._A[k] == obj:\n                # found the object\n                for j in range(k, self._n - 1):\n                    # shift other values to fill the gap\n                    self._A[j] = self._A[j+1]\n                # garbage collection\n                self._A[self._n - 1] = None\n                # have one less item\n                self._n -= 1\n                return\n\n        raise ValueError('object / value not found')\n\n\n    def _resize(self, c):\n        \"\"\"Resize internal array to capacity c\"\"\"\n\n        # create new array\n        B = self._make_array(c)\n\n        # copy all existing values to new array\n        for k in range(self._n):\n            B[k] = self._A[k]\n\n        self._A = B\n        self._capacity = c\n\n    def _make_array(self, c):\n        \"\"\"Return new array with capacity c\"\"\"\n\n        return (c * ctypes.py_object)()\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/arrays.html#amortized-analysis-of-dynamic-arrays","title":"Amortized Analysis of Dynamic Arrays","text":"<p>It may seem that replacing an array with a new larger array might seem slow (because a single append operation may require O(n) time to perform), however notice that by doubling the capacity during an array replacement the new array allows us to add <code>n</code> new elements before the array must be replaced again.</p> <p>Therefore performing a series of operations on an initially empty dynamic array is efficient in terms of its total running time (or amortized time).</p> <p>So the amortized bound of appending items to a list can be shown to be <code>O(1)</code> - and this is also true for any geometrically increasing progression of array sizes.</p> <p>But arithmetic progression (i.e. a constant increase in cells of a dynamic array) does not perform as well - in fact it causes the append operation to be quadratic to the number of operations.</p> <p>Also geometric increase in capacity for dynamic arrays guarantees that the final array size is proportional to the overall number of elements - i.e. the data structure uses O(n) memory. However some considerations need to be made if repeated insertions and removals from the array are possible.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/arrays.html#efficiencies","title":"Efficiencies","text":"<p>Space Complexity: <code>O(n)</code></p> <p>Time Complexities:</p> <p></p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/arrays.html#pythonic-code-notes","title":"Pythonic Code Notes","text":"<ol> <li> <p>Extending a list - use <code>extend</code> instead of <code>append</code> when adding multiple new entries in an array. This is because <code>extend</code> will pre-compute the amount of resizing needed for the new data in advance.</p> </li> <li> <p>Constructing new lists - use list comprehensions instead of <code>append</code> - again due to similar reasons as above. However be careful in how the objects are referenced:</p> </li> </ol> <pre><code># this will create a reference to the same [0, 0] list!\nincorrect_2d_array = [[0] * 2] * 5\n# this will create reference to independent [0, 0] lists\ncorrect_2d_array = [[0] * 2 for j in range(5)]\n\nincorrect_2d_array[0][0] = 100\ncorrect_2d_array[0][0] = 100\n\nprint(f\"Incorrect 2D array: {incorrect_2d_array}\")\nprint(f\"Correct 2D array: {correct_2d_array}\")\n</code></pre> <pre><code>Incorrect 2D array: [[100, 0], [100, 0], [100, 0], [100, 0], [100, 0]]\nCorrect 2D array: [[100, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n</code></pre> <p>References</p> <ol> <li> <p>Skiena, Steven S.. (2008). The Algorithm Design Manual, 2<sup>nd</sup> ed. (2). : Springer Publishing Company.\u00a0\u21a9</p> </li> <li> <p>Goodrich, M. T., Tamassia, R., &amp; Goldwasser, M. H. (2013). Data Structures and Algorithms in Python (1<sup>st</sup> ed.). Wiley Publishing.\u00a0\u21a9</p> </li> <li> <p>Cormen, T. H., Leiserson, C. E., &amp; Rivest, R. L. (1990). Introduction to algorithms. Cambridge, Mass. : New York, MIT Press.\u00a0\u21a9</p> </li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/conflict%20free%20replicated%20data%20type.html","title":"CRDT: Conflict Free Replicated Data Type","text":"<p>TODO: Add notes here - leaving blank for now.</p> <p>Reference [^1] https://jakelazaroff.com/words/an-interactive-intro-to-crdts/</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/disjoint%20set%20union.html","title":"Disjoint Set Union","text":"<p>TODO: Add notes here - leaving blank for now.</p> <p>References</p> <ol> <li> <p>Skiena, Steven S.. (2008). The Algorithm Design Manual, 2<sup>nd</sup> ed. (2). : Springer Publishing Company.\u00a0\u21a9</p> </li> <li> <p>Goodrich, M. T., Tamassia, R., &amp; Goldwasser, M. H. (2013). Data Structures and Algorithms in Python (1<sup>st</sup> ed.). Wiley Publishing.\u00a0\u21a9</p> </li> <li> <p>Cormen, T. H., Leiserson, C. E., &amp; Rivest, R. L. (1990). Introduction to algorithms. Cambridge, Mass. : New York, MIT Press.\u00a0\u21a9</p> </li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/geospatial.html","title":"Geospatial","text":"<p>TODO: Add notes here - leaving blank for now.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/graphs.html","title":"Graphs","text":"<p>TODO: Add notes here - leaving blank for now.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/graphs.html#representations","title":"Representations","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/graphs.html#edge-list","title":"Edge List","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/graphs.html#adjacency-list","title":"Adjacency List","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/graphs.html#adjacency-matrix","title":"Adjacency Matrix","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/graphs.html#minimum-spanning-trees","title":"Minimum Spanning Trees","text":"<p>References</p> <ol> <li> <p>Skiena, Steven S.. (2008). The Algorithm Design Manual, 2<sup>nd</sup> ed. (2). : Springer Publishing Company.\u00a0\u21a9</p> </li> <li> <p>Goodrich, M. T., Tamassia, R., &amp; Goldwasser, M. H. (2013). Data Structures and Algorithms in Python (1<sup>st</sup> ed.). Wiley Publishing.\u00a0\u21a9</p> </li> <li> <p>Cormen, T. H., Leiserson, C. E., &amp; Rivest, R. L. (1990). Introduction to algorithms. Cambridge, Mass. : New York, MIT Press.\u00a0\u21a9</p> </li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/linked%20lists.html","title":"Linked Lists","text":"<p>There are some notable disadvantages of dynamic arrays like <code>list</code> in Python</p> <ol> <li>the length of the dynamic array might be longer than the actual number of elements it stores</li> <li>amortized bounds for operations may be unacceptable in real-time systems</li> <li>insertions and deletions at interior positions of an array are expensive</li> </ol> <p>Linked lists provide an alternative to array-based ordered sequences.</p> <p>While an array provides more centralized representation with one large chunk of memory capable of accommodating references to many elements, a linked list relies on a distributed representation using a lightweight object called a node for each element.</p> <p>Each node maintains a reference to its element and one or more references to neighboring nodes in order to collectively represent the linear order of the sequence.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/linked%20lists.html#trade-offs-between-linked-lists-and-arrays","title":"Trade-offs between Linked Lists and Arrays","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/linked%20lists.html#advantages-of-array-based-sequences","title":"Advantages of Array-Based Sequences","text":"<ol> <li>Arrays provide <code>O(1)</code> time access to an element based on an integer index. Linked lists requires <code>O(k)</code> time to traverse the list from the beginning or possibly end of a doubly linked list.</li> <li>Operations with equivalent asymptotic bounds typically run a constant factor more efficiently with an array-based structure versus a linked structure. For example appending a new element to a linked list requires instantiation of a node, appropriate linking of nodes, and an increment of an integer. While this operation completes in <code>O(1)</code> time the actual number of CPU operations will be more in the linked version.</li> <li>Array based representations typically use proportionally less memory than linked structures. A dynamic array may be, in the worst case, allocated memory for 2n object references. With linked lists however the memory must be devoted not only to store a reference to each contained object, but also explicit references that link the nodes. So a singly linked list of length n already requires 2n references, and a doubly linked list 3n references.</li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/linked%20lists.html#advantages-of-link-based-sequences","title":"Advantages of Link-based sequences","text":"<ol> <li>Link based structures provide worst-case time bounds for their operations. This is in contrast to amortized bounds associated with the expansion and contraction of a dynamic array. In real-time systems designed for more immediate responses, a long delay caused a single (amortized) operation may have an adverse effect. Worst-case bound gives a guarantee on the sum of the time spent on the individual operations.</li> <li>Link based structures support <code>O(1)</code> time insertions and deletions at arbitrary positions. This is perhaps the biggest advantage of linked lists.</li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/linked%20lists.html#singly-linked-list","title":"Singly Linked List","text":"<p>A collection of nodes that collectively form a linear sequence. Each node stores a reference to an object that is an element of the sequence, as well as a reference to the next node of the list.</p> <p>Each node is represented as a unique object, with that instance storing a reference to its element and a reference to the next node (or <code>None</code>). The linked list must keep a reference to the head of the list - without an explicit reference to the head, there would be no way to locate that node.</p> <p>Unfortunately we cannot easily delete the last node of a singly linke list. The only way to access the node before the last is to start from the head of the list - which is inefficient. To make this efficient we will need a Doubly Linked List list.</p> <pre><code>class LinkedNode:\n    __slots__ = '_element', '_next'\n\n    def __init__(self, element, next):\n        self._element = element\n        self._next = next\n\nclass LinkedList:\n\n    def __init__(self, head, tail):\n        self._head = head\n        self._tail = tail\n        self._size = size\n\n    def add_first(self, element):\n        node = LinkedNode(element, self._head)\n        self._head = node\n        self._size += 1\n\n    def add_last(self, element):\n        node = LinkedNode(element, None)\n        self._tail._next = node\n        self._tail = node\n        self._size += 1\n\n    def remove_first(self):\n        head = self._head\n        self._head = self._head._next\n        self._size -= 1\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/linked%20lists.html#efficiencies","title":"Efficiencies","text":"<p>Space Complexity: <code>O(n)</code></p> <p>Time Complexities:</p> <p></p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/linked%20lists.html#stack-using-a-singly-linked-list","title":"Stack using a Singly Linked List","text":"<p>Using a singly linked list, we implement a stack. Since for a linked list we can insert and delete elements in constant time only at the head, we will use the head as the <code>top</code> of the stack.</p> <p>When implementing the node we also intentionally define <code>__slots__</code> to streamline the memory usage as there may potentially be many node instances in a single list.</p> <pre><code>class Node:\n    __slots__ = '_element', '_next'\n\n    def __init__(self, element, next):\n        self._element = element\n        self._next = next\n\n\nclass LinkedStack:\n\n    def __init__(self):\n        self._head = None\n        self._size = 0\n\n    def __len__(self):\n        return self._size\n\n    def is_empty(self):\n        return self._size == 0\n\n    def push(self, e):\n        self._head = Node(e, self._head)\n        self._size += 1\n\n    def top(self):\n        if self.is_empty():\n            raise Empty('Stack is Empty')\n        return self._head._element\n\n    def pop(self):\n        if self.is_empty():\n            raise Empty('Stack is Empty')\n\n        answer = self._head._element\n\n        self._head = self._head._next\n        self._size -= 1\n        return answer\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/linked%20lists.html#queue-using-a-singly-linked-list","title":"Queue using a Singly Linked List","text":"<pre><code>class Node:\n    __slots__ = '_element', '_next'\n\n    def __init__(self, element, next):\n        self._element = element\n        self._next = next\n\n\nclass LinkedQueue:\n\n    def __init__(self):\n        self._head = None\n        self._tail = None\n        self._size = 0\n\n    def __len__(self):\n        return self._size\n\n    def is_empty(self):\n        return self._size == 0\n\n    def first(self):\n        if self.is_empty():\n            raise Empty('Queue is empty')\n        return self._head._element\n\n    def dequeue(self):\n        if self.is_empty():\n            raise Empty('Queue is empty')\n\n        answer = self._head._element\n        self._head = self._head._next\n        self._size -= 1\n        if self.is_empty():\n            self._tail = None\n\n        return answer\n\n    def enqueue(self, element):\n        newest = Node(element, None)\n        if self.is_empty():\n            self._head = newest\n        else:\n            self._tail._next = newest\n        self._tail = newest\n        self._size += 1\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/linked%20lists.html#circularly-linked-lists","title":"Circularly Linked Lists","text":"<p>Case where the <code>next</code> reference of the tail of the list circularly points back to the head of the list.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/linked%20lists.html#example-of-use","title":"Example of use","text":"<p>A Round-Robin scheduler iterates through a collection of elements in a circular fashion and \"services\" each element by performing a given action on it. These are usually used to fairly allocate a resource that must be shared by a collection of clients e.g. to allocate slices of CPU time to various applications running concurrently on a computer. </p> <p>Round robin queues can be implemented using a general queue with the following operations: 1. <code>current_process = queue.dequeue()</code> 2. Service <code>current_process</code> 3. <code>queue.enqueue(current_process)</code></p> <p>With a circular linked list implementation of a queue, we just have to incrementing a reference that marks the <code>head</code> and <code>tail</code> of the queue without needing to do any operations. </p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/linked%20lists.html#circular-queue-implemented","title":"Circular Queue implemented","text":"<pre><code>class Node:\n    __slots__ = '_element', '_next'\n\n    def __init__(self, element, next):\n        self._element = element\n        self._next = next\n\n\nclass LinkedQueue:\n\n    def __init__(self):\n        self._tail = None\n        self._size = 0\n\n    def __len__(self):\n        return self._size\n\n    def is_empty(self):\n        return self._size == 0\n\n    def first(self):\n        if self.is_empty():\n            raise Empty('Queue is empty')\n        head = self._tail._next\n        return head._element\n\n    def dequeue(self):\n        if self.is_empty():\n            raise Empty('Queue is empty')\n        oldhead = self._tail._next\n        if self._size == 1:\n            self._tail = None\n        else:\n            self._tail._next = oldhead._next\n        self._size -= 1\n        return oldhead._element\n\n    def enqueue(self, element):\n        newest = Node(element, None)\n        if self.is_empty():\n            newest._next = newest\n        else:\n            newest._next = self._tail._next\n            self._tail._next = newest\n        self._tail = newest\n        self._size += 1\n\n    def rotate(self):\n        if self._size &gt; 0:\n            self._tail = self._tail._next\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/linked%20lists.html#doubly-linked-list","title":"Doubly Linked List","text":"<p>Each node keeps an explicit reference to the node before it and a reference to the node after it. This is known as a doubly linked list. This allows for a greater variety of <code>O(1)</code> updates, inlcuding insertions and deletions at arbitrary positions within the list.</p> <p><code>header</code> and <code>trailer</code> sentinels - i.e. special \"dummy\" nodes - will be used as the head and tail of the list. By using just slightly extra space, our logic of operations greatly simplifies. </p> <pre><code>class Node:\n    __slots__ = '_element', '_prev', '_next'\n\n    def __init__(self, element, prev, next):\n        self._element = element\n        self._prev = prev\n        self._next = next\n\n\nclass DoublyLinkedList:\n\n    def __init__(self):\n        self._header = Node(None, None, None)\n        self._trailer = Node(None, None, None)\n        self._header._next = self._trailer\n        self._trailer._next = self._header\n        self._size = 0\n\n    def __len__(self):\n        return self._size\n\n    def is_empty(self):\n        return self._size == 0\n\n    def _insert_between(self, element, predecessor, successor):\n        newest = Node(element, predecessor, successor)\n        predecessor._next = newest\n        successor._prev = newest\n\n        self._size += 1\n        return newest\n\n    def _delete_node(self, node):\n        pred = node._prev\n        succ = node._next\n        pred._next = succ\n        succ._prev = pred\n        self._size -= 1\n        element = node._element\n        node._prev = node._next = node._element = None\n        return element\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/linked%20lists.html#implementing-a-dequeue-using-a-doubly-linked-list","title":"Implementing a Dequeue using a Doubly Linked List","text":"<pre><code>class LinkedDequeue(DoublyLinkedBase):\n\n    def first(self):\n        if self.is_empty():\n            raise Empty('Dequeue is empty')\n\n        return self._header._next._element\n\n    def last(self):\n        if self.is_empty():\n            raise Empty('Dequeue is empty')\n        return self._trailer._prev._element\n\n    def insert_first(self, element):\n        self._insert_between(element, self._header, self._header._next)\n\n    def insert_last(self, element):\n        self._insert_between(element, self._trailer._prev, self.trailer)\n\n    def delete_first(self):\n        if self.is_empty():\n            raise Empty('Dequeue is empty')\n        return self._delete_node(self._header._next)\n\n    def delete_last(self):\n        if self.is_empty():\n            raise Empty('Dequeue is empty')\n        return self._delete_node(self._trailer._prev)\n</code></pre> <p>References</p> <ol> <li> <p>Skiena, Steven S.. (2008). The Algorithm Design Manual, 2<sup>nd</sup> ed. (2). : Springer Publishing Company.\u00a0\u21a9</p> </li> <li> <p>Goodrich, M. T., Tamassia, R., &amp; Goldwasser, M. H. (2013). Data Structures and Algorithms in Python (1<sup>st</sup> ed.). Wiley Publishing.\u00a0\u21a9</p> </li> <li> <p>Cormen, T. H., Leiserson, C. E., &amp; Rivest, R. L. (1990). Introduction to algorithms. Cambridge, Mass. : New York, MIT Press.\u00a0\u21a9</p> </li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/maps.html","title":"Maps, Hash Tables, Skip Lists and Sets","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/maps.html#maps-and-dictionaries","title":"Maps and Dictionaries","text":"<p>Where unique keys are mapped to associated vaLues, and the relationship they express are commonly known as associative arrays or maps. Maps or dictionaries are implemented so that a search for a key, and its associated value, can be performed very efficiently.</p> <p>In Python the <code>dict</code> class provides this functionality.</p> <p>However Python also provides two abstract base class in the <code>collections</code> module that are relevant: <code>Mapping</code> and <code>MutableMapping</code>. <code>Mapping</code> includes all nonmutating methods supported by Python's <code>dict</code> class, and <code>MutableMapping</code> class extends that to include the mutating methods.</p> <pre><code>from collections import MutableMapping\n\nclass MapBase(MutableMapping):\n  \"\"\"Our own abstract base class that includes a nonpublic _Item class.\"\"\"\n\n  #------------------------------- nested _Item class -------------------------------\n  class _Item:\n    \"\"\"Lightweight composite to store key-value pairs as map items.\"\"\"\n    __slots__ = '_key', '_value'\n\n    def __init__(self, k, v):\n      self._key = k\n      self._value = v\n\n    def __eq__(self, other):\n      return self._key == other._key   # compare items based on their keys\n\n    def __ne__(self, other):\n      return not (self == other)       # opposite of __eq__\n\n    def __lt__(self, other):\n      return self._key &lt; other._key    # compare items based on their keys\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/maps.html#hash-tables","title":"Hash Tables","text":"<p>Python's implementation of the <code>dict</code> class is a hash table - it uses a hash function to map general keys to corresponding indices in a table. Ideally the keys will be well distributed in the range from 0 to N-1 by a hash function, but in practice there may be two or more distinct keys that get mapped to the same index.</p> <p>Thus we will conceptualize the hash table as a bucket array in which each bucket may manage a collection of items that are sent to a specific index by the hash function.</p> <pre><code>class HashMapBase(MapBase):\n  \"\"\"Abstract base class for map using hash-table with MAD compression.\n\n  Keys must be hashable and non-None.\n  \"\"\"\n\n  def __init__(self, cap=11, p=109345121):\n    \"\"\"Create an empty hash-table map.\n\n    cap     initial table size (default 11)\n    p       positive prime used for MAD (default 109345121)\n    \"\"\"\n    self._table = cap * [ None ]\n    self._n = 0                                   # number of entries in the map\n    self._prime = p                               # prime for MAD compression\n    self._scale = 1 + randrange(p-1)              # scale from 1 to p-1 for MAD\n    self._shift = randrange(p)                    # shift from 0 to p-1 for MAD\n\n  def _hash_function(self, k):\n    return (hash(k)*self._scale + self._shift) % self._prime % len(self._table)\n\n  def __len__(self):\n    return self._n\n\n  def __getitem__(self, k):\n    j = self._hash_function(k)\n    return self._bucket_getitem(j, k)             # may raise KeyError\n\n  def __setitem__(self, k, v):\n    j = self._hash_function(k)\n    self._bucket_setitem(j, k, v)                 # subroutine maintains self._n\n    if self._n &gt; len(self._table) // 2:           # keep load factor &lt;= 0.5\n      self._resize(2 * len(self._table) - 1)      # number 2^x - 1 is often prime\n\n  def __delitem__(self, k):\n    j = self._hash_function(k)\n    self._bucket_delitem(j, k)                    # may raise KeyError\n    self._n -= 1\n\n  def _resize(self, c):\n    \"\"\"Resize bucket array to capacity c and rehash all items.\"\"\"\n    old = list(self.items())       # use iteration to record existing items\n    self._table = c * [None]       # then reset table to desired capacity\n    self._n = 0                    # n recomputed during subsequent adds\n    for (k,v) in old:\n      self[k] = v                  # reinsert old key-value pair\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/maps.html#hash-functions","title":"Hash Functions","text":"<p>A hash function maps a key <code>k</code> to an integer in the range <code>[0,N-1]</code>, where <code>N</code> is the capacity of the bucket array for a hash table. Such a hash function is used to index into a bucket array.</p> <p>If there are two or more keys with the same hash value, then two different items will be mapped to the same bucket in A. This is a collision - and there are strategies to deal with them. But it is best to avoid or minimize them.</p> <p>The hash function also needs to be fast and easy to compute.</p> <p>The hash function is also thought of being split into two components: 1) hash code that maps a key <code>k</code> to an integer and 2) compression function tat maps the hash code to an integer within a range of indices.</p> <p>This separation allows hash coding to happen independently of the specific hash table size - so the hash table may be dynamically resized easily.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/maps.html#hash-codes","title":"Hash Codes","text":"<p>The resulting integers may even be negative.</p> <code>Polynomial Hash Code</code> <p>Useful for string typed keys. A string is a sequence of characters encoded using the ASCII integer code. Example: \"H e l l o\" is <code>72 101 108 108 111</code>.</p> <p>So given a string \\(s=x_{k-1}x_{k-2}...x_{0}\\), the hash code is \\(h(s)=x_{k-1}a^{k-1}+x_{k-1}a^{k-2}+...+x_0a^{0}\\).</p> <p>The choice of the value of <code>a</code> is important for the spread of the hash value. Emperically \"good\" values are <code>33, 37, 39, 41</code>.</p> <code>Cyclic Shift Hash Code</code> Same as the polynomial hash coed but using the <code>shift operation</code> to perform multiplication. <p>In Python the built-in function for hashing is <code>hash(x)</code> that returns an integer value that serves as the hash code for object <code>x</code>. However only immutable data types are deemed hashable in Python. This is to ensure that a particular object's hash code remains constant during the object's lifespan.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/maps.html#compression-functions","title":"Compression Functions","text":"<p>A good compression function minimizes the number of collisions for a given set of distinct hash codes.</p> <code>Division Method</code> <p>Maps an integer <code>i</code> to <code>i mod N</code></p> <p>Here <code>N</code> is the size of the bucket array - a fixed positive integer.</p> <p>If <code>N</code> is a prime number then this compression function helps \"spread out\" the distribution of hashed values. Actually if <code>N</code> is not prime, then there is greater risk that patterns in the distribution of hash codes will be repeated in the distribution of hash values - causing collisions.</p> <code>The MAD Method</code> <p>Helps eliminate repeated patterns in a set of integer keys - Multiply-Add-and-Divide - where an integer <code>i</code> is mapped to: <code>[(ai+b) mod p] mod N</code></p> <p>Where <code>N</code> is the size of the bucket array, <code>p</code> i the prime number larger than <code>N</code>, and <code>a</code> and <code>b</code> are integers chosen at random from interval <code>[0, p-1]</code> with <code>a&gt;0</code>.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/maps.html#collision-handling","title":"Collision Handling","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/maps.html#separate-chaining","title":"Separate Chaining","text":"<p>Have each bucket <code>A[j]</code> store its own secondary container, holding items <code>k,v</code> such that <code>h(k)=j</code>.</p> <p>The secondary container can be a small map instance implemented using a list. Worst case - operations on an individual bucket take time proportional to the size of the bucket.</p> <p>Assuming we use a good hash function to index the <code>n</code> items of our map in a bucket array of capacity <code>N</code>, the expected size of a bucket i <code>n/N</code>. Therefore the core map operations run in <code>O[n/N]</code> time - the ration <code>lambda = n/N</code> is called the load factor. This usually below 1 - and so the core operations on the has table run in <code>O(1)</code> expected time.</p> <pre><code>class ChainHashMap(HashMapBase):\n  \"\"\"Hash map implemented with separate chaining for collision resolution.\"\"\"\n\n  def _bucket_getitem(self, j, k):\n    bucket = self._table[j]\n    if bucket is None:\n      raise KeyError('Key Error: ' + repr(k))        # no match found\n    return bucket[k]                                 # may raise KeyError\n\n  def _bucket_setitem(self, j, k, v):\n    if self._table[j] is None:\n      self._table[j] = UnsortedTableMap()     # bucket is new to the table\n    oldsize = len(self._table[j])\n    self._table[j][k] = v\n    if len(self._table[j]) &gt; oldsize:         # key was new to the table\n      self._n += 1                            # increase overall map size\n\n  def _bucket_delitem(self, j, k):\n    bucket = self._table[j]\n    if bucket is None:\n      raise KeyError('Key Error: ' + repr(k))        # no match found\n    del bucket[k]                                    # may raise KeyError\n\n  def __iter__(self):\n    for bucket in self._table:\n      if bucket is not None:                         # a nonempty slot\n        for key in bucket:\n          yield key\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/maps.html#open-addressing","title":"Open Addressing","text":"<p>TODO: Add more notes here to describe this clearly</p> <p>Once a collision takes place, open addressing (also known as closed hashing) computes new positions using a probe sequence and the next record is stored in that position. There are some well-known probe sequences:</p> <ul> <li>Linear Probing: The interval between the probes is fixed to 1. This means that the very next available position in the table would be tried.</li> <li>Quadratic Probing: The interval between the probes increases quadratically. This means that the next available position that would be tried would increase quadratically.</li> <li>Double Hashing: The interval between probes is fixed for each record but the hash is computed again by double hashing.</li> </ul> <pre><code>class ProbeHashMap(HashMapBase):\n  \"\"\"Hash map implemented with linear probing for collision resolution.\"\"\"\n  _AVAIL = object()       # sentinal marks locations of previous deletions\n\n  def _is_available(self, j):\n    \"\"\"Return True if index j is available in table.\"\"\"\n    return self._table[j] is None or self._table[j] is ProbeHashMap._AVAIL\n\n  def _find_slot(self, j, k):\n    \"\"\"Search for key k in bucket at index j.\n\n    Return (success, index) tuple, described as follows:\n    If match was found, success is True and index denotes its location.\n    If no match found, success is False and index denotes first available slot.\n    \"\"\"\n    firstAvail = None\n    while True:                               \n      if self._is_available(j):\n        if firstAvail is None:\n          firstAvail = j                      # mark this as first avail\n        if self._table[j] is None:\n          return (False, firstAvail)          # search has failed\n      elif k == self._table[j]._key:\n        return (True, j)                      # found a match\n      j = (j + 1) % len(self._table)          # keep looking (cyclically)\n\n  def _bucket_getitem(self, j, k):\n    found, s = self._find_slot(j, k)\n    if not found:\n      raise KeyError('Key Error: ' + repr(k))        # no match found\n    return self._table[s]._value\n\n  def _bucket_setitem(self, j, k, v):\n    found, s = self._find_slot(j, k)\n    if not found:\n      self._table[s] = self._Item(k,v)               # insert new item\n      self._n += 1                                   # size has increased\n    else:\n      self._table[s]._value = v                      # overwrite existing\n\n  def _bucket_delitem(self, j, k):\n    found, s = self._find_slot(j, k)\n    if not found:\n      raise KeyError('Key Error: ' + repr(k))        # no match found\n    self._table[s] = ProbeHashMap._AVAIL             # mark as vacated\n\n  def __iter__(self):\n    for j in range(len(self._table)):                # scan entire table\n      if not self._is_available(j):\n        yield self._table[j]._key\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/maps.html#sorted-maps","title":"Sorted Maps","text":"<p>A map does not provide any way to get a list of all keys ordered by some comparison - in fact the hash-based implementation relies on the intentionally scattered keys to make them more uniformly dstirbuted in a hash table.</p> <p>Data structures that implement a <code>sorted map</code> are also called <code>Sorted Search Tables</code>. </p> <p>TODO: Add notes here - leaving blank for now.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/maps.html#applications-of-sorted-maps","title":"Applications of Sorted Maps","text":"<p>TODO: Add notes here - leaving blank for now.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/maps.html#skip-lists","title":"Skip Lists","text":"<p>TODO: Add notes here - leaving blank for now.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/maps.html#sets-multisets-and-multimaps","title":"Sets, Multisets, and Multimaps","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/maps.html#sets","title":"Sets","text":"<p>Simply a map in which the keys do not have associated values. In python base classes <code>MutableSet</code> is available from the <code>collections</code> module.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/maps.html#multisets","title":"Multisets","text":"<p>The same element may occur several times in a multiset. One way to implement this is to have a count of the number of occurrences of an element within a multiset. </p> <p>In Python the <code>Counter</code> class in <code>collections</code> offers the capability. </p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/maps.html#multimaps","title":"Multimaps","text":"<p>Similar to a traditional map, in that it associates values with keys, however in a multimap the same key can be mapped to multiple values. </p> <p>Python does not have a native implementation of this.</p> <pre><code>class MultiMap:\n  \"\"\"\n  A multimap class built upon use of an underlying map for storage.\n\n  This uses dict for default storage.\n\n  Subclasses can override class variable _MapType to change the default.\n  That catalog class must have a default constructor that produces an empty map.\n  As an example, one might define the following subclass to use a SortedTableMap\n\n  class SortedTableMultimap(MultiMap):\n    _MapType = SortedTableMap\n  \"\"\"\n  _MapType = dict                 # Map type; can be redefined by subclass\n\n  def __init__(self):\n    \"\"\"Create a new empty multimap instance.\"\"\"\n    self._map = self._MapType()          # create map instance for storage\n    self._n = 0\n\n  def __len__(self):\n    \"\"\"Return number of (k,v) pairs in multimap.\"\"\"\n    return self._n\n\n  def __iter__(self):\n    \"\"\"Iterate through all (k,v) pairs in multimap.\"\"\"\n    for k,secondary in self._map.items():\n      for v in secondary:\n        yield (k,v)\n\n  def add(self, k, v):\n    \"\"\"Add pair (k,v) to multimap.\"\"\"\n    container = self._map.setdefault(k, [])     # create empty list, if needed\n    container.append(v)\n    self._n += 1\n\n  def pop(self, k):\n    \"\"\"Remove and return arbitrary (k,v) pair with key k (or raise KeyError).\"\"\"\n    secondary = self._map[k]                    # may raise KeyError\n    v = secondary.pop()\n    if len(secondary) == 0:\n      del self._map[k]                          # no pairs left\n    self._n -= 1\n    return (k, v)\n\n  def find(self, k):\n    \"\"\"Return arbitrary (k,v) pair with given key (or raise KeyError).\"\"\"\n    secondary = self._map[k]                    # may raise KeyError\n    return (k, secondary[0])\n\n  def find_all(self, k):\n    \"\"\"Generate iteration of all (k,v) pairs with given key.\"\"\"\n    secondary = self._map.get(k)\n</code></pre> <p>References</p> <ol> <li> <p>Skiena, Steven S.. (2008). The Algorithm Design Manual, 2<sup>nd</sup> ed. (2). : Springer Publishing Company.\u00a0\u21a9</p> </li> <li> <p>Goodrich, M. T., Tamassia, R., &amp; Goldwasser, M. H. (2013). Data Structures and Algorithms in Python (1<sup>st</sup> ed.). Wiley Publishing.\u00a0\u21a9</p> </li> <li> <p>Cormen, T. H., Leiserson, C. E., &amp; Rivest, R. L. (1990). Introduction to algorithms. Cambridge, Mass. : New York, MIT Press.\u00a0\u21a9</p> </li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/positional.html","title":"Positional","text":"<p>Stacks, queues, and double ended queues only allow update operations that occur at one end of a sequence or the other. But these are too limiting - we want to be able to refer to elements anywhere in a sequence, and to perform arbitrary insertions and deletions.</p> <p>Using indices is not preffered as: 1. linked lists don't allow access through indexing without traversing the list incrementally from the begening or end 2. it is not convenient to describe the location of an object purely by reference to the beginning or end of the data structure</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/positional.html#position-abstraction","title":"Position Abstraction","text":"<p>Two ADTS: 1. <code>Postion</code> - A marker or token within the broader positional list. A position <code>p</code> is unaffected by changes elsewhere in a list; the only way in which a position becomes invalid is if an explicit command is issued to delete it. 2. <code>PositionalList</code> - serves as a container of sequential elements that allow positional access.</p> <p>In the implementation below refer to the Doubly Linked List class defined in Linked Lists: Doubly Linked List</p> <pre><code>class Position\n\n    def __init__(self, container, node):\n        self._container = container\n        self._node = node\n\n    def element(self):\n        return self._node._element\n\n    def __eq__(self, other):\n        return type(other) is type(self) and other._node is self._node\n\n    def __ne__(self, other):\n        return not (self == other)\n\nclass PositionalList(DoublyLinkedBase):\n\n    def _validate(self, position):\n        if not isinstance(position, self.Position):\n            raise TypeError('p must be proper Position type')\n        if p._container is not self:\n            raise ValueError('p does not belong to this container')\n        if p._node._next is None:\n            raise ValueError('p is no longer valid')\n        return p._node\n\n    def _make_position(self, node):\n        if node is self._header or node is self._trailer:\n            return None\n        else:\n            return self.Position(self, node)\n\n    def first(self):\n        return self._make_position(self._header._next)\n\n    def last(self):\n        return self._make_position(self._trailer._prev)\n\n    def before(self, position):\n        node = self._validate(position)\n        return self._make_position(node._prev)\n\n    def after(self, position):\n        node = self._validate(position)\n        return self._make_position(node._next)\n\n    def __iter__(self):\n        cursor = self.first()\n        while cursor is not None:\n            yield cursor.element()\n            cursor = self.after(cursor)\n\n    def _insert_between(self, element, predecessor, successor):\n        node = super()._insert_between(element, predecessor, successor)\n        return self._make_position(node)\n\n    def add_first(self, element):\n        return self._insert_between(element, self._header, self._header._next)\n\n    def add_last(self, element):\n        return self._insert_between(element, self._trailer._prev, self._trailer)\n\n    def add_before(self, position, element):\n        original = self._validate(position)\n        return self._insert_between(element, original._prev, original)\n\n    def add_after(self, position, element):\n        original = self._validate(position)\n        return self._insert_between(element, original, original._next)\n\n    def delete(self, postion):\n        original = self._validate(position)\n        return self._delete_node(original)\n\n    def replace(self, position, element):\n        original = self._validate(position)\n        old_value = original._element\n        original._element = element\n        return old_value\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/positional.html#sorting-a-positional-list","title":"Sorting a Positional List","text":"<p>We can implement Insertion Sort for a <code>PositionalList</code>.</p> <pre><code>def insertion_sort(pos_list):\n    # no need to sort an list of 1\n    if len(pos_list) &gt; 1:\n\n        marker = pos_list.first()\n\n        while marker != pos_list.last():\n            # get the next item to compare against\n            pivot = pos_list.after(marker)\n            value = pivot.element()\n            # pivot already sorted\n            if value &gt; marker.element():\n                # pivot becomes the leftmost new marker\n                marker = pivot\n            # need to sort pivot \n            else:\n                # find leftmost item greater than value\n                walk = marker\n                while walk != pos_list.first() and pos_list.before(walk).element() &gt; value:\n                    walk = pos_list.before(walk)\n\n                pos_list.delete(pivot)\n                # reinsert value before walk\n                pos_list.add_before(walk, value)\n</code></pre> <p>References</p> <ol> <li> <p>Skiena, Steven S.. (2008). The Algorithm Design Manual, 2<sup>nd</sup> ed. (2). : Springer Publishing Company.\u00a0\u21a9</p> </li> <li> <p>Goodrich, M. T., Tamassia, R., &amp; Goldwasser, M. H. (2013). Data Structures and Algorithms in Python (1<sup>st</sup> ed.). Wiley Publishing.\u00a0\u21a9</p> </li> <li> <p>Cormen, T. H., Leiserson, C. E., &amp; Rivest, R. L. (1990). Introduction to algorithms. Cambridge, Mass. : New York, MIT Press.\u00a0\u21a9</p> </li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/priority%20queues.html","title":"Priority Queues and Heaps","text":"<p>A collection of prioritized elements that allow arbitrary element insertion, and allows the removal of the elmenet that has first priority. Priority is provided by an associated key - and the element with the minimum key will be the next to be removed from the queue. The key must define a natural order i.e. <code>a &lt; b</code> for any instance <code>a</code> and <code>b</code>. </p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/priority%20queues.html#priority-queue-adt","title":"Priority Queue ADT","text":"<pre><code>from abc import ABC, abstractmethod\n\nclass PriorityQueueBase:\n    class _Item:\n        __slots__ = '_key', '_value'\n\n        def __init__(self, k, v):\n            self._key = k\n            self._value = v\n\n        def __lt__(self, other):\n            return self._key &lt; other._key\n\n        def __repr__(self):\n            return '({0},{1})'.format(self._key, self._value)\n\n    def is_empty(self):\n        return len(self) == 0\n\n    def __len__(self):\n        raise NotImplementedError('must be implemented by subclass')\n\n    @abstractmethod\n    def add(self, key, value):\n        raise NotImplementedError('must be implemented by subclass')\n\n    @abstractmethod\n    def min(self):\n        raise NotImplementedError('must be implemented by subclass')\n\n    @abstractmethod\n    def remove_min(self):\n        raise NotImplementedError('must be implemented by subclass')\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/priority%20queues.html#implementation-with-unsorted-list","title":"Implementation with Unsorted List","text":"<p>In this we store entries within an unsorted list using a doubly-linked list (<code>PositionalList</code>) defined from </p> <p>The doubly linked list ensures all of the operations execute in <code>O(1)</code> time except for <code>min</code> and <code>_find_min</code> which would take <code>O(n)</code></p> <pre><code>class UnsortedPriorityQueue(PriorityQueueBase):\n    def _find_min(self):\n        if self.is_empty():\n            raise Empty('Priority queue is empty')\n        small = self._data.first()\n        walk = self._data.after(small)\n        while walk is not None:\n            if walk.element() &lt; small.element():\n                small = walk\n            walk = self._data.after(walk)\n        return small\n\n    def __init__(self):\n        self._data = PositionalList()\n\n    def __len__(self):\n        return len(self._data)\n\n    def add(self, key, value):\n        self._data.add_last(self._Item(key, value))\n\n    def min(self):\n        p = self._find_min()\n        item = p.element()\n        return (item._key, item._value)\n\n    def remove_min(self):\n        p = self._find_min()\n        item = self._data.delete(p)\n        return (item._key, item._value)\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/priority%20queues.html#implementation-with-a-sorted-list","title":"Implementation with a Sorted List","text":"<p>A alternative is to use a positional list but maintaining the entries sorted by non-decreasing keys. But in this case the insertion of a new entry has a cost of <code>O(n)</code> using insertion sort.</p> <p>But <code>min</code> and <code>remove_min</code> take oly <code>O(1)</code> time now.</p> <pre><code>class SortedPriorityQueue(PriorityQueueBase):\n    def __init__(self):\n        self._data = PositionalList()\n\n    def __len__(self):\n        return len(self._data)\n\n    def add(self, key, value):\n        newest = self._Item(key, value)\n        walk = self._data.last()\n        while walk is not None and newest &lt; walk.element():\n            walk = self._data.before(walk)\n        if walk is None:\n            self._data.add_first(newest)\n        else:\n            self._data.add_after(walk, newest)\n\n    def min(self):\n        if self.is_empty():\n            raise Empty('Priority queue is empty.')\n        p = self._data.first()\n        item = p.element()\n        return (item._key, item._value)\n\n    def remove_min(self):\n        if self.is_empty():\n            raise Empty('Priority queue is empty.')\n        item = self._data.delete(self._data.first())\n        return (item._key, item._value)\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/priority%20queues.html#heaps","title":"Heaps","text":"<p>A data structure that provides an efficient realization of a priority queue - both insertions and removals are in logarithmic time - which is a significant improvement over the list based implementations.</p> <p>A binary heap uses a binary tree which helps find a compromise between elements being entirely unsorted and perfectly sorted.</p> <p>In python this is implemented as a <code>heapq</code>.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/priority%20queues.html#properties","title":"Properties","text":"<ul> <li>Stores a collection of items at its positions</li> <li>Heap Order Property - for every position <code>p</code> other than the root, the key stored at <code>p</code> is greater than or equal to the key stored at <code>p's</code> parent<ul> <li>Point 2. means that the minimum key is always stored at the root of the heap. Keys encountered on a path from the root to the leaf of a heap are in nondecreasing order.</li> </ul> </li> <li>Complete Binary Tree Property - a heap with height <code>h</code> is complete</li> <li>Height of a Heap - given that it is complete, <code>height h = floor(log n)</code></li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/priority%20queues.html#array-based-representation","title":"Array based representation","text":"<p>The array-based representation of the complete binary tree are based on the properties:</p> <ol> <li>If <code>p</code> is the root of <code>T</code> then <code>f(p) = 0</code></li> <li>If <code>p</code> is the left child of position <code>q</code>, then <code>f(p) = 2f(q) + 1</code></li> <li>If <code>p</code> is the right child of position <code>q</code>, then <code>f(p) = 2f(q) + 2</code></li> </ol> <p>With this implementation, the last position of the heap is always at index <code>n-1</code>, where n is the number of positions of <code>T</code>. This reduces the complexities of a node-based tree structure and heap operations (e.g. locating the last position of a complete binary tree).</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/priority%20queues.html#implementation","title":"Implementation","text":"<p>Operations on a heap need to maintain its complete binary tree properties.</p> <p>Key operations of a heap:</p> <ol> <li>add(k,v) - adding a key, value pair at position <code>p</code> just beyond the right most node, and then <code>up-heap bubbling</code> to maintain heap-order property</li> <li>remove_min() - remove and return (k,v) tuple with minimum keyalong with <code>down-heap bubbling</code> after removal.</li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/priority%20queues.html#up-heap-bubbling","title":"Up-Heap Bubbling","text":"<p>Once a new <code>(k,v)</code> pair has been added to the rightmost node at the bottom level (or new level) of the tree, a comparison of the key at position <code>p</code> to that of the parent of <code>p</code> is done until heap-order property is satisfied. </p> <p>Worst case scenario, a new entry moves all the way to the root of the heap with a bound of <code>floor(log(n))</code>.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/priority%20queues.html#down-heap-bubbling","title":"Down-Heap Bubbling","text":"<p>When the minimumn is removed as a root of the tree, the leaf at the last position of the tree is copied to the root, and the node at the last position is deleted.</p> <p>If the heap has more than one node (the root) then we identify two cases where <code>p</code> initially denotes the root of the tree <code>T</code>:</p> <ul> <li>if <code>p</code> has no right child, let <code>c</code> be the left child of <code>p</code></li> <li>other (<code>p</code> has both children) then let <code>c</code> be a child of <code>p</code> with minimal key</li> </ul> <p>Until \\(k_p &gt; k_c\\) swap the entries stored at <code>p</code> and <code>c</code> until the heap-order property is satisfied for both <code>p</code> and all of <code>c</code>. This means that even though the heap-order property may be locally restored for node <code>p</code> relative to its children, there may be a violation of this property at <code>c</code>, hence we have to continue swapping down <code>T</code> until no violation of the heap-order property occurs.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/priority%20queues.html#python-implementation","title":"Python Implementation","text":"<pre><code>class HeapPriorityQueue(PriorityQueueBase):\n\n  def __init__(self):\n    self._data = []\n\n  def _parent(self, j):\n    return (j-1) // 2\n\n  def _left(self, j):\n    return 2*j + 1\n\n  def _right(self, j):\n    return 2*j + 2\n\n  def _has_left(self, j):\n    return self._left(j) &lt; len(self._data)\n\n  def _has_right(self, j):\n    return self._right(j) &lt; len(self._data)\n\n  def _swap(self, i, j):\n    self._data[i], self._data[j] = self._data[j], self._data[i]\n\n  def _upheap(self, j):\n    parent = self._parent(j)\n    if j &gt; 0 and self._data[j] &lt; self._data[parent]:\n      self._swap(j, parent)\n      self._upheap(parent)\n\n  def _downheap(self, j):\n    if self._has_left(j):\n      left = self._left(j)\n      small_child = left\n      if self._has_right(j):\n        right = self._right(j)\n        if self._data[right] &lt; self._data[left]:\n          small_child = right\n      if self._data[small_child] &lt; self._data[j]:\n        self._swap(j, small_child)\n        self._downheap(small_child)\n\n  def __len__(self):\n    return len(self._data)\n\n  def add(self, key, value):\n    self._data.append(self._Item(key, value))\n    self._upheap(len(self._data) - 1)\n\n  def min(self):\n    if self.is_empty():\n      raise Empty('Priority queue is empty.')\n    item = self._data[0]\n    return (item._key, item._value)\n\n  def remove_min(self):\n    if self.is_empty():\n      raise Empty('Priority queue is empty.')\n    self._swap(0, len(self._data) - 1)\n    item = self._data.pop()\n    self._downheap(0)\n    return (item._key, item._value)\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/priority%20queues.html#applications","title":"Applications","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/priority%20queues.html#sorting","title":"Sorting","text":"<ul> <li>Selction Sort</li> <li>Insertion Sort</li> <li>Heap Sort</li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/priority%20queues.html#adaptable-priority-queues","title":"Adaptable Priority Queues","text":"<p>Adaptable priority queues extends the base implementation to additional methods:</p> <ul> <li>remove(loc) ability to remove any node (not just the min) from the heap</li> <li>update(loc, k, v) ability to modify the priority of an existing node</li> </ul> <p>To support this we will implement a locator - similar to <code>Position</code> but with one major difference - locator does not represent a tangible placement of an element within the structure.</p> <pre><code>class AdaptableHeapPriorityQueue(HeapPriorityQueue):\n  \"\"\"A locator-based priority queue implemented with a binary heap.\"\"\"\n\n  #------------------------------ nested Locator class ------------------------------\n  class Locator(HeapPriorityQueue._Item):\n    \"\"\"Token for locating an entry of the priority queue.\"\"\"\n    __slots__ = '_index'                 # add index as additional field\n\n    def __init__(self, k, v, j):\n      super().__init__(k,v)\n      self._index = j\n\n  #------------------------------ nonpublic behaviors ------------------------------\n  # override swap to record new indices\n  def _swap(self, i, j):\n    super()._swap(i,j)                   # perform the swap\n    self._data[i]._index = i             # reset locator index (post-swap)\n    self._data[j]._index = j             # reset locator index (post-swap)\n\n  def _bubble(self, j):\n    if j &gt; 0 and self._data[j] &lt; self._data[self._parent(j)]:\n      self._upheap(j)\n    else:\n      self._downheap(j)\n\n  #------------------------------ public behaviors ------------------------------\n  def add(self, key, value):\n    \"\"\"Add a key-value pair.\"\"\"\n    token = self.Locator(key, value, len(self._data)) # initiaize locator index\n    self._data.append(token)\n    self._upheap(len(self._data) - 1)\n    return token\n\n  def update(self, loc, newkey, newval):\n    \"\"\"Update the key and value for the entry identified by Locator loc.\"\"\"\n    j = loc._index\n    if not (0 &lt;= j &lt; len(self) and self._data[j] is loc):\n      raise ValueError('Invalid locator')\n    loc._key = newkey\n    loc._value = newval\n    self._bubble(j)\n\n  def remove(self, loc):\n    \"\"\"Remove and return the (k,v) pair identified by Locator loc.\"\"\"\n    j = loc._index\n    if not (0 &lt;= j &lt; len(self) and self._data[j] is loc):\n      raise ValueError('Invalid locator')\n    if j == len(self) - 1:                # item at last position\n      self._data.pop()                    # just remove it\n    else:\n      self._swap(j, len(self)-1)          # swap item to the last position\n      self._data.pop()                    # remove it from the list\n      self._bubble(j)                     # fix item displaced by the swap\n    return (loc._key, loc._value)             \n</code></pre> <p>References</p> <ol> <li> <p>Skiena, Steven S.. (2008). The Algorithm Design Manual, 2<sup>nd</sup> ed. (2). : Springer Publishing Company.\u00a0\u21a9</p> </li> <li> <p>Goodrich, M. T., Tamassia, R., &amp; Goldwasser, M. H. (2013). Data Structures and Algorithms in Python (1<sup>st</sup> ed.). Wiley Publishing.\u00a0\u21a9</p> </li> <li> <p>Cormen, T. H., Leiserson, C. E., &amp; Rivest, R. L. (1990). Introduction to algorithms. Cambridge, Mass. : New York, MIT Press.\u00a0\u21a9</p> </li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/queues%20and%20deques.html","title":"Queues and Deques","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/queues%20and%20deques.html#queues","title":"Queues","text":"<p>A <code>queue</code> is a collection of objects that are inserted and removed according to the <code>first-in, first-out (FIFO)</code> principle. Elements can be inserted at any time, but only the elements that has been in the queue the longest can be next removed.</p> <p>Some common methods supported by a queue are:</p> <ul> <li><code>queue.enqueue(e)</code> - add an element <code>e</code> to the back of a queue</li> <li><code>queue.dequeue()</code> - remove and return the first element from a stack</li> <li><code>queue.first()</code> - return the first element of a stack</li> <li><code>queue.is_empty()</code> - check if queue is empty</li> <li><code>len(queue)</code> - the number of elements in a queue</li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/queues%20and%20deques.html#array-based-queue-implementation","title":"Array-Based Queue Implementation","text":"<p>Utilizing a <code>list</code> for a queue would be very inefficient - this is because <code>pop(0)</code> i.e. a <code>pop</code> on a non-default index causes a loop to be executed that shifts all elements beyon the index to te left. This would cause the worst-case behavior of O(n) time.</p> <p>Therefore we need to implement a queue using a circular array with the following logic:</p> <ol> <li>initialize a <code>list</code> with a fixed size that is larger than the actual number of elements t be ever added tot he queue</li> <li>define <code>front</code> as the index of the \"first\" element of a queue</li> <li>enqueue items to the index calculated by <code>(front + size) % capacity</code> of the queue</li> <li>for example for a queue of size 3, enqueue of [2, 1, 4] would be built as following: [2, None, None] -&gt; [2, 1, None] -&gt; [2, 1, 4]</li> <li>then following dequeue operations will happen like so: [None, 1, 4] and front = 1 -&gt; [None, None, 4] and front = 2 etc.</li> <li>whenever the size of the queue is equal to its capacity, resize it to be double the capacity, and whenever the size is \u00bcth of the capacity, resize it to be half the capacity</li> </ol> <pre><code>class Empty(Exception):\n    \"\"\"Error attempting to access an element from an empty container.\"\"\"\n    pass\n\nclass ArrayQueue:\n    \"\"\"FIFO queue implementation using list\"\"\"\n\n    DEFAULT_CAPACITY = 10\n\n    def __init__(self):\n        self._data = [None] * ArrayQueue.DEFAULT_CAPACITY\n        self._size = 0\n        self._front = 0\n\n    def __len__(self):\n        return self._size\n\n    def is_empty(self):\n        return self._size == 0\n\n    def first(self):\n        if self.is_empty():\n            raise Empty('Queue is Empty')\n\n        return self._data[self._front]\n\n    def dequeue(self):\n        if self._is_empty():\n            raise Empty('Queue is Empty')\n\n        answer = self._data[self._front]\n\n        self._data[self._front] = None\n        self._front = (self._front + 1) % len(self._data)\n        self._size -= 1\n\n        if 0 &lt; self._size &lt; len(self._data) // 4:\n            self._resize(len(self._data) // 2)\n\n        return answer\n\n    def enqueue(self, e):\n        if self._size == len(self._data):\n            self._resize(2 * len(self.data))\n\n        avail = (self._front + self._size) % len(self._data)\n        self._data[avail] = e\n        self._size += 1\n\n    def _resize(self, cap):\n        old = self._data\n        self._data = [None] * cap\n        walk = self._front\n\n        for k in range(self._size):\n            self._data[k] = old[walk]\n            walk = (1 + walk) % len(old)\n\n        self._front = 0\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/queues%20and%20deques.html#efficiencies","title":"Efficiencies","text":"<p>Space Complexity: <code>O(n)</code></p> <p>Time Complexities:</p> <p></p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/queues%20and%20deques.html#dequeue-double-ended-queues","title":"Dequeue - Double-Ended Queues","text":"<p>A queue like data structure that supports insertion and deletion from both the front and back of the queue.</p> <p>An implementation of this is available through the <code>deque</code> class in Python in the standard collections module.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/queues%20and%20deques.html#array-based-deqeue-implementation","title":"Array-Based Deqeue Implementation","text":"<p>We can implement a deque in a similar way as the Queue. The main modifications are:</p> <ol> <li>adding a pointer to the last element as <code>.last()</code> where <code>last = (self._front + self._size - 1) % len(self._data)</code></li> <li>ability to add and remove from the back of the queue</li> </ol> <pre><code>class Empty(Exception):\n    \"\"\"Error attempting to access an element from an empty container.\"\"\"\n    pass\n\nclass ArrayQueue:\n    \"\"\"FIFO queue implementation using list\"\"\"\n\n    DEFAULT_CAPACITY = 10\n\n    def __init__(self):\n        self._data = [None] * ArrayQueue.DEFAULT_CAPACITY\n        self._size = 0\n        self._front = 0\n\n    def __len__(self):\n        return self._size\n\n    def is_empty(self):\n        return self._size == 0\n\n    def first(self):\n        if self.is_empty():\n            raise Empty('Queue is Empty')\n\n        return self._data[self._front]\n\n    def last(self):\n        if self.is_empty():\n            raise Empty('Queue is Empty')\n\n        back = (self._front + self._size - 1) % len(self._data)\n\n        return back\n\n    def add_first(self, e):\n        if self._size == len(self._data):\n            self._resize(2 * len(self.data))\n\n        self._front = (self._front - 1) % len(self._data)\n        self._data[self._front] = e\n        self._size += 1\n\n    def delete_first(self):\n        if self.is_empty():\n            raise Empty('Deque is Empty')\n\n        answer = self._data[self._front]\n\n        self._data[self._front] = None\n        self._front = (self._front + 1) % len(self._data)\n        self._size -= 1\n\n        if 0 &lt; self._size &lt; len(self._data) // 4:\n            self._resize(len(self._data) // 2)\n\n        return answer\n\n    def add_last(self, e):\n        if self._size == len(self._data):\n            self._resize(2 * len(self.data))\n\n        avail = (self._front + self._size) % len(self._data)\n        self._data[avail] = e\n        self._size += 1\n\n    def delete_last(self):\n        if self.is_empty():\n            raise Empty('Deque is empty')\n\n        back = self.last()\n        answer = self._data[back]\n        self._data[back] = None\n        self._size -= 1\n\n        if 0 &lt; self._size &lt; len(self._data) // 4:\n            self._resize(len(self._data) // 2)\n\n        return answer\n\n    def _resize(self, cap):\n        old = self._data\n        self._data = [None] * cap\n        walk = self._front\n\n        for k in range(self._size):\n            self._data[k] = old[walk]\n            walk = (1 + walk) % len(old)\n\n        self._front = 0\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/queues%20and%20deques.html#efficiencies_1","title":"Efficiencies","text":"<p>Space Complexity: <code>O(n)</code></p> <p>Time Complexities:</p> <p></p> <p>References</p> <ol> <li> <p>Skiena, Steven S.. (2008). The Algorithm Design Manual, 2<sup>nd</sup> ed. (2). : Springer Publishing Company.\u00a0\u21a9</p> </li> <li> <p>Goodrich, M. T., Tamassia, R., &amp; Goldwasser, M. H. (2013). Data Structures and Algorithms in Python (1<sup>st</sup> ed.). Wiley Publishing.\u00a0\u21a9</p> </li> <li> <p>Cormen, T. H., Leiserson, C. E., &amp; Rivest, R. L. (1990). Introduction to algorithms. Cambridge, Mass. : New York, MIT Press.\u00a0\u21a9</p> </li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/skip%20lists.html","title":"Skip Lists","text":"<p>TODO: Add notes here - leaving blank for now.</p> <p>References</p> <ol> <li> <p>Skiena, Steven S.. (2008). The Algorithm Design Manual, 2<sup>nd</sup> ed. (2). : Springer Publishing Company.\u00a0\u21a9</p> </li> <li> <p>Goodrich, M. T., Tamassia, R., &amp; Goldwasser, M. H. (2013). Data Structures and Algorithms in Python (1<sup>st</sup> ed.). Wiley Publishing.\u00a0\u21a9</p> </li> <li> <p>Cormen, T. H., Leiserson, C. E., &amp; Rivest, R. L. (1990). Introduction to algorithms. Cambridge, Mass. : New York, MIT Press.\u00a0\u21a9</p> </li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/stacks.html","title":"Stacks","text":"<p>A stack is a collection of objects that are inserted and removed according to the last-in, first-out (LIFO) principle. Popular uses of stacks are for \"back\" and \"undo\" operations in browsers and editors.</p> <p>In general the LIFO protocol allows the stack to be used as a tool for reversing a data sequence, matching tags, parenthesis etc.</p> <p>Some common methods supported by a stack are:</p> <ul> <li><code>stack.push(e)</code> - add an element <code>e</code> to the top of a stack</li> <li><code>stack.pop()</code> - remove and return the top elements from a stack</li> <li><code>stack.top()</code> - return the top element of a stack</li> <li><code>stack.is_empty()</code> - check if stack is empty</li> <li><code>len(stack)</code> - the number of elements in a stack</li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/stacks.html#simple-array-based-stacks","title":"Simple Array-Based Stacks","text":"<p>We can implements stacks easily by using a <code>list</code> in Python.</p> <p>Look at Adapter Pattern to get an example of a stack implementation using lists.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/stacks.html#efficiencies","title":"Efficiencies","text":"<p>Space Complexity: <code>O(n)</code></p> <p>Time Complexities:</p> <p></p> <p>References</p> <ol> <li> <p>Skiena, Steven S.. (2008). The Algorithm Design Manual, 2<sup>nd</sup> ed. (2). : Springer Publishing Company.\u00a0\u21a9</p> </li> <li> <p>Goodrich, M. T., Tamassia, R., &amp; Goldwasser, M. H. (2013). Data Structures and Algorithms in Python (1<sup>st</sup> ed.). Wiley Publishing.\u00a0\u21a9</p> </li> <li> <p>Cormen, T. H., Leiserson, C. E., &amp; Rivest, R. L. (1990). Introduction to algorithms. Cambridge, Mass. : New York, MIT Press.\u00a0\u21a9</p> </li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html","title":"Trees","text":"<p>Trees are a non-linear data structure that allow us to implement a range of agorithms that are faster than using linear data structures like array-based lists or linked lists. Trees also provide a natural organizationof data, and have become ubiquitous structures in file systems, GUIs, databses etc.</p> <p>Trees are hierarchical representation of a set of nodes storing elements such that the nodes have a parent-child relationship with the following definitions and properties:</p> <ul> <li>root - top node of a tree</li> <li>parent - except for the root node, all nodes in a tree have a parent</li> <li>children - nodes in a tree may have zero or more children</li> <li>depth \\(d_p\\) - for a position <code>p</code> of a node in a tree <code>T</code>, the depth of <code>p</code> is the number of ancestors of <code>p</code>, excluding <code>p</code> itself. (but it includes the root node)</li> <li>e.g. if <code>p</code> is the root, then its depth is 0, otherwise it is <code>1 + depth of the parent</code></li> <li>height \\(h_p\\) - defined from the leaf of the tree, defined recursively</li> <li>e.g. if <code>p</code> is the leaf of a tree, its height is 0, otherwise it is <code>1 + max(height of children)</code></li> <li>the height of a non-empty tree T is equal to the maximum of the depths of its leaf positions</li> </ul> <p>In a tree, each node of the tree apart from the root node has a unique parent node, and every node with a parent is a child node.</p> <p>Trees can be empty (i.e. don't even have a root node) which allows for recursive definitions such that:</p> <ol> <li>a tree <code>T</code> is either empty or consists of a root node <code>r</code></li> <li>root node <code>r</code> has a set of set of subtrees whose roots are the children of <code>r</code></li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#ordered-trees","title":"Ordered Trees","text":"<p>A tree is ordered if there is a meaningful linear order among the children of each node - i.e. we can identify the children of a node as being the first, second, third and so on. This is usually visualized by arranng siblings left to right, according to their order.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#tree-abstract-base-class","title":"Tree Abstract Base Class","text":"<pre><code>from abc import ABC, abstractmethod\n\nclass Position(ABC):\n\n    def element(self):\n        raise NotImplementedError('must be implemented by subclass')\n\n    def __eq__(self, other):\n        raise NotImplementedError('must be implemented by subclass')\n\n    def __ne__(self, other):\n        return not self == other\n\nclass Tree(ABC):\n\n    def root(self):\n        raise NotImplementedError('must be implemented by subclass')\n\n    def parent(self, position):\n        raise NotImplementedError('must be implemented by subclass')\n\n    def num_children(self, position):\n        raise NotImplementedError('must be implemented by subclass')\n\n    def children(self, position):\n        raise NotImplementedError('must be implemented by subclass')\n\n    def __len__(self):\n        raise NotImplementedError('must be implemented by subclass')\n\n    def is_root(self, position):\n        return self.root() == position\n\n    def is_leaf(self, position):\n        return self.num_children(position) == 0\n\n    def is_empty(self):\n        return len(self) == 0\n\n    def depth(self, position): # O(d_p + 1)\n        if self.is_root(position):\n            return 0\n        else:\n            return 1 + self.depth(self.parent(p))\n\n    def _height(self, position): # O(n)\n        if self.is_leaf(position):\n            return 0\n        else:\n            return 1 + max(self.height(child) for child in self.children(position))\n\n    def height(self, position=None):\n        if not position:\n            position = self.root()\n\n        return self._height(position)\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#linked-structure-for-general-trees","title":"Linked Structure for General Trees","text":"<p>A natural way to represent a tree is through a linked structure, with nodes that maintain references to the elment stores at a position <code>p</code> and to the nodes associated with the children and parent of <code>p</code>.</p> <p>For a general tree, there is no limit on the number of children that a node may have. A natural way to realize a general tree <code>T</code> as a linked structure is to have each node store a single container of references to its children.</p> <p>The tree itself also maintainsan instance variable storing a reference to the root node, and a variable called <code>size</code> that represents the overall number of nodes of <code>T</code>.</p> <pre><code>class TreeNode:\n    __slots__ = '_element', '_parent', '_left', '_right'\n\n    def __init__(self, element, parent=None, children=[]):\n        self._element = element\n        self._parent = parent\n        self._children = children\n\nclass TreePosition(Position):\n\n    def __init__(self, container, node):\n        self._container = container\n        self._node = node\n\n    def element(self):\n        return self._node._element\n\n    def __eq__(self, other):\n        return type(other) is type(self) and other._node is self._node\n\nclass GeneralTree(Tree):\n\n    def __init__(self):\n        self._root = None\n        self._size = 0\n\n    def _make_position(self, node):\n        return self.Position(self, node) ifnode is not None else None\n\n    def _validate(self, p):\n        if not isinstance(p, self.Position):\n            raise TypeError('p must be proper Position type')\n\n        if p._container is not self:\n            raise ValueError('p does not belong to this container')\n\n        if p._node._parent is p._node:\n            raise ValueError('p is no longer valid')\n\n        return p._node\n\n    def __len__(self):\n        return self._size\n\n    def root(self):\n        return self._make_postion(self._root)\n\n    def parent(self, p):\n        node = self._validate(p)\n        return self._make_position(node._parent)\n\n    def children(self, p):\n        node = self._validate(p)\n        return self._make_position(node._children)\n\n    def num_children(self, p):\n        node = self._validate(p)\n        return len(node._children)\n\n    def __iter__(self):\n        for p in self.positions():\n            yield p.element()\n\n    def positions(self):\n        return self.preorder()\n\n    def preorder(self):\n        if not self.is_empty():\n            for p in self._subtree_preorder(self.root()):\n                yield p\n\n    def _subtree_preorder(self, p):\n        yield p\n        for c in self.children(p):\n            for other in self._subtree_preorder(c):\n                yield other\n\n    def postorder(self):\n        if not self.is_empty():\n            for p in self._subtree_postorder(self.root()):\n                yield p\n\n    def _subtree_postorder(self, p):\n        for c in self.children(p):\n            for other in self._subtree_postorder(c):\n                yield other\n        yield p\n\n    def breadthfirst(self):\n        if not self.is_empty():\n            fringe = LinkedQueue()\n            fringe.enqueue(self.root())\n            while not fringe.is_empty():\n                p = fringe.dequeue()\n                yield p\n                for c in self.children(p):\n                    fringe.enqueue(c)\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#binary-trees","title":"Binary Trees","text":"<p>A binary tree is an ordered tree with the following properties:</p> <ol> <li>Every node has at most two children</li> <li>Each child node is labeled as being either a left child or a right child</li> <li>A left child precedes a right child in the order of children of the node</li> </ol> <p>The subtree rooted at the left child is called a left subtree and rooted at the right a right subtree.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#recursive-definitions-of-a-binary-search-tree","title":"Recursive Definitions of a Binary Search Tree","text":"<p>A binary tree is either empty or consists of:</p> <ol> <li>a node <code>r</code>, called the root of <code>T</code>, that stores an element</li> <li>a binary tree (possibly empty), called the left subtree of <code>T</code></li> <li>a binary tree (possibly empty), called the right subtree of <code>T</code></li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#types","title":"Types","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#properfull","title":"Proper/Full","text":"<p>A Full or Proper binary search tree is one where each node has either zero or two children. Therefore internal parent nodes always have exactly two children. </p> <p>An example is expression trees that capture the arithmetic expression e.g. <code>(((3+1)x3)/((9-5)+2)</code> as shown below:</p> <pre><code>                (/)\n                / \\\n               /   \\\n             (x)    (+)\n            /  \\    /  \\\n          (+)   3  (-)  2\n         /  \\      / \\\n        3    1    9   5\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#perfect","title":"Perfect","text":"<p>All interior nodes have two children and all leaves have the same depth or same level. A perfect binary tree is also a full binary tree.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#complete","title":"Complete","text":"<p>All levels of the binary tree (0, 1, 2, ..., h -1) have maximum number of nodes possible (i.e. \\(2^i\\) nodes for \\(i \\leq i \\leq h-1\\)) and the remaining nodes at level <code>h</code> reside in the leftmost possible positions at that level. </p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#balanced","title":"Balanced","text":"<p>Where left and rigt subtrees of every node differ in height by at most 1. i.e. where no leaf is much farther away from the root than any other leaf.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#properties","title":"Properties","text":"<p>Level of a tree is a set of nodes at the same depth.</p> <p>So in a binary tree, level 0 has at most 1 node. Level 1 has at most 2 nodes. Level 2 has at most 4 nodes etc.</p> <p>So in general, a level <code>d</code> has at most \\(2^d\\) nodes.</p> <p>Therefore from this we can see that the number of nodes at any given level is exponential. From this we can define the following properties where - \\(n_E\\) is number of external nodes, \\(n_I\\) is the number of internal nodes, \\(n\\) is the numberof nodes, and \\(h\\) is the height of the tree:</p> <ol> <li>\\(h + 1 \\leq n \\leq 2^{h+1}-1\\)</li> <li>\\(1 \\leq n_E \\leq 2^h\\)</li> <li>\\(h \\leq n_I \\leq 2^h - 1\\)</li> <li>\\(log(n+1) - 1 \\leq h \\leq n - 1\\)</li> </ol> <p>Also if the tree is proper then:</p> <ol> <li>\\(2h + 1 \\leq n \\leq w^{h+1} - 1\\)</li> <li>\\(h + 1 \\leq n_E \\leq 2^h\\)</li> <li>\\(h \\leq n_I \\leq 2^h - 1\\)</li> <li>\\(log(n+1) - 1 \\leq h \\leq (n-1)/2\\)</li> <li>\\(n_E = n_I + 1\\)</li> </ol> <p>For reference, \\(n_E\\) = external nodes = nodes with no children. \\(n_I\\) = internal nodes = nodes which are not leaves.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#array-based-binary-tree","title":"Array Based Binary Tree","text":"<pre><code>                (/)\n                / \\\n               /   \\\n             (x)    (+)\n            /  \\    /  \\\n          (+)   3  (-)  2\n         /  \\      / \\\n        3    1    9   5\n\nbfs: [/, x, +, +, 3, -, 2, 3, 1, 9, 5, Null, Null]\npre-order: [/, x, +, 3, Null, Null, 1, Null, Null, 3, Null, Null, +, -, 9, Null, Null, 5, Null, Null, 2, Null, Null]\n</code></pre> <p>BFS or level numbering of positions to build an array is fairly common. Here a position can be represented by a single integer - the main advantage of this data structure. But operations like deleting and promoting children takes <code>O(n)</code> time.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#utilizing-the-general-tree-linked-structure","title":"Utilizing the General Tree Linked Structure","text":"<p>For a binary tree the children are defined as <code>left</code> and <code>right</code> child.</p> <pre><code>class BinaryTreeNode:\n    __slots__ = '_element', '_parent', '_left', '_right'\n\n    def __init__(self, element, parent=None, left=None, right=None):\n        self._element = element\n        self._parent = parent\n        self._left = left\n        self._right = right\n\nclass BinaryTreePosition(Position):\n\n    def __init__(self, container, node):\n        self._container = container\n        self._node = node\n\n    def element(self):\n        return self._node._element\n\n    def __eq__(self, other):\n        return type(other) is type(self) and other._node is self._node\n\nclass BinaryTree(Tree):\n\n    def __init__(self):\n        self._root = None\n        self._size = 0\n\n    def _make_position(self, node):\n        return self.Position(self, node) ifnode is not None else None\n\n    def _validate(self, p):\n        if not isinstance(p, self.Position):\n            raise TypeError('p must be proper Position type')\n\n        if p._container is not self:\n            raise ValueError('p does not belong to this container')\n\n        if p._node._parent is p._node:\n            raise ValueError('p is no longer valid')\n\n        return p._node\n\n    def __len__(self):\n        return self._size\n\n    def root(self):\n        return self._make_postion(self._root)\n\n    def parent(self, p):\n        node = self._validate(p)\n        return self._make_position(node._parent)\n\n    def left(self, p):\n        node = self._validate(p)\n        return self._make_position(node._left)\n\n    def right(self, p):\n        node = self._validate(p)\n        return self._make_position(node._right)\n\n    def num_children(self, p):\n        node = self._validate(p)\n        count = 0:\n        if node._left is not None:\n            count += 1\n        if node._right is not None:\n            count += 1\n        return len(count)\n\n    def _add_root(self, e):\n        if self._root is not None: raise ValueError('root exists')\n        self._size = 1\n        self._root = Node(e)\n        return self._make_position(self._root)\n\n    def _add_left(self, p, e):\n        node = self._validate(p)\n        if node._left is not None:\n            raise ValueError('Left child exists')\n        self._size += 1\n        node._left = self._Node(e, node)\n        return self._make_position(node._left)\n\n    def _add_right(self, p, e):\n        node = self._validate(p)\n        if node._right is not None:\n            raise ValueError('Right child exists')\n        self._size += 1\n        node._right = self._Node(e, node)\n        return self._make_position(node._right)\n\n    def _replace(self, p, e):\n        node = self._validate(p)\n        old = node._element\n        node._element = e\n        return old\n\n    def _delete(self, p):\n        node = self._validate(p)\n        if self.num_children(p) == 2:\n            raise ValueError('Position has two children')\n        child = node._left if node._left else node._right\n        if child is not None:\n            child._parent = node._parent\n        if node is self._root:\n            self._root = child\n        else:\n            parent = node._parent\n            if node is parent._left:\n                parent._left = child\n            else:\n                parent._right = child\n        self._size -= 1\n        node._parent = node\n        return node._element\n\n    def _attach(self, p, t1, t2):\n        node = self._validate(p)\n        if not self.is_leaf(p):\n            raise ValueError('position must be leaf')\n        if not type(self) is type(t1) is type(t2):\n            raise TypeError('Tree types must match')\n        self._size += len(t1) + len(t2)\n        if not t1.is_empty():\n            t1._root._parent = node\n            node._left = t1._root\n            t1._root = None\n            t1._size = 0\n        if not t2.is_empty():\n            t2._root._parent = node\n            node._right = t2._root\n            t2._root = None\n            t2._size = 0\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#tree-traversal-algorithms","title":"Tree Traversal Algorithms","text":"<p>Go to Algorithms &gt;&gt; Tree Traversals</p> <p>We can also modify our base <code>Tree</code> class to implement the traversal methods as well, with the pre-order traversal being the default one.</p> <p>Refer to Tree Abstract Base Class for the other parts of the base class.</p> <p>Some key observations:</p> <ol> <li>we implement the traversals as generator functions where we yield each position to the caller and let the caller decide what action to perform at that position</li> <li>the <code>positions</code> method returns the iteration as an object - again acting as a generator</li> <li>the <code>breadth first search</code> or <code>bfs</code> traversal is iterative - not recursive</li> <li>the <code>inorder</code> traversal is only implemented for <code>binary trees</code> - it does not generalize for generic trees</li> </ol> <pre><code>class Tree(ABC):\n\n    ...\n    def positions(self):\n        self.preorder()\n\n    def preorder(self):\n        if not self.is_empty():\n            for p in self._subtree_preorder(self.root()):\n                yield p\n\n    def _subtree_preorder(self, p):\n        yield p\n        for c in self.children(p):\n            for other in self._subtree_preorder(c):\n                yield other\n\n    def postorder(self):\n        if not self.is_empty():\n            for p in self._subtree_postorder(self.root()):\n                yield p\n\n    def _subtree_postorder(self, p):\n        for c in self.children(p):\n            for other in self._subtree_postorder(c):\n                yield other\n        yield p \n\n    def bfs(self):\n        if not self.is_empty():\n            fringe = LinkedQueue()\n            fringe.enqueue(self.root())\n            while not fringe.is_empty():\n                p = fringe.dequeue()\n                yield p\n                for c in self.children(p):\n                    fringe.enqueue(c)\n\n    def inorder(self):\n        if not self.is_empty():\n            for p in self._subtree_inorder(self.root()):\n                yield p\n\n    def _subtree_inorder(self, p):\n        if self.left(p) is not None:\n            for other in self._subtree_inorder(self.left(p)):\n                yield other\n        yield p\n        if self.right(p) is not None:\n            for other in self._subtree_inorder(self.right(p)):\n                yield other\n    ...\n</code></pre> <p>As can be seen, while we can implement specific traversals for the <code>Tree</code> class, or the <code>inorder</code> method for binary trees, the code is not general enough to capture the range of computations possible. These were all custom implementations with some repeating algorithmic patterns (e.g. blending of work performed before or after recursion of subtrees). </p> <p>A more general framework for implemting tree traversals is through Euler tour traversals which also uses the Template method.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#binary-search-trees","title":"Binary Search Trees","text":"<p>If we have a set of unique elements that have an order relation  (e.g. a set of integers), we can construct a binary search tree such that for each position <code>p</code> of the tree <code>T</code>:</p> <ol> <li>Position <code>p</code> stores an element of the set, denoted as <code>e(p)</code></li> <li>Elements stored in the left subtree of <code>p</code> (if any) are less than <code>e(p)</code></li> <li>Elements stored in the right subtree of <code>p</code> (if any) are greater than <code>e(p)</code></li> </ol> <p></p> <p>Binary Search Trees can be used to efficiently implement a sorted map - assuming we have an order relation defined on the keys. </p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#navigating-binary-search-trees","title":"Navigating Binary Search Trees","text":"<p>An inorder traversal of a binary search tree visits positions in increasing order of their keys. Therefore if presented with a binary search tree, we can produce a sorted iteration of the keys of a map in linear time.</p> <p>Hence binary search trees provide a way to produce ordered data in linear time.</p> <p>Additionally binary search trees can be used to find:</p> <ol> <li>position containing the greatest key that is less than the position <code>p</code> - the position that would be visited immediately before position <code>p</code> in an inorder traversal</li> <li>position containing the least key that is great than the position <code>p</code> - the position that would be visited immediately after position <code>p</code> in an inorder traversal</li> </ol> <pre><code>def after(p):\n    if right(p) is not None then {successor is leftmost position in p's right subtree}:\n        walk = right(p)\n        while left(walk) is not None do:\n            walk = left(walk)\n        return walk\n\n    else: {successor is nearest ancestor having p in its left subtree}\n        walk = p\n        ancestor = parent(walk)\n        while ancestor is not None and walk == right (ancestor) do:\n            walk = ancestor\n            ancestor = parent(walk)\n        return ancestor\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#searches","title":"Searches","text":"<p>We can traverse a binary search tree to search for specific keys in O(log(n)) time complexty as long as it is balanced. In the worst (imbalanced) case the height <code>h</code> of the tree can be equal to the <code>n</code> in which case the time complexity would be O(h) = O(n) instead.</p> <pre><code>def TreeSearch(T, p, k):\n    if k == p.key() then:\n        return p\n    else if k &lt; p.key() and T.left(p) is not None then:\n        return TreeSearch(T, T.left(p), k)\n    else if k &gt; p.key() and T.right(p) is not None then:\n        return TreeSearch(T, T.right(p), k)\n    return p # return the last node if key not found\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#insertions","title":"Insertions","text":"<p>Search for key <code>k</code>, and if found, that item's existing value is reassigned. If not found, a new item is inserted into the underlying tree T in place of theempty subtree that was reached at the end of the fialed search.</p> <pre><code>def TreeInsert(T, k, v):\n    # Tree T, search key k, and value v to be associated with key k\n    p = TreeSearch(T, T.root(), k)\n    if k == p.key() then:\n        set p's value to v\n    elif k &lt; p.key() then:\n        add node with item (k, v) as left child of p\n    else:\n        add node with item (k, v) as righ child of p\n</code></pre> <p>This is always enacted at the bottom of a path.</p> <p>O(h)</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#deletions","title":"Deletions","text":"<p>More complicated than insertions as they can happen in any part of the tree.</p> <p>Steps: - If key k is successfully found in tree T:   - if p has at most one child, delete item with key k at position p, and reoplace it with its child   - if p has two children: locate the predecessor (the rightmost position of the left subtree of p) i.e. <code>r = before(p)</code>, then use <code>r</code> as replace for the position <code>p</code>, and then delete <code>r</code> (simple as it does not have a right child by defition)</p> <p>O(h)</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#python-implementation","title":"Python Implementation","text":"<pre><code>class BinarySearchTreePosition(BinaryTreePosition):\n    def key(self):\n      \"\"\"Return key of map's key-value pair.\"\"\"\n      return self.element()._key\n\n    def value(self):\n      \"\"\"Return value of map's key-value pair.\"\"\"\n      return self.element()._value\n\nclass BinarySearchTree(BinaryTree, MapBase):\n    def _subtree_search(self, p, k):\n        \"\"\"Return Position of p's subtree having key k, or last node searched.\"\"\"\n        if k == p.key():                                   # found match\n        return p                                         \n        elif k &lt; p.key():                                  # search left subtree\n        if self.left(p) is not None:\n            return self._subtree_search(self.left(p), k)   \n        else:                                              # search right subtree\n        if self.right(p) is not None:\n            return self._subtree_search(self.right(p), k)\n        return p                                           # unsucessful search\n\n    def _subtree_first_position(self, p):\n        \"\"\"Return Position of first item in subtree rooted at p.\"\"\"\n        walk = p\n        while self.left(walk) is not None:                 # keep walking left\n        walk = self.left(walk)\n        return walk\n\n    def _subtree_last_position(self, p):\n        \"\"\"Return Position of last item in subtree rooted at p.\"\"\"\n        walk = p\n        while self.right(walk) is not None:                # keep walking right\n        walk = self.right(walk)\n        return walk\n\n    #--------------------- public methods providing \"positional\" support ---------------------\n    def first(self):\n        \"\"\"Return the first Position in the tree (or None if empty).\"\"\"\n        return self._subtree_first_position(self.root()) if len(self) &gt; 0 else None\n\n    def last(self):\n        \"\"\"Return the last Position in the tree (or None if empty).\"\"\"\n        return self._subtree_last_position(self.root()) if len(self) &gt; 0 else None\n\n    def before(self, p):\n        \"\"\"Return the Position just before p in the natural order.\n\n        Return None if p is the first position.\n        \"\"\"\n        self._validate(p)                            # inherited from LinkedBinaryTree\n        if self.left(p):\n        return self._subtree_last_position(self.left(p))\n        else:\n        # walk upward\n        walk = p\n        above = self.parent(walk)\n        while above is not None and walk == self.left(above):\n            walk = above\n            above = self.parent(walk)\n        return above\n\n    def after(self, p):\n        \"\"\"Return the Position just after p in the natural order.\n\n        Return None if p is the last position.\n        \"\"\"\n        self._validate(p)                            # inherited from LinkedBinaryTree\n        if self.right(p):\n        return self._subtree_first_position(self.right(p))\n        else:\n        walk = p\n        above = self.parent(walk)\n        while above is not None and walk == self.right(above):\n            walk = above\n            above = self.parent(walk)\n        return above\n\n    def find_position(self, k):\n        \"\"\"Return position with key k, or else neighbor (or None if empty).\"\"\"\n        if self.is_empty():\n        return None\n        else:\n        p = self._subtree_search(self.root(), k)\n        self._rebalance_access(p)                  # hook for balanced tree subclasses\n        return p\n\n    def delete(self, p):\n        \"\"\"Remove the item at given Position.\"\"\"\n        self._validate(p)                            # inherited from LinkedBinaryTree\n        if self.left(p) and self.right(p):           # p has two children\n        replacement = self._subtree_last_position(self.left(p))\n        self._replace(p, replacement.element())    # from LinkedBinaryTree\n        p =  replacement\n        # now p has at most one child\n        parent = self.parent(p)\n        self._delete(p)                              # inherited from LinkedBinaryTree\n        self._rebalance_delete(parent)               # if root deleted, parent is None\n\n    #--------------------- public methods for (standard) map interface ---------------------\n    def __getitem__(self, k):\n        \"\"\"Return value associated with key k (raise KeyError if not found).\"\"\"\n        if self.is_empty():\n        raise KeyError('Key Error: ' + repr(k))\n        else:\n        p = self._subtree_search(self.root(), k)\n        self._rebalance_access(p)                  # hook for balanced tree subclasses\n        if k != p.key():\n            raise KeyError('Key Error: ' + repr(k))\n        return p.value()\n\n    def __setitem__(self, k, v):\n        \"\"\"Assign value v to key k, overwriting existing value if present.\"\"\"\n        if self.is_empty():\n        leaf = self._add_root(self._Item(k,v))     # from LinkedBinaryTree\n        else:\n        p = self._subtree_search(self.root(), k)\n        if p.key() == k:\n            p.element()._value = v                   # replace existing item's value\n            self._rebalance_access(p)                # hook for balanced tree subclasses\n            return\n        else:\n            item = self._Item(k,v)\n            if p.key() &lt; k:\n            leaf = self._add_right(p, item)        # inherited from LinkedBinaryTree\n            else:\n            leaf = self._add_left(p, item)         # inherited from LinkedBinaryTree\n        self._rebalance_insert(leaf)                 # hook for balanced tree subclasses\n\n    def __delitem__(self, k):\n        \"\"\"Remove item associated with key k (raise KeyError if not found).\"\"\"\n        if not self.is_empty():\n        p = self._subtree_search(self.root(), k)\n        if k == p.key():\n            self.delete(p)                           # rely on positional version\n            return                                   # successful deletion complete\n        self._rebalance_access(p)                  # hook for balanced tree subclasses\n        raise KeyError('Key Error: ' + repr(k))\n\n    def __iter__(self):\n        \"\"\"Generate an iteration of all keys in the map in order.\"\"\"\n        p = self.first()\n        while p is not None:\n        yield p.key()\n        p = self.after(p)\n\n    #--------------------- public methods for sorted map interface ---------------------\n    def __reversed__(self):\n        \"\"\"Generate an iteration of all keys in the map in reverse order.\"\"\"\n        p = self.last()\n        while p is not None:\n        yield p.key()\n        p = self.before(p)\n\n    def find_min(self):\n        \"\"\"Return (key,value) pair with minimum key (or None if empty).\"\"\"\n        if self.is_empty():\n        return None\n        else:\n        p = self.first()\n        return (p.key(), p.value())\n\n    def find_max(self):\n        \"\"\"Return (key,value) pair with maximum key (or None if empty).\"\"\"\n        if self.is_empty():\n        return None\n        else:\n        p = self.last()\n        return (p.key(), p.value())\n\n    def find_le(self, k):\n        \"\"\"Return (key,value) pair with greatest key less than or equal to k.\n\n        Return None if there does not exist such a key.\n        \"\"\"\n        if self.is_empty():\n        return None\n        else:\n        p = self.find_position(k)\n        if k &lt; p.key():\n            p = self.before(p)\n        return (p.key(), p.value()) if p is not None else None\n\n    def find_lt(self, k):\n        \"\"\"Return (key,value) pair with greatest key strictly less than k.\n\n        Return None if there does not exist such a key.\n        \"\"\"\n        if self.is_empty():\n        return None\n        else:\n        p = self.find_position(k)\n        if not p.key() &lt; k:\n            p = self.before(p)\n        return (p.key(), p.value()) if p is not None else None\n\n    def find_ge(self, k):\n        \"\"\"Return (key,value) pair with least key greater than or equal to k.\n\n        Return None if there does not exist such a key.\n        \"\"\"\n        if self.is_empty():\n        return None\n        else:\n        p = self.find_position(k)                   # may not find exact match\n        if p.key() &lt; k:                             # p's key is too small\n            p = self.after(p)\n        return (p.key(), p.value()) if p is not None else None\n\n    def find_gt(self, k):\n        \"\"\"Return (key,value) pair with least key strictly greater than k.\n\n        Return None if there does not exist such a key.\n        \"\"\"\n        if self.is_empty():\n        return None\n        else:\n        p = self.find_position(k)\n        if not k &lt; p.key():                   \n            p = self.after(p)\n        return (p.key(), p.value()) if p is not None else None\n\n    def find_range(self, start, stop):\n        \"\"\"Iterate all (key,value) pairs such that start &lt;= key &lt; stop.\n\n        If start is None, iteration begins with minimum key of map.\n        If stop is None, iteration continues through the maximum key of map.\n        \"\"\"\n        if not self.is_empty():\n        if start is None:\n            p = self.first()\n        else:\n            # we initialize p with logic similar to find_ge\n            p = self.find_position(start)\n            if p.key() &lt; start:\n            p = self.after(p)\n        while p is not None and (stop is None or p.key() &lt; stop):\n            yield (p.key(), p.value())\n            p = self.after(p)\n\n    #--------------------- hooks used by subclasses to balance a tree ---------------------\n    def _rebalance_insert(self, p):\n        \"\"\"Call to indicate that position p is newly added.\"\"\"\n        pass\n\n    def _rebalance_delete(self, p):\n        \"\"\"Call to indicate that a child of p has been removed.\"\"\"\n        pass\n\n    def _rebalance_access(self, p):\n        \"\"\"Call to indicate that position p was recently accessed.\"\"\"\n        pass\n\n    #--------------------- nonpublic methods to support tree balancing ---------------------\n\n    def _relink(self, parent, child, make_left_child):\n        \"\"\"Relink parent node with child node (we allow child to be None).\"\"\"\n        if make_left_child:                           # make it a left child\n        parent._left = child\n        else:                                         # make it a right child\n        parent._right = child\n        if child is not None:                         # make child point to parent\n        child._parent = parent\n\n    def _rotate(self, p):\n        \"\"\"Rotate Position p above its parent.\n\n        Switches between these configurations, depending on whether p==a or p==b.\n\n            b                  a\n            / \\                /  \\\n            a  t2             t0   b\n        / \\                     / \\\n        t0  t1                  t1  t2\n\n        Caller should ensure that p is not the root.\n        \"\"\"\n        \"\"\"Rotate Position p above its parent.\"\"\"\n        x = p._node\n        y = x._parent                                 # we assume this exists\n        z = y._parent                                 # grandparent (possibly None)\n        if z is None:            \n        self._root = x                              # x becomes root\n        x._parent = None        \n        else:\n        self._relink(z, x, y == z._left)            # x becomes a direct child of z\n        # now rotate x and y, including transfer of middle subtree\n        if x == y._left:\n        self._relink(y, x._right, True)             # x._right becomes left child of y\n        self._relink(x, y, False)                   # y becomes right child of x\n        else:\n        self._relink(y, x._left, False)             # x._left becomes right child of y\n        self._relink(x, y, True)                    # y becomes left child of x\n\n    def _restructure(self, x):\n        \"\"\"Perform a trinode restructure among Position x, its parent, and its grandparent.\n\n        Return the Position that becomes root of the restructured subtree.\n\n        Assumes the nodes are in one of the following configurations:\n\n            z=a                 z=c           z=a               z=c  \n        /  \\                /  \\          /  \\              /  \\  \n        t0  y=b             y=b  t3       t0   y=c          y=a  t3 \n            /  \\            /  \\               /  \\         /  \\     \n            t1  x=c         x=a  t2            x=b  t3      t0   x=b    \n            /  \\        /  \\               /  \\              /  \\    \n            t2  t3      t0  t1             t1  t2            t1  t2   \n\n        The subtree will be restructured so that the node with key b becomes its root.\n\n                b\n                /   \\\n            a       c\n            / \\     / \\\n            t0  t1  t2  t3\n\n        Caller should ensure that x has a grandparent.\n        \"\"\"\n        \"\"\"Perform trinode restructure of Position x with parent/grandparent.\"\"\"\n        y = self.parent(x)\n        z = self.parent(y)\n        if (x == self.right(y)) == (y == self.right(z)):  # matching alignments\n        self._rotate(y)                                 # single rotation (of y)\n        return y                                        # y is new subtree root\n        else:                                             # opposite alignments\n        self._rotate(x)                                 # double rotation (of x)     \n        self._rotate(x)\n        return x                                        # x is new subtree root\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#self-balancing-trees","title":"Self Balancing Trees","text":"<p>Some sequences of operations on binary search trees may lead to an unbalanced tree with height proportional to <code>n</code>. Hence we may only claim <code>O(n)</code> search.</p> <p>However there are common search tree algorthms that provide stronger performance guarantees. AVL, Splay and Red-Black trees are based on augmenting the standard binary search tree with occasional operations to reshape the tree and reduce its height. </p> <p>The primary operation to rebalance a binary search tree is known as a rotation. During a rotation, we \"rotate\" a child to be above its parent, as diagrammed below:</p> <p></p> <p>There are two primary types of rotations: left rotations and right rotations. Both types are used to adjust the structure of the tree without violating the binary search tree properties, where the left child of a node is less than the node, and the right child is greater.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#right-rotation","title":"Right Rotation","text":"<p>A right rotation is performed on a node that has a left child. This operation shifts the node down to the right, making its left child the new parent of the subtree. Here's how it works:</p> <ol> <li>Given a node <code>X</code> with a left child <code>Y</code>, <code>Y</code> will become the new root of the subtree.</li> <li><code>X</code>'s left subtree becomes <code>Y</code>'s right subtree.</li> <li><code>Y</code> takes the place of <code>X</code>, and <code>X</code> becomes the right child of <code>Y</code>.</li> </ol> <p>This operation is used when the tree becomes left-heavy, and we need to decrease the height of the left subtree to maintain balance.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#left-rotation","title":"Left Rotation","text":"<p>A left rotation is the mirror operation of a right rotation. It is performed on a node that has a right child, shifting the node down to the left and making its right child the new parent of the subtree. Here's the step-by-step process:</p> <ol> <li>Given a node <code>X</code> with a right child <code>Y</code>, <code>Y</code> will become the new root of the subtree.</li> <li><code>X</code>'s right subtree becomes <code>Y</code>'s left subtree.</li> <li><code>Y</code> takes the place of <code>X</code>, and <code>X</code> becomes the left child of <code>Y</code>.</li> </ol> <p>This operation is used when the tree becomes right-heavy, and we need to decrease the height of the right subtree.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#visual-representation","title":"Visual Representation","text":"<p>Let's illustrate both rotations with diagrams. For a right rotation:</p> <pre><code>    X               Y\n   / \\             / \\\n  Y   C   ---&gt;    A   X\n / \\                 / \\\nA   B               B   C\n</code></pre> <pre><code>class TreeNode:\n    def __init__(self, key):\n        self.key = key\n        self.left = None\n        self.right = None\n\ndef rightRotate(root):\n    newRoot = root.left\n    root.left = newRoot.right\n    newRoot.right = root\n    return newRoot\n</code></pre> <p>and for a left rotation:</p> <pre><code>  X               Y\n / \\             / \\\nA   Y    ---&gt;   X   C\n   / \\         / \\\n  B   C       A   B\n</code></pre> <pre><code>def leftRotate(root):\n    newRoot = root.right\n    root.right = newRoot.left\n    newRoot.left = root\n    return newRoot\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#avl-trees","title":"AVL Trees","text":"<p>AVL Trees store a balance factor on each node that follows the height balance property where for each position p of T, th heights of the children of p differ at most by  1.</p> <p>The height is defined as the longest path from the node to a leaf.</p> <p>Hence the subtree of an AVL tree is itself an AVL tree. The height of an AVL tree storing n entries is <code>O(logn)</code>.</p> <p>The insertion and deletion operations of AVL trees begin similarly to the corresponding operations for binary search trees, but with post-processing for each oepration to restore the balance of ay portions of the tree that are adversely affected by the change.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#insertion-in-avl-trees","title":"Insertion in AVL Trees","text":"<p>Insertion into an AVL tree involves several steps:</p> <ol> <li>BST Insertion: The node is inserted following the binary search tree rules.</li> <li>Updating Heights: After insertion, the heights of the ancestors of the inserted node are updated.</li> <li>Balancing the Tree: If any ancestor node becomes unbalanced (balance factor is not -1, 0, or 1), rotations are performed to balance the tree.</li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#rotations-for-rebalancing","title":"Rotations for Rebalancing","text":"<p>Depending on the structure of the tree at the unbalanced node, one of the following rotations is performed:</p> <ul> <li>Single Right Rotation (LL Rotation): When the left subtree of the left child is causing the imbalance.</li> <li>Single Left Rotation (RR Rotation): When the right subtree of the right child is causing the imbalance.</li> <li>Left-Right Rotation (LR Rotation): When the right subtree of the left child is causing the imbalance.</li> <li>Right-Left Rotation (RL Rotation): When the left subtree of the right child is causing the imbalance.</li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#deletion-in-avl-trees","title":"Deletion in AVL Trees","text":"<p>Deletion from an AVL tree is a bit more complex:</p> <ol> <li>Node Removal: Follows the basic BST deletion rules.</li> <li>Balancing the Tree: After deletion, the tree might become unbalanced. The same procedure of updating heights and performing rotations is followed to restore balance.</li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#performance-of-avl-trees","title":"Performance of AVL Trees","text":"<ul> <li>Search: \\(O(\\log n)\\) - AVL trees ensure that the height of the tree is always logarithmic in the number of nodes, guaranteeing efficient searches.</li> <li>Insertion: \\(O(\\log n)\\) - Insertions may require a constant number of rotations, but the overall time complexity remains logarithmic.</li> <li>Deletion: \\(O(\\log n)\\) - Similar to insertion, deletions are logarithmic in time complexity, including potential rotations for rebalancing.</li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#red-black-trees","title":"Red-Black Trees","text":"<p>A Red-Black tree is characterized by several key properties:</p> <ol> <li>Node Color: Each node is colored either red or black.</li> <li>Root Property: The root node is always black.</li> <li>Red Node Property: Red nodes cannot have red children. This means no two red nodes can be adjacent.</li> <li>Black Height Property: Every path from a node to its descendant NULL nodes must contain the same number of black nodes. This consistent count of black nodes is known as the black height.</li> <li>New Nodes are Red: Newly inserted nodes are always colored red. This helps maintain the black height property with fewer rotations.</li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#insertion","title":"Insertion","text":"<p>Inserting a new node into a Red-Black tree involves several steps:</p> <ol> <li>BST Insertion: Initially, the node is inserted according to the rules of a binary search tree (BST), based on key comparisons.</li> <li>Coloring: The new node is colored red.</li> <li>Rebalancing: The tree is adjusted to maintain the Red-Black properties through recoloring and rotations.</li> </ol> <p>The rebalancing phase takes into account various cases depending on the color of the uncle node (the sibling of the new node's parent) and the structure of the tree, aiming to fix any violations of the Red-Black properties.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#deletion","title":"Deletion","text":"<p>Deletion in a Red-Black tree is more complex due to the balancing properties:</p> <ol> <li>Node Removal: Follows the BST deletion rules. If the node to be deleted has two children, it's replaced with its in-order successor or predecessor, which is then deleted.</li> <li>Rebalancing: If a black node was deleted or replaced, the tree might need to be rebalanced to maintain the black height.</li> </ol> <p>Rebalancing after deletion also considers several cases, focusing on the color of the sibling of the node being fixed and the color of the sibling's children.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#rotations","title":"Rotations","text":"<p>Rotations help maintain or restore Red-Black properties during insertions and deletions:</p> <ul> <li>Left Rotation: Performed when the tree is too heavy on the right, rotating a parent node to the left around its right child.</li> <li>Right Rotation: Used when the tree is too heavy on the left, rotating a parent node to the right around its left child.</li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#performance","title":"Performance","text":"<p>Operations such as search, insertion, and deletion in Red-Black trees have a time complexity of \\(O(\\log n)\\), where \\(n\\) is the number of nodes. This efficiency is due to the tree's self-balancing nature, ensuring operations remain fast.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#python-implementation_1","title":"Python Implementation","text":"<pre><code>class RedBlackTreeMap(BinarySearchTree):\n  \"\"\"Sorted map implementation using a red-black tree.\"\"\"\n\n  #-------------------------- nested _Node class --------------------------\n  class _Node(TreeMap._Node):\n    \"\"\"Node class for red-black tree maintains bit that denotes color.\"\"\"\n    __slots__ = '_red'     # add additional data member to the Node class\n\n    def __init__(self, element, parent=None, left=None, right=None):\n      super().__init__(element, parent, left, right)\n      self._red = True     # new node red by default\n\n  #------------------------- positional-based utility methods -------------------------\n  # we consider a nonexistent child to be trivially black\n  def _set_red(self, p): p._node._red = True\n  def _set_black(self, p): p._node._red = False\n  def _set_color(self, p, make_red): p._node._red = make_red\n  def _is_red(self, p): return p is not None and p._node._red\n  def _is_red_leaf(self, p): return self._is_red(p) and self.is_leaf(p)\n\n  def _get_red_child(self, p):\n    \"\"\"Return a red child of p (or None if no such child).\"\"\"\n    for child in (self.left(p), self.right(p)):\n      if self._is_red(child):\n        return child\n    return None\n\n  #------------------------- support for insertions -------------------------\n  def _rebalance_insert(self, p):\n    self._resolve_red(p)                         # new node is always red\n\n  def _resolve_red(self, p):\n    if self.is_root(p):\n      self._set_black(p)                         # make root black\n    else:\n      parent = self.parent(p)\n      if self._is_red(parent):                   # double red problem\n        uncle = self.sibling(parent)\n        if not self._is_red(uncle):              # Case 1: misshapen 4-node\n          middle = self._restructure(p)          # do trinode restructuring\n          self._set_black(middle)                # and then fix colors\n          self._set_red(self.left(middle))\n          self._set_red(self.right(middle))\n        else:                                    # Case 2: overfull 5-node\n          grand = self.parent(parent)            \n          self._set_red(grand)                   # grandparent becomes red\n          self._set_black(self.left(grand))      # its children become black\n          self._set_black(self.right(grand))\n          self._resolve_red(grand)               # recur at red grandparent\n\n  #------------------------- support for deletions -------------------------\n  def _rebalance_delete(self, p):\n    if len(self) == 1:                                     \n      self._set_black(self.root())  # special case: ensure that root is black\n    elif p is not None:\n      n = self.num_children(p)\n      if n == 1:                    # deficit exists unless child is a red leaf\n        c = next(self.children(p))\n        if not self._is_red_leaf(c):\n          self._fix_deficit(p, c)\n      elif n == 2:                  # removed black node with red child\n        if self._is_red_leaf(self.left(p)):\n          self._set_black(self.left(p))\n        else:\n          self._set_black(self.right(p))\n\n  def _fix_deficit(self, z, y):\n    \"\"\"Resolve black deficit at z, where y is the root of z's heavier subtree.\"\"\"\n    if not self._is_red(y): # y is black; will apply Case 1 or 2\n      x = self._get_red_child(y)\n      if x is not None: # Case 1: y is black and has red child x; do \"transfer\"\n        old_color = self._is_red(z)\n        middle = self._restructure(x)\n        self._set_color(middle, old_color)   # middle gets old color of z\n        self._set_black(self.left(middle))   # children become black\n        self._set_black(self.right(middle))\n      else: # Case 2: y is black, but no red children; recolor as \"fusion\"\n        self._set_red(y)\n        if self._is_red(z):\n          self._set_black(z)                 # this resolves the problem\n        elif not self.is_root(z):\n          self._fix_deficit(self.parent(z), self.sibling(z)) # recur upward\n    else: # Case 3: y is red; rotate misaligned 3-node and repeat\n      self._rotate(y)\n      self._set_black(y)\n      self._set_red(z)\n      if z == self.right(y):\n        self._fix_deficit(z, self.left(z))\n      else:\n        self._fix_deficit(z, self.right(z))\n</code></pre>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#splay-trees","title":"Splay Trees","text":"<p>Splay trees are a type of self-adjusting binary search tree with the unique property that recently accessed elements are moved to the root of the tree using a process called splaying. This makes frequently accessed elements quicker to access again in the future.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#key-characteristics","title":"Key Characteristics","text":"<p>Unlike traditional binary search trees, splay trees do not maintain strict balance. Instead, they perform a series of rotations (splays) to move an accessed node to the root. This adaptive strategy optimizes the tree's structure based on usage patterns, often providing better performance for sequences of non-random operations.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#splaying-operations","title":"Splaying Operations","text":"<p>Splaying involves performing rotations on a node until it becomes the root of the tree. There are several types of splay operations, depending on the position of the node and its parents:</p> <ul> <li>Zig: A single rotation is used when the node is a direct child of the root.</li> <li>Zig-Zig: A double rotation is used when both the node and its parent are either left children or right children.</li> <li>Zig-Zag: A double rotation of different types (first right then left, or first left then right) is used when the node is a right child and its parent is a left child, or vice versa.</li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#insertion-in-splay-trees","title":"Insertion in Splay Trees","text":"<p>Insertion in a splay tree follows these steps:</p> <ol> <li>Insert the new node: First, insert the new node using the standard binary search tree insertion procedure.</li> <li>Splay the node: After insertion, splay the inserted node, moving it to the root of the tree.</li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#deletion-in-splay-trees","title":"Deletion in Splay Trees","text":"<p>Deletion in a splay tree involves:</p> <ol> <li>Splay the node: First, splay the node to be deleted, bringing it to the root of the tree.</li> <li>Remove the node: If the node has two children, replace it with its successor or predecessor, then splay that node to the root and remove the original node. If it has one or no children, simply remove it as in a BST.</li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#searching-in-splay-trees","title":"Searching in Splay Trees","text":"<p>To search for a value in a splay tree:</p> <ol> <li>Perform standard BST search: Find the node containing the value.</li> <li>Splay the node: Whether found or not, splay the last accessed node to the root.</li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#performance-of-splay-trees","title":"Performance of Splay Trees","text":"<p>The performance of splay trees is not strictly \\(O(\\log n)\\) for individual operations. However, they offer amortized \\(O(\\log n)\\) performance over a sequence of operations due to the self-adjusting nature of splaying. This makes them particularly effective for applications with non-uniform access patterns.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#24-trees","title":"(2,4) Trees","text":"<p>(2,4) trees, also known as 2-4 trees, are a type of balanced tree data structure that provides an efficient way to manage and search for data. They are a special case of B-trees and are particularly useful in databases and filesystems.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#key-characteristics_1","title":"Key Characteristics","text":"<ul> <li>Node Capacity: Each node in a (2,4) tree can have between 2 and 4 children, and accordingly, 1 to 3 keys.</li> <li>Balanced Height: All leaves are at the same depth, which ensures that operations are performed in logarithmic time.</li> <li>Flexible Node Configuration: The ability to have a variable number of children and keys allows (2,4) trees to remain balanced with a minimal number of tree rotations and rebalancing.</li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#operations-in-24-trees","title":"Operations in (2,4) Trees","text":""},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#insertion_1","title":"Insertion","text":"<p>When inserting a new key into a (2,4) tree: 1. Find the correct leaf node where the key should be inserted, following standard tree search procedures. 2. Insert the key into the node.    - If the node has fewer than 3 keys, simply add the new key.    - If the node already contains 3 keys, it is split into two nodes, each with fewer keys, and the middle key is moved up to the parent. This process may propagate up the tree.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#deletion_1","title":"Deletion","text":"<p>Deletion from a (2,4) tree involves: 1. Locating the key to be deleted. 2. Removing the key and rebalancing the tree.    - If removing the key would leave a node with fewer than 1 key, keys or nodes are redistributed to maintain the tree's balance. This might involve borrowing a key from a sibling node or merging with a sibling if both have the minimum number of keys.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#searching","title":"Searching","text":"<p>Searching for a key in a (2,4) tree follows the basic principles of binary search within the keys of each visited node and choosing the appropriate child to continue the search based on the comparison.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#balancing-and-performance","title":"Balancing and Performance","text":"<ul> <li>Balancing: (2,4) trees are self-balancing, with each operation ensuring that the tree remains balanced, thus maintaining its performance guarantees.</li> <li>Performance: The operations of search, insertion, and deletion in a (2,4) tree all have a worst-case time complexity of \\(O(\\log n)\\), where \\(n\\) is the number of keys in the tree.</li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#segment-trees","title":"Segment Trees","text":"<p>TODO: Add notes here - leaving blank for now.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/trees.html#fenwick-trees","title":"Fenwick Trees","text":"<p>TODO: Add notes here - leaving blank for now.</p> <p>References</p> <ol> <li> <p>Skiena, Steven S.. (2008). The Algorithm Design Manual, 2<sup>nd</sup> ed. (2). : Springer Publishing Company.\u00a0\u21a9</p> </li> <li> <p>Goodrich, M. T., Tamassia, R., &amp; Goldwasser, M. H. (2013). Data Structures and Algorithms in Python (1<sup>st</sup> ed.). Wiley Publishing.\u00a0\u21a9</p> </li> <li> <p>Cormen, T. H., Leiserson, C. E., &amp; Rivest, R. L. (1990). Introduction to algorithms. Cambridge, Mass. : New York, MIT Press.\u00a0\u21a9</p> </li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/tries.html","title":"Tries","text":"<p>Tries, also known as digital trees or radix trees, are tree-like data structures used for storing and searching a set of strings or keys. Tries offer efficient lookup operations, especially for string-related tasks like autocomplete, spell checking, and searching.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/tries.html#standard-tries","title":"Standard Tries:","text":"<ul> <li>Definition: A standard trie is a tree structure where each node represents a single character of the string. The root node is typically empty, and each edge corresponds to a character in the string. Nodes are labeled with characters, and each node may have multiple children representing possible continuations of the string.</li> <li>Representation in Python: Tries can be represented using nested dictionaries or arrays. Each node can be a dictionary where keys represent characters and values represent pointers to child nodes.</li> <li>Complexity:<ul> <li>Space Complexity: \\(O(n \\cdot m)\\), where \\(n\\) is the number of strings and \\(m\\) is the average length of the strings.</li> <li>Search Complexity: \\(O(m)\\), where \\(m\\) is the length of the string being searched.</li> </ul> </li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/tries.html#compressed-tries","title":"Compressed Tries:","text":"<ul> <li>Definition: Compressed tries aim to reduce space consumption by merging chains of nodes with a single child into a single node. This compression technique reduces the memory footprint of the trie while preserving search efficiency.</li> <li>Representation in Python: Compressed tries can also be represented using nested dictionaries or arrays similar to standard tries. The main difference lies in the compression algorithm used during trie construction.</li> <li>Complexity:<ul> <li>Space Complexity: Varies based on the compression ratio achieved. In practice, it can significantly reduce space consumption compared to standard tries.</li> <li>Search Complexity: Similar to standard tries, \\(O(m)\\).</li> </ul> </li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/tries.html#compression-algorithm","title":"Compression Algorithm:","text":"<ol> <li> <p>Iterative Compression: Starting from the root of the trie, traverse each branch of the trie. If a node has only one child, merge it with its child node and continue the traversal until no further compression can be performed.</p> </li> <li> <p>Updating Pointers: During compression, ensure that pointers to the merged nodes are updated appropriately. Update the parent node's reference to point directly to the merged child node.</p> </li> <li> <p>Recursive Compression: To handle deeper levels of compression, the compression process can be applied recursively to each child node.</p> </li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/tries.html#suffix-tries","title":"Suffix Tries:","text":"<ul> <li>Definition: Suffix tries are a type of trie specifically designed to store all suffixes of a given string. They are useful in pattern matching algorithms and string-related tasks where efficient suffix searches are required.</li> <li>Representation in Python: Suffix tries can be represented similarly to standard tries, but they specifically store all suffixes of a string. Each node in the trie represents a suffix of the original string.</li> <li>Complexity:<ul> <li>Space Complexity: \\(O(n^2)\\), where \\(n\\) is the length of the original string.</li> <li>Search Complexity: \\(O(m)\\), where \\(m\\) is the length of the search string.</li> </ul> </li> </ul>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/tries.html#construction-algorithm","title":"Construction Algorithm:","text":"<ol> <li> <p>Suffix Tree: Start with an empty trie. Insert all suffixes of the input string into the trie, ensuring that each suffix is inserted as a separate branch from the root.</p> </li> <li> <p>Suffix Link: For each internal node representing a suffix, add a suffix link pointing to the node representing the longest proper suffix of the current suffix.</p> </li> <li> <p>Leaf Nodes: Leaf nodes in the trie represent the end positions of suffixes in the original string.</p> </li> </ol>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/tries.html#search-engine-indexing","title":"Search Engine Indexing","text":"<p>Search engine indexing is the process of organizing and storing information retrieved from web pages to facilitate efficient and accurate search queries. Central to this process is the construction of an inverted index, which maps each unique term in a document collection to the documents that contain it.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/tries.html#inverted-index","title":"Inverted Index:","text":"<p>An inverted index is a data structure that maps terms (words or phrases) to the documents that contain them. Instead of searching through entire documents, a search engine can quickly locate relevant documents by querying the inverted index.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/tries.html#occurrence-lists","title":"Occurrence Lists:","text":"<p>For each term in the inverted index, an occurrence list (also known as a posting list) is maintained. The occurrence list contains information about the occurrences of the term in the document collection, such as the document IDs or positions where the term appears.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/tries.html#index-terms","title":"Index Terms:","text":"<p>Index terms refer to the terms extracted from documents that are indexed for searching. Commonly, index terms undergo preprocessing steps like tokenization, stemming, and stop word removal to improve search accuracy and efficiency.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/tries.html#data-structure","title":"Data Structure:","text":"<p>The inverted index is typically implemented using a data structure like a hash table or a balanced tree (e.g., B-tree or AVL tree) for efficient term lookup. Each entry in the inverted index points to an occurrence list containing information about the occurrences of the term.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/tries.html#example","title":"Example:","text":"<p>Consider a small document collection with the following documents:</p> <ol> <li>Document 1: \"The quick brown fox jumps over the lazy dog.\"</li> <li>Document 2: \"A brown fox is quick and agile.\"</li> <li>Document 3: \"The dog is lazy but friendly.\"</li> </ol> <p>The inverted index for this collection might look like:</p> Term Occurrence List a {2} agile {2} and {2} brown {1, 2} but {3} dog {1, 3} fox {1, 2} friendly {3} is {2, 3} jumps {1} lazy {1, 3} over {1} quick {1, 2} the {1, 3} <p>In this example, each term is mapped to the documents where it occurs, forming the inverted index. For instance, the term \"brown\" appears in documents 1 and 2, so its occurrence list contains document IDs 1 and 2.</p>"},{"location":"Technical/Algorithms%20and%20Data%20Structures/Data%20Structures/tries.html#importance","title":"Importance:","text":"<p>Search engine indexing is crucial for enabling fast and accurate information retrieval from large document collections. By organizing documents into an inverted index and maintaining occurrence lists, search engines can efficiently identify relevant documents in response to user queries.</p> <p>References</p> <ol> <li> <p>Skiena, Steven S.. (2008). The Algorithm Design Manual, 2<sup>nd</sup> ed. (2). : Springer Publishing Company.\u00a0\u21a9</p> </li> <li> <p>Goodrich, M. T., Tamassia, R., &amp; Goldwasser, M. H. (2013). Data Structures and Algorithms in Python (1<sup>st</sup> ed.). Wiley Publishing.\u00a0\u21a9</p> </li> <li> <p>Cormen, T. H., Leiserson, C. E., &amp; Rivest, R. L. (1990). Introduction to algorithms. Cambridge, Mass. : New York, MIT Press.\u00a0\u21a9</p> </li> </ol>"},{"location":"Technical/CUDA/references.html","title":"References","text":""},{"location":"Technical/CUDA/references.html#references","title":"References","text":"<ol> <li> <p>KeOps: Kernel Operations on the GPU with autodiff https://github.com/getkeops/keops\u00a0\u21a9</p> </li> <li> <p>NVIDIA CUDA Training Series https://www.olcf.ornl.gov/cuda-training-series/\u00a0\u21a9</p> </li> </ol>"},{"location":"Technical/Distributed%20Systems%20Design/ACID%20vs%20BASE.html","title":"ACID vs BASE","text":""},{"location":"Technical/Distributed%20Systems%20Design/ACID%20vs%20BASE.html#acid","title":"ACID","text":"<p>ACID is a set of properties that guarantee the reliability and integrity of database transactions, especially in traditional relational database management systems (RDBMS). The properties are:</p> <ol> <li> <p>Atomicity: A transaction is an indivisible and indivisible unit of work, which either completes entirely or does not execute at all. If any part of the transaction fails, the entire transaction is aborted, and the database state is left unchanged.</p> </li> <li> <p>Consistency: A transaction must move the database from one valid state to another valid state, according to all defined rules, constraints, cascades, and triggers. Transactions cannot violate database constraints like unique keys, foreign keys, etc.</p> </li> <li> <p>Isolation: Concurrent transactions are isolated from each other, meaning that the intermediate state of a transaction is invisible to other transactions. This prevents dirty reads, non-repeatable reads, and phantom reads.</p> </li> <li> <p>Durability: Once a transaction is committed, it will remain committed even in the case of a system failure (e.g., power outage or crash). The results are permanently stored in the database.</p> </li> </ol> <p>ACID properties are essential for systems that require data integrity and consistency, such as banking, finance, and e-commerce applications.</p>"},{"location":"Technical/Distributed%20Systems%20Design/ACID%20vs%20BASE.html#base","title":"BASE","text":"<p>BASE is a model that prioritizes availability and partition tolerance over consistent behavior in distributed systems, particularly in large-scale web applications and NoSQL databases. The properties are:</p> <ol> <li> <p>Basically Available: The system guarantees availability, i.e., every request receives a response, although the response could be incomplete or inconsistent with the desired state.</p> </li> <li> <p>Soft State: The state of the system may change over time, even without any input from clients, due to eventual consistency mechanisms or background processes.</p> </li> <li> <p>Eventual Consistency: The system will eventually become consistent once it stops receiving inputs. The data will converge to a consistent state after a period of time, but the system may show inconsistent data temporarily.</p> </li> </ol> <p>BASE properties are useful in systems where high availability and partition tolerance are more important than strong consistency, such as real-time analytics, content delivery networks, and collaborative applications.</p>"},{"location":"Technical/Distributed%20Systems%20Design/ACID%20vs%20BASE.html#trade-offs","title":"Trade-offs","text":"<p>ACID and BASE represent different trade-offs between consistency and availability in distributed systems, according to the CAP theorem (Consistency, Availability, Partition Tolerance). ACID systems prioritize strong consistency, while BASE systems prioritize availability and partition tolerance.</p> <p>ACID systems are suitable for applications that require strict data integrity, such as financial transactions or database systems, but may suffer from reduced availability or scalability in distributed environments.</p> <p>BASE systems are better suited for large-scale distributed systems, where high availability and partition tolerance are more important than strong consistency. However, they may exhibit temporary inconsistencies, which might be acceptable for certain types of applications, such as real-time analytics or collaborative editing tools.</p> <p>The choice between ACID and BASE depends on the specific requirements of the application, the trade-offs between consistency and availability, and the tolerance for potential data inconsistencies.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Asynchronous%20Processing.html","title":"Asynchronous and Realtime Processing","text":"<p>Asynchronous and real-time processing are essential concepts in distributed systems, enabling efficient and responsive handling of data and events. These processing paradigms play a crucial role in achieving scalability, fault tolerance, and responsiveness in modern distributed architectures.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Asynchronous%20Processing.html#asynchronous","title":"Asynchronous","text":"<p>Asynchronous processing involves executing tasks independently of the main program flow, allowing systems to handle high loads and scale effectively. Key concepts and techniques include:</p>"},{"location":"Technical/Distributed%20Systems%20Design/Asynchronous%20Processing.html#key-concepts","title":"Key Concepts:","text":"<ul> <li>Asynchronous Communication: Decouples components, allowing them to communicate without synchronization.</li> <li>Non-blocking Operations: Operations that do not halt program execution while waiting for results.</li> <li>Concurrency: Simultaneous execution of multiple tasks.</li> <li>Event-Driven Architecture: Components react to events asynchronously.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Asynchronous%20Processing.html#techniques","title":"Techniques:","text":"<ul> <li>Message Queues: Decouples producers and consumers, enabling asynchronous communication.</li> <li>Callback Functions: Executed upon completion of tasks.</li> <li>Futures/Promises: Represent results of asynchronous operations.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Asynchronous%20Processing.html#message-queues","title":"Message Queues","text":"<p>Message queues serve as essential components for asynchronous communication in distributed systems, providing various delivery patterns and associated guarantees. While they are primarily used for asynchronous communication, they play a role in facilitating real-time processing through efficient message delivery and decoupling of components.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Asynchronous%20Processing.html#delivery-patterns","title":"Delivery Patterns:","text":"<ul> <li> <p>Point-to-Point (Queue) Pattern:</p> <ul> <li>Each message is delivered to exactly one consumer, ensuring sequential processing.</li> <li>Suitable for tasks such as task distribution and workload balancing.</li> </ul> </li> <li> <p>Publish-Subscribe (Pub/Sub) (Topic) Pattern:</p> <ul> <li>Messages are published to topics, and multiple subscribers receive copies of the same message.</li> <li>Useful for broadcasting events and implementing event-driven architectures.</li> </ul> </li> <li> <p>Competing Consumers Pattern:</p> <ul> <li>Multiple consumers compete to process messages from the same queue, enabling load balancing and parallel processing.</li> <li>Ideal for scenarios requiring scalable and concurrent message processing.</li> </ul> </li> <li> <p>Priority Queue Pattern:</p> <ul> <li>Messages are processed according to priority levels, ensuring critical tasks are processed promptly.</li> <li>Prioritizes high-priority messages while maintaining fairness in processing.</li> </ul> </li> <li> <p>Batch Processing Pattern:</p> <ul> <li>Messages are aggregated into batches and processed together to optimize throughput and reduce processing overhead.</li> <li>Enhances efficiency in handling high volumes of messages.</li> </ul> </li> <li> <p>Dead Letter Queue Pattern:</p> <ul> <li>Undelivered messages are moved to a separate queue for error handling and retry mechanisms.</li> <li>Facilitates error recovery and ensures reliable message processing.</li> </ul> </li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Asynchronous%20Processing.html#delivery-guarantees","title":"Delivery Guarantees:","text":"<ul> <li> <p>At-most-once Delivery:</p> <ul> <li>Messages are delivered to consumers at most once, with no guarantee of delivery.</li> <li>Acceptable for scenarios where occasional message loss is tolerable.</li> </ul> </li> <li> <p>At-least-once Delivery:</p> <ul> <li>Each message is delivered to consumers at least once, ensuring no message loss.</li> <li>May result in message duplication but ensures message reliability.</li> </ul> </li> <li> <p>Exactly-once Delivery:</p> <ul> <li>Messages are processed and delivered exactly once, without duplication or loss.</li> <li>Challenging to achieve but crucial for scenarios intolerant to duplicates or losses.</li> </ul> </li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Asynchronous%20Processing.html#realtime-processing","title":"Realtime Processing","text":"<p>Real-time processing involves handling data and events as they occur, with minimal latency, ensuring timely responses to events and data changes. Key concepts and techniques include:</p>"},{"location":"Technical/Distributed%20Systems%20Design/Asynchronous%20Processing.html#key-concepts_1","title":"Key Concepts:","text":"<ul> <li>Low Latency: Minimizing the time between event occurrence and processing.</li> <li>Streaming Data: Processing continuous data streams in real-time.</li> <li>Event Sourcing: Capturing and storing events for later analysis or processing.</li> <li>Stateful Processing: Maintaining state across multiple events or data streams.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Asynchronous%20Processing.html#components","title":"Components:","text":"<ul> <li>Pub/Sub (Publish/Subscribe): Messaging pattern for real-time event distribution.</li> <li>Stream Processing Frameworks: Tools for processing continuous data streams.</li> <li>Complex Event Processing (CEP): Analyzing and correlating events in real-time.</li> <li>In-memory Databases: Storing data in memory for fast access and processing.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Asynchronous%20Processing.html#kafka","title":"Kafka","text":"<p>Apache Kafka is a distributed streaming platform designed for building real-time data pipelines and applications. It provides scalable, fault-tolerant, and high-throughput messaging capabilities, making it ideal for real-time event streaming and stream processing use cases.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Asynchronous%20Processing.html#key-features","title":"Key Features","text":"<ul> <li>Distributed Messaging: Kafka distributes messages across multiple nodes in a cluster for fault tolerance and scalability.</li> <li>Topics: Messages are organized into topics, allowing producers to publish messages and consumers to subscribe to topics of interest.</li> <li>Partitions: Topics are divided into partitions, enabling parallel processing and horizontal scalability.</li> <li>Persistence: Kafka persists messages to disk, ensuring durability and enabling message replay from any point in time.</li> <li>Consumer Groups: Kafka supports consumer groups, allowing multiple consumers to independently consume messages from a topic for parallel processing and load balancing.</li> <li>Exactly-once Semantics: Kafka provides configurable message delivery semantics, including exactly-once processing, ensuring message reliability and consistency.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Asynchronous%20Processing.html#use-cases","title":"Use Cases","text":"<ul> <li>Log Aggregation: Collecting and centralizing logs from multiple sources for real-time analysis and monitoring.</li> <li>Event Sourcing: Capturing and storing events as they occur for subsequent analysis, auditing, and replay.</li> <li>Stream Processing: Performing real-time analytics, transformations, and computations on continuous data streams.</li> <li>Data Integration: Integrating data from various sources in real-time for near real-time insights and decision-making.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Asynchronous%20Processing.html#integration","title":"Integration","text":"<ul> <li>Stream Processing: Kafka integrates seamlessly with stream processing frameworks like Apache Kafka Streams, Apache Flink, and Apache Spark Streaming for real-time data processing and analytics.</li> <li>Microservices: Kafka serves as a communication backbone for building event-driven microservices architectures, enabling loosely coupled and scalable systems.</li> <li>Data Pipelines: Kafka forms the core of modern data pipelines, facilitating the ingestion, processing, and delivery of large-scale data streams across organizations.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Big%20Data%2C%20Hadoop%20and%20Spark.html","title":"Big Data, Hadoop and Spark","text":""},{"location":"Technical/Distributed%20Systems%20Design/Big%20Data%2C%20Hadoop%20and%20Spark.html#big-data","title":"Big Data","text":"<p>Big data refers to extremely large and complex datasets that traditional data processing applications are inadequate to handle efficiently. It encompasses various types of data, including structured, unstructured, and semi-structured data, and presents challenges related to storage, processing, and analysis.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Big%20Data%2C%20Hadoop%20and%20Spark.html#types-of-big-data","title":"Types of Big Data","text":"<ol> <li>Structured Data:</li> <li>Definition: Data organized in a predefined format with a well-defined schema.</li> <li> <p>Examples: Relational databases, Excel spreadsheets, CSV files.</p> </li> <li> <p>Unstructured Data:</p> </li> <li>Definition: Data that does not have a predefined data model or structure.</li> <li> <p>Examples: Text documents, images, videos, social media posts.</p> </li> <li> <p>Semi-Structured Data:</p> </li> <li>Definition: Data that does not fit neatly into a structured format but contains some organizational properties.</li> <li>Examples: JSON, XML, log files.</li> </ol>"},{"location":"Technical/Distributed%20Systems%20Design/Big%20Data%2C%20Hadoop%20and%20Spark.html#storage-solutions-for-big-data","title":"Storage Solutions for Big Data","text":"<ol> <li>Data Warehouses:</li> <li>Description: Centralized repositories for structured data collected from various sources within an organization.</li> <li>Characteristics: Optimized for query and analysis, typically using SQL-based querying languages.</li> <li> <p>Examples: Amazon Redshift, Google BigQuery, Snowflake.</p> </li> <li> <p>Data Lakes:</p> </li> <li>Description: Storage systems that can store vast amounts of structured, semi-structured, and unstructured data in its raw format.</li> <li>Characteristics: Designed for storing and processing raw data without the need for a predefined schema.</li> <li> <p>Examples: Amazon S3, Hadoop Distributed File System (HDFS), Azure Data Lake Storage.</p> </li> <li> <p>Databases:</p> </li> <li>Description: Software systems for storing, managing, and retrieving data efficiently.</li> <li>Characteristics: Can handle structured, semi-structured, and sometimes unstructured data depending on the database type.</li> <li> <p>Examples: MongoDB (NoSQL for semi-structured and unstructured data), Cassandra (NoSQL for time-series data), MySQL (Structured data).</p> </li> <li> <p>NoSQL Databases:</p> </li> <li>Description: Databases designed for handling unstructured or semi-structured data at scale.</li> <li>Characteristics: Schema-less or flexible schema design, horizontal scalability.</li> <li> <p>Examples: MongoDB, Cassandra, Couchbase.</p> </li> <li> <p>Object Storage:</p> </li> <li>Description: Storage architecture that manages data as objects rather than files or blocks.</li> <li>Characteristics: Scalability, durability, and flexibility in handling various types of data.</li> <li>Examples: Amazon S3, Google Cloud Storage, Azure Blob Storage.</li> </ol>"},{"location":"Technical/Distributed%20Systems%20Design/Big%20Data%2C%20Hadoop%20and%20Spark.html#hadoop","title":"Hadoop","text":"<p>Hadoop is an open-source distributed processing framework for big data workloads. It consists of two main components:</p>"},{"location":"Technical/Distributed%20Systems%20Design/Big%20Data%2C%20Hadoop%20and%20Spark.html#hadoop-distributed-file-system-hdfs","title":"Hadoop Distributed File System (HDFS)","text":"<ul> <li> <p>Architecture:</p> </li> <li> <p>NameNode: Manages the file system metadata and coordinates file operations.</p> </li> <li>DataNodes: Stores the actual data blocks and serves read/write requests.</li> <li> <p>Files are split into blocks (default 128MB) and replicated across multiple DataNodes for fault tolerance.</p> </li> <li> <p>Features:</p> </li> <li>Highly fault-tolerant through data replication and automatic failover.</li> <li>Suitable for batch processing of large datasets.</li> <li>Supports high throughput data access patterns.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Big%20Data%2C%20Hadoop%20and%20Spark.html#mapreduce","title":"MapReduce","text":"<ul> <li> <p>Architecture:</p> <ul> <li>JobTracker: Manages and schedules jobs on the cluster.</li> <li>TaskTrackers: Run individual Map and Reduce tasks on worker nodes.</li> </ul> </li> <li> <p>Advantages:</p> <ul> <li>Suitable for batch processing of large datasets.</li> <li>Scalable and fault-tolerant through task parallelization and automatic retries.</li> </ul> </li> <li> <p>Limitations:</p> <ul> <li>High latency due to disk I/O and batch processing nature.</li> <li>Not suitable for real-time or iterative workloads.</li> </ul> </li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Big%20Data%2C%20Hadoop%20and%20Spark.html#workflow","title":"Workflow","text":"<p>The MapReduce programming model follows a specific workflow to process large datasets in parallel across a cluster of machines:</p> <ol> <li> <p>Input Data Splitting:</p> <ul> <li>The input data (e.g., files from HDFS) is split into fixed-size chunks called <code>input splits</code> (typically 128MB or 256MB).</li> <li>Each input split is processed by an individual <code>map</code> task.</li> </ul> </li> <li> <p>Map Phase:</p> <ul> <li>The <code>map</code> tasks run in parallel across the cluster nodes.</li> <li>Each map task processes its input split by applying the user-defined <code>map</code> function.</li> <li>The map function takes key-value pairs as input and emits intermediate key-value pairs.</li> </ul> </li> <li> <p>Shuffle and Sort:</p> <ul> <li>The intermediate key-value pairs produced by the map tasks are shuffled and sorted by their keys.</li> <li>This process involves transferring data across the network to ensure that all values with the same key are grouped together.</li> <li>The shuffling is typically performed using HTTP or specialized shuffling services.</li> </ul> </li> <li> <p>Reduce Phase:</p> <ul> <li>The <code>reduce</code> tasks run in parallel across the cluster nodes.</li> <li>Each reduce task processes a bucket of intermediate key-value pairs with the same key.</li> <li>The reduce task applies the user-defined <code>reduce</code> function to the grouped values, producing the final output key-value pairs.</li> </ul> </li> <li> <p>Output Writing:</p> <ul> <li>The final output key-value pairs from the reduce tasks are written to the desired output location (e.g., HDFS, database, or other storage systems).</li> </ul> </li> <li> <p>Job Tracking and Fault Tolerance:</p> <ul> <li>The <code>JobTracker</code> (in Hadoop 1.x) or <code>ResourceManager</code> (in Hadoop 2.x) manages and monitors the execution of MapReduce jobs.</li> <li>If a task fails, it can be automatically restarted on another node, leveraging the fault tolerance capabilities of the framework.</li> </ul> </li> <li> <p>Combiner (Optional):</p> <ul> <li>A <code>combiner</code> function can be used to perform partial aggregation or filtering on the intermediate key-value pairs before the shuffle phase.</li> <li>This can reduce the amount of data transferred across the network and improve performance.</li> </ul> </li> </ol>"},{"location":"Technical/Distributed%20Systems%20Design/Big%20Data%2C%20Hadoop%20and%20Spark.html#apache-spark","title":"Apache Spark","text":"<p>Apache Spark is a fast and general-purpose cluster computing system for big data analytics. It offers several advantages over Hadoop MapReduce:</p>"},{"location":"Technical/Distributed%20Systems%20Design/Big%20Data%2C%20Hadoop%20and%20Spark.html#resilient-distributed-datasets-rdds","title":"Resilient Distributed Datasets (RDDs)","text":"<ul> <li>Immutable, partitioned collections of records cached in memory for faster processing.</li> <li>Fault-tolerant through lineage and automatic recomputation of lost partitions.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Big%20Data%2C%20Hadoop%20and%20Spark.html#in-memory-processing","title":"In-Memory Processing","text":"<ul> <li>Spark processes data in memory, avoiding expensive disk I/O operations.</li> <li>Suitable for iterative algorithms and real-time streaming workloads.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Big%20Data%2C%20Hadoop%20and%20Spark.html#spark-components","title":"Spark Components","text":"<ul> <li>Spark Core: Provides RDDs and parallel processing capabilities.</li> <li>Spark SQL: Structured data processing with SQL-like queries.</li> <li>Spark Streaming: Real-time streaming data processing.</li> <li>MLlib: Machine learning algorithms and utilities.</li> <li>GraphX: Graph processing and analysis.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Big%20Data%2C%20Hadoop%20and%20Spark.html#advantages-over-hadoop-mapreduce","title":"Advantages over Hadoop MapReduce","text":"<ul> <li>Faster processing due to in-memory computation.</li> <li>Supports iterative and interactive workloads.</li> <li>Integrated libraries for SQL, streaming, machine learning, and graph processing.</li> <li>Simplified programming model with RDDs and high-level APIs.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Big%20Data%2C%20Hadoop%20and%20Spark.html#deployment-modes","title":"Deployment Modes","text":"<ul> <li>Standalone Mode: Spark manages its own cluster resources.</li> <li>YARN Mode: Spark runs on top of Hadoop YARN for resource management.</li> <li>Kubernetes Mode: Spark runs on Kubernetes for container orchestration.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Big%20Data%2C%20Hadoop%20and%20Spark.html#spark-architecture","title":"Spark Architecture","text":"<ul> <li>Driver: Coordinates the execution of Spark applications.</li> <li>Executors: Run tasks and cache data on worker nodes.</li> <li>Cluster Manager: Allocates resources (e.g., YARN, Kubernetes, Mesos).</li> </ul> <p>Hadoop and Spark are complementary technologies, with Hadoop providing a robust distributed file system and batch processing capabilities, while Spark offers faster in-memory processing and advanced analytics capabilities.</p>"},{"location":"Technical/Distributed%20Systems%20Design/CAP%20Theorem.html","title":"CAP Theorem","text":"<p>A theoretical framework to understand tradeoffs in data intensive distributed systems. It states that a distributed system can only have two of the three characteristics of consistency, availability, and partition tolerance.</p> <ul> <li>Consistency - also known as <code>CAP consistency, strong consistency, or strict consistency</code> - means that simultaneous read requests at different nodes always return the same data. Additionally all operations \"appear\" atomic i.e. when a write transaction has been completed, all read requests from all clients must reflect the changes from that write transaction. Different from ACID Consistency</li> <li>Availability - every request to the system at any node recieves a response</li> <li>Partition Tolerance - when a network's nodes are split into groups that cannot communicate with each other, the system continues to operate despite the communication breakdown.</li> </ul> <p>For distributed system partition tolerance is a necessary characteristic - so the choice is usually between consistency and availability. Hence effectively the theorem states that during a network partition, a distributed system can provide either availability or consistency.</p> <p></p> <ul> <li>CP System: CP means the system exhibits consistency and partition tolerance at the cost of availability. During a network partition, a CP system either rejects write requests or shuts down the inconsistent nodes. Both scenarios reduce availability but provide consistency.</li> <li>AP System: AP means that the system exhibits availability, and partition tolerance at the cost of consistency. During a network partition, an AP database continues to process write requests and allows inconsistent nodes to operate, but the inconsistent nodes may have stale data. Hence the system needs to resync all nodes when the network partition is repaired.</li> <li>CA System: CA means the system exhibits availability and consistency at the cost of partition tolerance. This is not useful for distributed systems as they all suffer from network partitions.</li> </ul> <p>However in practice the boundaries are not so hard - for example distributed systems rarely have 100% availability, but instead availability is denoted by the percent of uptime (e.g. four or five 9's).</p>"},{"location":"Technical/Distributed%20Systems%20Design/Caching.html","title":"Caching","text":"<p>Caching is a technique used in software systems to store frequently accessed or computationally expensive data in a temporary storage area known as a cache. By keeping this data readily accessible, caching aims to improve system performance and responsiveness.</p> <p></p>"},{"location":"Technical/Distributed%20Systems%20Design/Caching.html#benefits-of-caching","title":"Benefits of Caching","text":"<ul> <li>Faster Access: Cached data can be retrieved more quickly than fetching it from its original source.</li> <li>Reduced Latency: Caching reduces the time required to access data, thereby decreasing latency for user requests.</li> <li>Improved Scalability: Caching helps alleviate the load on backend systems by serving cached data, enabling better scalability.</li> <li>Bandwidth Conservation: Caching reduces the amount of data transmitted over the network, conserving bandwidth.</li> <li>Enhanced User Experience: Faster response times lead to a better user experience, particularly for applications with high traffic volumes.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Caching.html#locations-of-caching","title":"Locations of Caching","text":"<ul> <li> <p>Local/Colocated Cache</p> </li> <li> <p>Data is stored in memory on the same server/process as the application, providing fast access and low latency.</p> </li> <li> <p>Suitable for scenarios where data is frequently accessed by a single application instance and does not need to be shared across multiple nodes.</p> </li> <li> <p>Distributed Cache</p> </li> <li> <p>Caches are distributed across multiple nodes or servers to improve scalability and fault tolerance.</p> </li> <li>Data is replicated or partitioned across nodes, allowing for horizontal scaling and resilience to node failures.</li> <li> <p>Suitable for scenarios where data needs to be shared and accessed by multiple application instances or nodes in a distributed environment.</p> </li> <li> <p>Client-side Caching</p> </li> <li> <p>Browser Caching: Web browsers cache resources like HTML, CSS, and JavaScript files to avoid redundant downloads and improve page load times.</p> </li> <li> <p>Local Storage: Client-side storage mechanisms like localStorage and sessionStorage allow web applications to store data locally in the browser, reducing server requests and improving performance.</p> </li> <li> <p>Web Server Caching</p> </li> <li> <p>Proxy Caching: Intermediate proxies or reverse proxies cache responses from backend servers, reducing server load and improving response times for subsequent requests.</p> </li> <li> <p>Application-level Caching: Web servers and application frameworks provide mechanisms to cache dynamic content or database query results, reducing processing overhead and improving scalability.</p> </li> <li> <p>Content Delivery Network (CDN) Caching</p> </li> <li> <p>Static or cacheable content is cached on edge servers distributed geographically to reduce latency for users worldwide.</p> </li> <li> <p>CDN caches commonly include static assets like images, videos, and documents, improving their delivery speed and reducing server load.</p> </li> <li> <p>Database Caching</p> </li> <li>Queries and query results are cached to reduce the load on the database and improve query response times.</li> <li>Database caching can be implemented at various levels, including query result caching, object caching, or query plan caching, depending on the database system and application requirements.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Caching.html#terminology","title":"Terminology","text":"<ul> <li> <p>Cache Hit:</p> </li> <li> <p>A cache hit occurs when a requested item is found in the cache, resulting in a faster response time as the data can be retrieved directly from the cache.</p> </li> <li> <p>Cache Miss:</p> </li> <li> <p>A cache miss occurs when a requested item is not found in the cache, requiring the data to be fetched from its original source, resulting in a longer response time.</p> </li> <li> <p>Cache Hit Ratio (Hit Rate):</p> </li> <li> <p>The cache hit ratio, or hit rate, is the ratio of cache hits to the total number of cache accesses (hits plus misses). It indicates the effectiveness of the cache in serving requests and is often expressed as a percentage.</p> </li> <li> <p>Cache Invalidation:</p> </li> <li> <p>Cache invalidation is the process of removing or updating cached items when the underlying data changes, ensuring the cache remains consistent with the source of truth. This prevents stale or outdated data from being served to users.</p> </li> <li> <p>TTL (Time to Live):</p> </li> <li> <p>TTL, or Time to Live, is a parameter that specifies the duration for which cached data remains valid before it is considered stale and needs to be refreshed from the original source. It is commonly used in cache systems to control the lifespan of cached items.</p> </li> <li> <p>Cache Coherence (Cache Consistency):</p> </li> <li> <p>Cache coherence, or cache consistency, refers to the property of ensuring that multiple caches holding copies of the same data remain synchronized and consistent with each other. This ensures that all cache copies reflect the most recent updates to the data.</p> </li> <li> <p>Cache Replacement Policy:</p> </li> <li> <p>Cache replacement policy defines the strategy used to select which items in the cache should be evicted when the cache reaches its capacity and needs to make space for new entries. Common replacement policies include Least Recently Used (LRU), First-In-First-Out (FIFO), and Random Replacement.</p> </li> <li> <p>Cache Write Policy:</p> </li> <li> <p>Cache write policy determines how data modifications are handled in the cache. Common write policies include Write-Through, where data is written to both the cache and the underlying storage simultaneously, and Write-Back, where data is initially written only to the cache and later flushed to the underlying storage.</p> </li> <li> <p>Cache Read Policy:</p> </li> <li>Cache read policy determines how read requests are handled in the cache. Common read policies include Read-Through, where read requests are served directly from the cache if the data is available, and Read-Ahead, where the cache anticipates future read requests and pre-fetches data into the cache before it is requested.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Caching.html#best-practices","title":"Best Practices","text":"<ul> <li>Identify Hotspots: Determine which data should be cached based on access patterns and usage frequency.</li> <li>Monitor Cache Performance: Regularly monitor cache hit rates, miss rates, and eviction rates to optimize cache configuration.</li> <li>Consider Cache Expiration: Set appropriate expiration times for cached data to balance freshness with performance.</li> <li>Plan for Cache Warming: Pre-load the cache with frequently accessed data during system startup or maintenance periods to minimize cache misses.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Caching.html#cache-replacement-policies","title":"Cache Replacement Policies","text":"Policy Description Pros Cons Least Recently Used (LRU) The least recently used items are evicted from the cache when it reaches its capacity. Effective in removing items that are least likely to be used in the near future. Requires tracking of access times for cache items, which may introduce overhead. Least Frequently Used (LFU) The least frequently used items are evicted from the cache when it reaches its capacity. Removes items that have been accessed the least number of times, reducing cache pollution. Requires maintaining access frequency counts for cache items, which may consume additional memory and processing power. Most Recently Used (MRU) The most recently used items are retained in the cache, and the least recently used items are evicted when it reaches its capacity. Ensures that recently accessed items remain in the cache, improving cache hit rates. May lead to cache pollution if frequently accessed items are not representative of future access patterns. First-In-First-Out (FIFO) The oldest items are evicted from the cache first when it reaches its capacity. Simple and straightforward eviction strategy. May not reflect the actual access patterns of cache items. Last-In-First-Out (LIFO) The most recently added items are evicted from the cache first when it reaches its capacity. Reflects the order in which items were added to the cache. May not be suitable for scenarios where recently added items are more likely to be accessed again. Random Replacement Randomly selects items from the cache for eviction when it reaches its capacity. Simple to implement and does not require tracking access patterns. May result in suboptimal cache performance if important or frequently accessed items are evicted randomly."},{"location":"Technical/Distributed%20Systems%20Design/Caching.html#choosing-the-right-replacement-policy","title":"Choosing the Right Replacement Policy","text":"<p>LRU and LFU are the most commonly used.</p> <ul> <li>Considerations:</li> <li>Maximizing Hit Ratio: Select a cache replacement policy that maximizes the hit ratio by evicting the least useful items from the cache.</li> <li>Data Update Frequency: Choose a policy that aligns with the frequency of data updates in the application.</li> <li>Consistency Requirements: Determine the level of consistency required between cached data and the source of truth.</li> <li>Performance Overhead: Evaluate the overhead introduced by different invalidation mechanisms on system performance and scalability.</li> <li>Combination: In many cases, a combination of invalidation policies may be used to address different requirements for different data sets within the application.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Caching.html#lru-code-example","title":"LRU Code Example","text":"<pre><code>from collections import OrderedDict\n\nclass LRUCache:\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.cache = OrderedDict()\n\n    def get(self, key):\n        if key in self.cache:\n            # Move the accessed key to the end to indicate it's the most recently used\n            self.cache.move_to_end(key)\n            return self.cache[key]\n        return -1\n\n    def put(self, key, value):\n        if key in self.cache:\n            # If key already exists, update its value and move it to the end\n            self.cache[key] = value\n            self.cache.move_to_end(key)\n        else:\n            # Check if cache is full\n            if len(self.cache) &gt;= self.capacity:\n                # If cache is full, remove the least recently used item (first item)\n                self.cache.popitem(last=False)\n            # Add the new key-value pair to the cache\n            self.cache[key] = value\n\n# Example usage\ncache = LRUCache(2)  # Create a cache with capacity 2\ncache.put(1, 1)\ncache.put(2, 2)\nprint(cache.get(1))  # Output: 1\ncache.put(3, 3)  # Evicts key 2, as key 1 was accessed most recently\nprint(cache.get(2))  # Output: -1 (Key 2 is no longer in the cache)\ncache.put(4, 4)  # Evicts key 1\nprint(cache.get(1))  # Output: -1 (Key 1 is no longer in the cache)\nprint(cache.get(3))  # Output: 3\nprint(cache.get(4))  # Output: 4\n</code></pre>"},{"location":"Technical/Distributed%20Systems%20Design/Caching.html#cache-read-policies","title":"Cache Read Policies","text":"<ol> <li> <p>Read-Through:</p> </li> <li> <p>With this policy, when a read request is made for a data item, the cache first checks if the item is present. If the item is found in the cache, it is returned to the requester. If the item is not found, the cache fetches the data from the underlying data source (such as a database), stores it in the cache, and then returns it to the requester.</p> </li> <li> <p>Read-Ahead:</p> </li> <li> <p>Read-ahead, also known as prefetching, involves anticipating future read requests and proactively fetching data items into the cache before they are requested. This policy aims to reduce latency by preloading data items into the cache in anticipation of future access.</p> </li> <li> <p>Cache-Aside:</p> </li> <li>In cache-aside (also known as lazy loading), the application code is responsible for checking the cache for a requested item before accessing the underlying data source. If the item is found in the cache, it is returned to the requester. If not, the data is fetched from the underlying data source, stored in the cache, and then returned to the requester.</li> </ol>"},{"location":"Technical/Distributed%20Systems%20Design/Caching.html#cache-write-policies","title":"Cache Write Policies","text":"<ol> <li> <p>Write-Through:</p> </li> <li> <p>Data is written to both the cache and the underlying storage simultaneously. Acknowledgments are sent only after both the cache and the storage have been updated.</p> </li> <li> <p>Write-Back:</p> </li> <li> <p>Data modifications are initially written only to the cache and later asynchronously flushed to the underlying storage. Acknowledgments are sent once data is written to the cache, with flushing to the storage happening in the background.</p> </li> <li> <p>Write-Around:</p> </li> <li>Data modifications are written directly to the underlying storage, bypassing the cache. Acknowledgments are sent only after data is successfully written to the storage. Data is loaded into the cache on a read cache miss.</li> </ol>"},{"location":"Technical/Distributed%20Systems%20Design/Caching.html#tradeoffs","title":"Tradeoffs","text":"Write Policy Advantages Disadvantages Best Use Cases Write-Through - Strong consistency between cache and storage. - May impact availability due to synchronous writes to storage. - Applications requiring strong data consistency. - Guarantees data durability since data is written to storage immediately. - Transactional systems and applications where data written is re-read immediately and frequently. - Minimizes risk of data loss in case of cache failure. Write-Back - Improved write performance due to asynchronous writes to storage. - Potential inconsistency until data is flushed to storage. - Applications with high write throughput that can accept data inconsistency or loss in order to optimize latency. - Enhances cache availability with immediate acknowledgment. - Increased risk of data loss if cache fails before flushing to storage. - Caching systems prioritizing performance over strong consistency. - Reduces I/O latency by delaying writes to storage. Write-Around - Reduces cache pollution for infrequently accessed data. - Eventual consistency; cache may not always have the latest data. - Archival or logging systems where data is written once and rarely read. - Minimizes cache overhead for data with unpredictable access patterns. - Increased read latency for recently written data not yet in cache. - Applications prioritizing write performance over read performance. - Prevents cache from filling with data that will not be re-accessed."},{"location":"Technical/Distributed%20Systems%20Design/Consensus%20Algorithms.html","title":"Consensus Algorithms","text":"<p>Algorithms that achieve consensus and help solve atomic commits. </p> <p>Atomic commits are when we want distributed transactions that touch multiple partitions (needed in a partitioned relational database) to succeed or fail together. This is easy to do on a single node (like traditional RDBMS) but when transactions span multiple nodes connected via a network, they all need to agree on whether the transaction is committed or oborted.</p> <p>Atomic commits are needed to prevent partitions from getting out of sync due to partially completed transactions:</p> <ul> <li>cross partition transactions</li> <li>global secondary indexes</li> <li>keeping other derived data consistent like data warehouses or cache</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Consensus%20Algorithms.html#two-phase-commit-2pc","title":"Two-Phase Commit (2PC)","text":"<p>Two-Phase Commit is a classic consensus algorithm used to ensure all nodes agree on committing a transaction in a distributed database.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Consensus%20Algorithms.html#overview","title":"Overview:","text":"<ul> <li>Coordinator-Participant Model: One node acts as the coordinator, while others are participants.</li> <li>Two Phases:</li> <li>Prepare Phase: Coordinator asks participants if they are ready to commit.</li> <li>Commit Phase: If all participants agree, coordinator instructs them to commit.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Consensus%20Algorithms.html#advantages","title":"Advantages:","text":"<ul> <li>Provides atomicity, ensuring all nodes either commit or abort a transaction.</li> <li>Guarantees consistency by coordinating transaction commits across distributed nodes.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Consensus%20Algorithms.html#limitations","title":"Limitations:","text":"<ul> <li>Synchronous: Coordination requires blocking communication, leading to potential scalability issues.</li> <li>Blocking Failure: Coordinator failure during the commit phase can lead to a blocking state, requiring timeouts for recovery.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Consensus%20Algorithms.html#raft","title":"Raft","text":"<p>Raft is a consensus algorithm designed for fault tolerance and ease of understanding, often used in distributed systems.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Consensus%20Algorithms.html#overview_1","title":"Overview:","text":"<ul> <li>Leader-Based: One node acts as the leader, coordinating replication of log entries to followers.</li> <li>Leader Election: Raft uses a leader election mechanism to ensure fault tolerance and continuity of operations.</li> <li>Log Replication: Leader replicates log entries to followers, ensuring consistency across the cluster.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Consensus%20Algorithms.html#advantages_1","title":"Advantages:","text":"<ul> <li>Simplified Design: Raft's leader-based approach and clear separation of roles make it easier to understand and implement.</li> <li>Fault Tolerance: Raft ensures fault tolerance through leader election and log replication mechanisms.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Consensus%20Algorithms.html#limitations_1","title":"Limitations:","text":"<ul> <li>Scalability: Large Raft clusters may experience performance issues due to leader bottleneck.</li> <li>Availability: Raft requires a majority of nodes to be available for progress, which can limit availability in some scenarios.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Consensus%20Algorithms.html#paxos","title":"Paxos","text":"<p>Paxos is a foundational consensus algorithm used for achieving agreement among distributed nodes, particularly in fault-tolerant systems.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Consensus%20Algorithms.html#overview_2","title":"Overview:","text":"<ul> <li>Phase-Based Protocol: Paxos operates in phases, including proposal, acceptance, and commitment.</li> <li>Leaderless: Paxos does not have a designated leader; any node can propose values.</li> <li>Quorum-Based Decision: Consensus is achieved when a quorum of nodes agrees on a value.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Consensus%20Algorithms.html#advantages_2","title":"Advantages:","text":"<ul> <li>Fault Tolerance: Paxos ensures consistency and fault tolerance in distributed systems, even in the presence of failures.</li> <li>Decentralization: Paxos does not rely on a single leader, enabling distributed decision-making.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Consensus%20Algorithms.html#limitations_2","title":"Limitations:","text":"<ul> <li>Complexity: Paxos can be challenging to understand and implement correctly due to its phase-based protocol.</li> <li>Scalability: Paxos may suffer from performance issues in large-scale deployments due to the need for quorum-based decision-making.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Consistency.html","title":"Consistency","text":"<p>Consistency is an overloaded term with multiple uses.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Consistency.html#flavors-of-consistency","title":"Flavors of Consistency","text":"<ol> <li> <p>CAP Consistency/Strong Consistency/Linearizability/Strict Consistency/Immediate Consistency</p> <ul> <li>Means that simultaneous reads at different nodes in a distributed system always return the same data. This holds even if there is a simultaneous write at one of the nodes.</li> <li>Example:</li> <li>Updating passwords</li> </ul> </li> <li> <p>Weak Consistency</p> <ul> <li>Means that simultaneous reads at different nodes could yield different values. Nodes can diverge in state, and there is no guarantee that they will converge - which is different from eventual consistency!</li> </ul> </li> <li> <p>Eventual Consistency</p> <ul> <li>Type of weak consistency where if there are no new writes, data at all nodes will eventually converge. Updates and writes arepropogated throughout the nodes of a system, but those changes will not be immediately reflected. This is associated with BASE databases.</li> <li>Examples: <ul> <li>The Domain Name System (DNS) is highly available with high throughput, but may not reflect the latest value at any given time.</li> </ul> </li> </ul> </li> <li> <p>Sequential Consistency</p> <ul> <li>Means that the ordering of write requests at a single node is preserved in a distributed system. Two sequential writes cannot be applied out of order at another node in the system.</li> <li>Example:</li> <li>Ordering of scial media posts of friends. We expect that for a given friend the comments or news posts are displayed in the order they were submitted.</li> </ul> </li> <li> <p>ACID Consistency</p> <ul> <li>Means that data stored by a database is in a correct and valid state - e.g. if a schema specifies that a value must be unique, the database will ensure that the value is unique throughout all actions. If a transaction violates data integrity constraints, that transaction is rejected, and the state is reverted. This is associated with ACID databases.</li> <li>Example: <ul> <li>if a foreign key is deleted, associated rows in other tables will also be deleted. </li> </ul> </li> </ul> </li> </ol>"},{"location":"Technical/Distributed%20Systems%20Design/Consistency.html#tradeoffs","title":"Tradeoffs","text":"<p>A common decision to make is the choice between strong and weak consistency - e.g. a database with BASE characteristics, transactions have eventual consistency, a form of weak consistency.</p> <p>This is a choice because achieving consistency in a distributed system has additional overhead and costs e.g. synchronization between nodes, verification mechanism to verify data is most up to date,</p>"},{"location":"Technical/Distributed%20Systems%20Design/Consistent%20Hashing.html","title":"Consistent Hashing","text":""},{"location":"Technical/Distributed%20Systems%20Design/Consistent%20Hashing.html#what-is-consistent-hashing","title":"What is Consistent Hashing?","text":"<p>Consistent hashing is a technique used in distributed systems for load balancing and data partitioning. It provides a way to distribute data or requests among a cluster of nodes in a way that minimizes the need for remapping when nodes are added or removed.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Consistent%20Hashing.html#why-is-consistent-hashing-useful","title":"Why is Consistent Hashing Useful?","text":"<p>Consistent hashing solves two main problems in distributed systems:</p> <ol> <li> <p>Load Balancing: It allows for an even distribution of data or requests across nodes in a cluster, ensuring no single node gets overloaded.</p> </li> <li> <p>Partition Tolerance: When a node is added or removed from the cluster, only a small fraction of data or requests need to be remapped, ensuring minimal disruption to the system.</p> </li> </ol>"},{"location":"Technical/Distributed%20Systems%20Design/Consistent%20Hashing.html#how-does-consistent-hashing-work","title":"How Does Consistent Hashing Work?","text":"<ol> <li> <p>Hash Function: A hash function is used to map data or requests to points on a circular hash ring. The hash function should provide a uniform distribution of values.</p> </li> <li> <p>Hash Ring: The hash ring is a circular space where each node in the cluster is assigned a position on the ring using the hash function.</p> </li> <li> <p>Data/Request Mapping: To map a data item or request to a node, its key is hashed using the same hash function, and the resulting value is mapped to the nearest node position on the ring in a clockwise direction.</p> </li> <li> <p>Node Addition/Removal: When a new node is added to the cluster, it is assigned a position on the ring. Only the data or requests that fall between the new node and the next node in a clockwise direction need to be remapped. Similarly, when a node is removed, only the data or requests that were previously mapped to that node need to be remapped to other nodes.</p> </li> </ol>"},{"location":"Technical/Distributed%20Systems%20Design/Consistent%20Hashing.html#advantages-of-consistent-hashing","title":"Advantages of Consistent Hashing","text":"<ul> <li>Incremental Scalability: Nodes can be added or removed without disrupting the entire system, ensuring scalability.</li> <li>Even Load Distribution: Data or requests are distributed evenly across nodes, preventing hotspots and ensuring better performance.</li> <li>Fault Tolerance: If a node fails, its data or requests are automatically remapped to other nodes, providing fault tolerance.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Consistent%20Hashing.html#disadvantages-of-consistent-hashing","title":"Disadvantages of Consistent Hashing","text":"<ul> <li>Non-uniform Load Distribution: In some cases, data or requests may not be distributed evenly across nodes due to the hash function's behavior or the distribution of keys.</li> <li>Remapping Overhead: While consistent hashing minimizes remapping, there is still some overhead involved when nodes are added or removed.</li> <li>Hot Spot Prevention: Additional techniques like virtual nodes may be required to prevent hotspots when the distribution of keys is skewed.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Consistent%20Hashing.html#use-cases-for-consistent-hashing","title":"Use Cases for Consistent Hashing","text":"<p>Consistent hashing is widely used in distributed systems, including:</p> <ul> <li>Distributed Caching: Memcached, Redis</li> <li>Distributed Storage: Cassandra, Amazon DynamoDB</li> <li>Load Balancing: HAProxy, Nginx</li> <li>Peer-to-Peer Networks: BitTorrent</li> <li>Content Delivery Networks (CDNs): Akamai, Cloudflare</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Content%20Delivery%20Network.html","title":"Content Delivery Network (CDN)","text":"<p>A Content Delivery Network (CDN) is a globally distributed network of proxy servers deployed in multiple data centers across different geographic locations. Its primary purpose is to serve web content (static/dynamic) to end-users with high availability and high performance by serving content from the edge server closest to the user.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Content%20Delivery%20Network.html#motivations-for-using-a-cdn","title":"Motivations for using a CDN","text":"<ol> <li>Improved Performance: By caching content at edge servers closer to users, CDNs reduce latency, improve page load times, and enhance the overall user experience.</li> <li>Increased Scalability and Availability: CDNs can handle sudden traffic spikes and high loads by distributing requests across multiple edge servers, ensuring high availability.</li> <li>Reduced Bandwidth Costs: By caching content closer to users, CDNs reduce the amount of data that needs to be transferred from the origin server, resulting in lower bandwidth costs.</li> <li>Enhanced Security: CDNs can provide security features like DDoS protection, SSL offloading, and web application firewalls, improving the overall security posture.</li> <li>Content Optimization: CDNs can optimize content delivery by compressing, minifying, and transforming content for faster transmission and better performance.</li> </ol>"},{"location":"Technical/Distributed%20Systems%20Design/Content%20Delivery%20Network.html#how-is-a-cdn-different-from-general-caching","title":"How is a CDN different from general caching?","text":"<p>While general caching can be implemented at various levels (client-side, server-side, proxy servers, etc.), a CDN differentiates itself through its unique characteristics:</p> <ol> <li> <p>Geographical Distribution: CDNs have a globally distributed network of edge servers located in multiple data centers across different regions and countries. This allows content to be cached and served from the edge server closest to the user, minimizing latency.</p> </li> <li> <p>Intelligent Request Routing: CDNs employ sophisticated request routing algorithms to direct user requests to the optimal edge server based on factors like proximity, server load, network conditions, and more. This ensures efficient content delivery.</p> </li> <li> <p>Massive Scale: CDNs are designed to handle massive amounts of content and traffic, serving millions of users simultaneously with high availability and performance.</p> </li> <li> <p>Advanced Caching Strategies: CDNs implement advanced caching strategies, such as cache hierarchies, content pre-fetching, cache invalidation mechanisms, and more, to ensure efficient content delivery and cache management.</p> </li> <li> <p>Content Management and Replication: CDNs provide mechanisms to manage and replicate content from the origin server to edge servers, ensuring that the cached content is up-to-date and consistent across the network.</p> </li> </ol>"},{"location":"Technical/Distributed%20Systems%20Design/Content%20Delivery%20Network.html#key-components-of-a-cdn","title":"Key Components of a CDN","text":"<ol> <li>Origin Server: The main server that stores the original/master copy of the content.</li> <li>Edge Servers/PoPs (Points of Presence): Geographically distributed servers that cache/store copies of the content for faster delivery.</li> <li>Request Routing: A mechanism to redirect client requests to the nearest/optimal edge server (e.g., Anycast, DNS-based routing, Application-level routing).</li> <li>Content Caching: Caching policies and algorithms to cache popular/frequently accessed content on edge servers while invalidating/updating stale content.</li> <li>Content Management/Replication: Mechanisms to replicate/update content from the origin to edge servers as needed.</li> </ol>"},{"location":"Technical/Distributed%20Systems%20Design/Coordination%20Service.html","title":"Coordination Services","text":"<p>Coordinating operations and sharing data between the nodes of a distributed system can lead to complex behaviors like race conditions, deadlocks, and inconsistencies. Systems often use a Coordination Service (also called \"Metadata Service\" or \"Configuration Service\") to manage and synchronize a group of nodes (a \"cluster) in a distributed environment.</p> <p>This is unlike gossip protocols which pass information directly from node to node. Instead a centralized, replicated key value store built on top of consensus algorithms is used.</p> <p>Example: Apache Zookeeper</p> <p>The responsibilities of this service are:</p> <ol> <li>Data Sync - sync metadata, config, and other data between nodes with strong consistency. Nodes should know about other nodes in the system and can read and write to other nodes atomically without race conditions.</li> <li>Heartbeats - periodic messages sent by the nodes to the service to indicate that they are operating normally. The service monitors nodes that fail to heartbeat and send alerts.</li> <li>Adding and removing nodes - if a system has autoscaling, nodes are added and removed based on traffic. Information about the newly added or removed nodes needs to be propagated through the system.</li> <li>Failed nodes - if a node fails, the service should remove it from the metadata and notify services that could be impacted.</li> <li>Leader election - the service should have a mechanism for leader election. If a primary node fails, the remaining nodes should elect a new primary node using this service.</li> <li>Maintain Distributed Locks - if nodes need to access shared resources in a mutually exclusive way, distributed locks are used to synchronize their access.</li> <li>Persist Metadata - the cluster's metadata is a key-value mapping of node key to node data that can be persisted to a database.</li> </ol>"},{"location":"Technical/Distributed%20Systems%20Design/Coupling%20and%20Cohesion.html","title":"Coupling and Cohesion","text":""},{"location":"Technical/Distributed%20Systems%20Design/Coupling%20and%20Cohesion.html#loose-coupling","title":"Loose Coupling","text":"<p>Different modules, components, and services should have minimal dependency on each other.</p> <p>All interactions ebtween services is thorugh service interfaces, and implementation details are hidden from other services. Deprecating ro chaning logic can be achieved with only interface changes.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Coupling%20and%20Cohesion.html#high-cohesion","title":"High Cohesion","text":"<p>The logi, methods, and classes of a single service should functionally be related and support a single purpose. This helps define boundaries between services and makes services easier to maintain.</p> <p>This allows for:</p> <ol> <li>easier maintainence and deployment of services</li> <li>esier testing - a functional change is limited to only relevant services</li> <li>easier to understand code</li> </ol> <p>Low cohesion means that </p> <ol> <li>functionality that logically belongs to a single service is spread out</li> <li>unrelated methods and logic are grouped together</li> </ol> <p>Rule of Thumb: CRUD operations for a single data model should belong to a single service</p>"},{"location":"Technical/Distributed%20Systems%20Design/Data%20Models.html","title":"Data Models","text":""},{"location":"Technical/Distributed%20Systems%20Design/Data%20Models.html#overview","title":"Overview","text":"<p>Data Modeling (how data is stored) and Data Querying (how data is retrieved) choices go hand in hand.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Data%20Models.html#data-abstractions","title":"Data Abstractions","text":""},{"location":"Technical/Distributed%20Systems%20Design/Data%20Models.html#main-categories-of-databases","title":"Main Categories of Databases","text":""},{"location":"Technical/Distributed%20Systems%20Design/Data%20Models.html#1-relational-database","title":"1. Relational Database","text":"<p>When you have a relatively fixed structure and you know this structure is not going to change too rapidly.</p> <ul> <li>Optimized for Transaction and Batch Processing (Read Throughput), Joins etc.</li> <li>Data Organized as tables/relations</li> <li>Object Relational Mapping Needed</li> <li>Oracle, MySQL, PostgreSQL</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Data%20Models.html#2-document-database","title":"2. Document Database","text":"<p>Target use cases are where data comes in self-constrained documents and relationships between one document and another are rare. Also the schema is easily evolved.</p> <ul> <li>NoSQL - or not only SQL</li> <li>Flexible schemas, better performance due to locality / high write throughput</li> <li>Mainly free and open source</li> <li>MongoDB, CouchDB, Espresso</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Data%20Models.html#3-graph-database","title":"3. Graph Database","text":"<p>Target use cases are where anything is potentially related to everything.</p> <ul> <li>Best suited for highly interconnected data - many to many relationships</li> <li>Social graphs, web graphs etc.</li> <li>Neo4j, SPARQL, Cypher</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Databases.html","title":"Databases","text":"<p>A database is a structured collection of data stored through a computer system.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Databases.html#transactions-and-operations","title":"Transactions and Operations","text":"<p>A database transaction is a logical unit of work performed on a database and may consist of multiple operations. An operation is a smaller unit of work used to complete a transaction.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Databases.html#key-metrics","title":"Key Metrics","text":"<ol> <li>Availability - percentage of time the suers of the database can access it</li> <li>Reliability - rate of data corruption, unsecure authorization, and recoverability</li> <li>Persistence - the data can be either written stably to non-volatile memory such as a hard drive or solid state drive (which will retain data even if it is not powered), or to volatile memory (e.g RAM) on in-memory databases that lack persistence.</li> </ol>"},{"location":"Technical/Distributed%20Systems%20Design/Databases.html#database-model","title":"Database Model","text":"<p>Defining the data types used and stored within a system, the relationship between these types, and the way the data is organized through its attributes.</p> <p>Entity is an object represented in a database, and a property of this entity is called an attribute.</p> <p>Database schema refers to the physical implementation of the database model to a specific database platform. The design of the data models is the same regardless of the database platform or type.</p> <p>ERDs visualizes the relationships between entities and attributes.There are three main components: Entities, Attributes, and Relationships.</p> <pre><code>// Courses\n{\n    \"course_id\": long,\n    \"created_at\": timestamp,\n    \"title\": string,\n    \"creator_id\": long\n}\n// Creator\n{\n    \"creator_id\": long,\n    \"name\": string\n}\n</code></pre>"},{"location":"Technical/Distributed%20Systems%20Design/Databases.html#attributes-and-relationships","title":"Attributes and Relationships","text":"<p>Attributes have types and expected sizes specified.</p> <p>Also they may be:</p> <ul> <li>Primary Key (PK) - used to identify an entity</li> <li>Foreign Key (PK) - used to uniquely identify another entity and link two entities together</li> <li>Composite Primary Key (CPK) - a key that uses two or more attributes to uniquely identify that entity (e.g. child entities that can be uniquely identified using other entities)</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Databases.html#cardinality-and-modality","title":"Cardinality and Modality","text":"<p>Cardinality refers to the maximum number of elements of an entity that are associated with the elements in another entity. There are the following cardinalities:</p> <ul> <li>One-to-One</li> <li>One-to-Many</li> <li>Many-to-Many</li> </ul> <p>Modality refers tot he minimum number of elements of an entity that are associated with elements in another entity.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Databases.html#relational-vs-nosqlnon-relational-databases","title":"Relational vs. NoSQL/Non-Relational Databases","text":"Feature SQL Databases NoSQL Databases Data Model Organized into structured tables, relational and normalized data Flexible schema, supports various data models Query Language SQL (Structured Query Language) Custom query languages or APIs (e.g., MongoDB Query Language) Consistency Model ACID-compliant transactions Basically Available, Soft state, Eventually consistent (BASE) Scalability Generally vertical scaling, limited horizontal scaling Horizontal scaling, distributed architectures Use Cases Enterprise applications, OLTP, data warehousing, OLAP Real-time analytics, web applications, IoT, content management Typical Index Datastructure B-Trees, better for reads than writes LSM Tree and SSTables, for fast writes Examples MySQL, PostgreSQL, SQL Server MongoDB, Cassandra, Redis, Elasticsearch, Neo4j Challenges Schema evolution, performance tuning, may require two phase commit Data consistency, schema design, tooling ecosystem"},{"location":"Technical/Distributed%20Systems%20Design/Databases.html#storage-engines-data-structures","title":"Storage Engines / Data Structures","text":"<p>Two families of storage engines used by databases:</p> <ul> <li>Log structured - LSM-Trees e.g. SSTables -&gt; HBASE, Cassandra -&gt; write optimized</li> <li>Page-Oriented - B-trees -&gt; Traditional RDBMS -&gt; read optimized</li> </ul> <p>These are answers to limitations of disk access.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Databases.html#relational-database-details","title":"Relational Database Details","text":""},{"location":"Technical/Distributed%20Systems%20Design/Databases.html#types","title":"Types","text":"<p>There are two main categories of databases - OLTP (Online Transaction Processing Database) and OLAP (Online Analytical Processing Database) each with a different read pattern, write patterns, user using it, data size etc.</p> <ol> <li>OLTP - Online Transaction Processing Database optimized for latency.</li> <li>eg. MySQL</li> <li>Usually row-order store<ul> <li>easy to modify/add a record</li> <li>might read in unnecessary data</li> </ul> </li> <li>OLAP - Online Analytical Processing Databases optimized for data crunching.</li> <li>Data Warehousing (Star/Snowflake schema), column oriented</li> <li>Column compression, data cubes, optimized for reads/queries</li> <li>Materialized views, lack of flexibility</li> <li>HBase, Hive, Spark</li> <li>Usually column-order store<ul> <li>Only need to read in relevant data</li> <li>Tuple writes require multiple accesses</li> <li>Suitable for read-mostly, read-intensive, large data repositories</li> </ul> </li> </ol>"},{"location":"Technical/Distributed%20Systems%20Design/Databases.html#nosql-database-details","title":"NoSQL Database Details","text":""},{"location":"Technical/Distributed%20Systems%20Design/Databases.html#key-characteristics","title":"Key Characteristics","text":"<ul> <li>Flexible Schema: NoSQL databases typically support schema-less or flexible schema designs, allowing for dynamic and varied data structures within the same database.</li> <li>Scalability: NoSQL databases are designed to scale horizontally, allowing for distributed architectures and seamless expansion across multiple nodes or clusters.</li> <li>High Performance: NoSQL databases are optimized for specific use cases, such as high-volume read/write operations, real-time analytics, and large-scale data processing.</li> <li>Diverse Data Models: NoSQL databases support various data models, including document-oriented, key-value, wide-column, and graph databases, catering to different data storage and retrieval requirements.</li> <li>CAP Theorem: NoSQL databases often prioritize availability and partition tolerance over strict consistency, adhering to the principles outlined in the CAP theorem.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Databases.html#types-of-nosql-databases","title":"Types of NoSQL Databases","text":""},{"location":"Technical/Distributed%20Systems%20Design/Databases.html#document-oriented-databases","title":"Document-oriented Databases","text":"<p>Store data in flexible, JSON-like documents, allowing for nested structures and dynamic schemas. Nested documents allow for embedded schemas for data locality, and is denormalized.</p> <p>Examples include MongoDB, Couchbase, and Google Firestore.</p> <p>They are suitable for unstructured catalog data, and complex structured hierarchical data. For example in e-commerce applications a product can have thousands of attributes, and content management applications like blogs and video platforms.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Databases.html#key-value-stores","title":"Key-Value Stores","text":"<p>Store data as key-value pairs using methods like hash tables, providing fast access to individual records. </p> <p>Examples include Redis, Amazon DynamoDB.</p> <p>They are efficient for session-oriented applications such as user session data/profile information, recommendations, discounts, promotions etc. </p>"},{"location":"Technical/Distributed%20Systems%20Design/Databases.html#wide-column-stores-column-family-databases","title":"Wide-column Stores (Column-family Databases)","text":"<p>Store data in columns rather than rows, allowing for efficient storage and retrieval of large datasets. The store has shard key and sort key, allows for flexible schemas and easy partitioning.</p> <p>Examples include Apache Cassandra, HBase.</p> <p>Efficient for large number of aggregation and data anlytics queries. They reduce the disk I/O requirements drastically. Examples may be applications related to financial institutions that need to sum financial transaction over a period of time. </p>"},{"location":"Technical/Distributed%20Systems%20Design/Databases.html#graph-databases","title":"Graph Databases","text":"<p>Store data in graph structures, representing relationships between entities as nodes and edges. Hence we are able to store the data once and interpret it differently based on the relationships. </p> <p>Examples include Neo4j, Amazon Neptune.</p> <p>Can be used in social applications, provide interesting facts and figures among different kinds of users and activities. </p>"},{"location":"Technical/Distributed%20Systems%20Design/Databases.html#indexing","title":"Indexing","text":"<ul> <li>Purpose: Indexing is a technique used in databases to optimize query performance by creating data structures that allow for fast data retrieval.</li> <li>Why it's Used: Indexing improves query performance by providing efficient access paths to data, reducing the time required to search and retrieve data from tables.</li> </ul> <p>Types of Indices:</p> <ul> <li>B-Tree Index: Widely used for range queries, ordered traversal, and equality searches in both SQL and NoSQL databases.</li> <li>Hash-based Index: Provides fast lookups for equality searches, commonly used in SQL databases.</li> <li>Bitmap Index: Efficient for low-cardinality columns, suitable for boolean and categorical data. Not commonly used in SQL databases, but may be utilized in some NoSQL databases.</li> <li>Full-Text Index: Optimized for searching textual data, supports complex text searches and relevance ranking. Used in some SQL databases (e.g., MySQL, PostgreSQL) and NoSQL databases (e.g., Elasticsearch, MongoDB with text search).</li> <li>Spatial Index: Designed for indexing geometric data, enables efficient spatial queries and joins. Used in SQL databases supporting geospatial data types (e.g., PostGIS in PostgreSQL) and some NoSQL databases (e.g., MongoDB with geospatial queries).</li> </ul> <p>Additionally you would also sometimes define a secondary index for more read optimization. You can have local secondary indices (usual default) or global secondary indices (for cases where the service is read heavy for a range of queries).</p>"},{"location":"Technical/Distributed%20Systems%20Design/Domain%20Name%20System.html","title":"Domain Name System","text":""},{"location":"Technical/Distributed%20Systems%20Design/Domain%20Name%20System.html#dns","title":"DNS","text":"<p>Domain Name System (DNS) is used to resolve domain names (such as www.google.com) to IP addresses. </p> <p>The translation from a domain name to IP address occurs through a process called DNS resolution or DNS lookup. There are four types of DNS Name Servers used for DNS resolution, and they process a lookup request in the following order:</p> <ol> <li>Resolver Name Server (DNS Resolver) - interacts with clients such as web browsers and is the initial server in a DNS lookup. The resolver either directs the request to the root name server or returns cahced data. Since there might be clients that request the same domain name, the resolver caches the IP addres of the domain and returns it directly.</li> <li>Root Name Server - responds to the resolver's request by directing the request to a TLD server based on the extension of the domain name (.com, .org, .net)</li> <li>TLD Name Server - a TLD (Top Level Domain) server holds information for all the domain names that have the same extension. For example the <code>.com</code> TLD server holds the information about which authoritative name server holds the DNS records for the domain name.</li> <li>Authoritative Name Server - holds the actual DNS record which contains the IP address of the server that corresponds to the domain name.</li> </ol> <p></p>"},{"location":"Technical/Distributed%20Systems%20Design/Embedded%20Schemas%20and%20Denormalization.html","title":"Embedded Schemas and Denormalization","text":""},{"location":"Technical/Distributed%20Systems%20Design/Embedded%20Schemas%20and%20Denormalization.html#embedded-schemas-in-nosql-databases","title":"Embedded Schemas in NoSQL Databases","text":"<p>In NoSQL databases, embedded schemas allow nesting one document within another, enabling the representation of complex data structures without the need for joins or references to other collections. This approach can improve query performance and simplify data access in certain scenarios.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Embedded%20Schemas%20and%20Denormalization.html#rules-of-thumb-for-using-embedded-schemas-in-nosql-databases","title":"Rules of Thumb for Using Embedded Schemas in NoSQL Databases","text":"<ol> <li>One-to-One or One-to-Few Relationships:</li> <li> <p>Embedding Works Best: When representing relationships where one document is closely associated with another and the cardinality is one-to-one or one-to-few.</p> </li> <li> <p>Data Aggregation:</p> </li> <li> <p>Embedding Works Best: When aggregating related data into a single document to improve query efficiency and reduce the need for multiple round-trip database requests.</p> </li> <li> <p>Schema Design Simplicity:</p> </li> <li>Embedding Works Best: When designing schemas that require straightforward data structures without complex relationships or frequent updates.</li> </ol>"},{"location":"Technical/Distributed%20Systems%20Design/Embedded%20Schemas%20and%20Denormalization.html#when-non-embedded-or-using-references-works-best","title":"When Non-Embedded or Using References Works Best","text":"<ol> <li>One-to-Many or Many-to-Many Relationships:</li> <li> <p>Non-Embedded or References Work Best: When representing relationships with high cardinality or when the associated data is frequently updated or accessed independently.</p> </li> <li> <p>Scalability and Performance Considerations:</p> </li> <li> <p>Non-Embedded or References Work Best: When considering scalability and performance trade-offs, such as avoiding document size limitations or minimizing data duplication.</p> </li> <li> <p>Schema Flexibility:</p> </li> <li>Non-Embedded or References Work Best: When designing schemas that require flexibility to support evolving data models or schema changes over time.</li> </ol>"},{"location":"Technical/Distributed%20Systems%20Design/Embedded%20Schemas%20and%20Denormalization.html#normalization-vs-denormalization-in-databases","title":"Normalization vs. Denormalization in Databases","text":"<p>Normalization is the process of organizing data in a database to reduce redundancy and dependency by splitting tables into smaller, related tables. It aims to minimize data duplication and maintain data integrity by enforcing constraints.</p> <p>Denormalization, on the other hand, is the process of adding redundant data or repeating information across tables to improve query performance by reducing the need for joins and aggregations. It can lead to faster read operations but may increase data redundancy and complexity.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Embedded%20Schemas%20and%20Denormalization.html#advantages-of-denormalization","title":"Advantages of Denormalization","text":"<ol> <li> <p>Improved Query Performance: Denormalized schemas can result in faster query execution by reducing the need for complex joins and aggregations, especially in read-heavy workloads.</p> </li> <li> <p>Reduced Latency: With denormalization, data retrieval can be optimized for low-latency access, particularly in distributed systems or real-time applications where minimizing query response time is critical.</p> </li> <li> <p>Simplified Data Access: Denormalization can simplify data access and application logic by pre-computing and storing frequently accessed or aggregated data, eliminating the need for complex data transformations or calculations at runtime.</p> </li> </ol>"},{"location":"Technical/Distributed%20Systems%20Design/Embedded%20Schemas%20and%20Denormalization.html#disadvantages-of-denormalization","title":"Disadvantages of Denormalization","text":"<ol> <li> <p>Data Redundancy: Denormalization increases data redundancy and storage requirements, as redundant data is replicated across multiple tables or documents, leading to larger database sizes and increased storage costs.</p> </li> <li> <p>Data Consistency: Maintaining data consistency becomes more challenging with denormalized schemas, as redundant data must be updated consistently across all copies to prevent inconsistencies and integrity issues.</p> </li> <li> <p>Complexity and Maintenance: Denormalized schemas can introduce complexity and maintenance overhead, as they require careful design and management to ensure data integrity, handle updates, and manage schema changes effectively.</p> </li> </ol>"},{"location":"Technical/Distributed%20Systems%20Design/Fan%20Out.html","title":"Fan Out Component","text":"<p>Fan-out component are a type of distributed system component responsible for propagating data or events to multiple destinations or consumers in a scalable and efficient manner. They act as an intermediary between a source (e.g., a database, message queue, or application) and multiple targets or services that need to receive or process the same data or events.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Fan%20Out.html#examples-of-fan-out-services","title":"Examples of Fan-out Services","text":"<p>Some common examples of fan-out services or components include:</p> <ol> <li>News Feed Service: Responsible for updating news feeds or activity streams for users based on their connections or subscriptions.</li> <li>Timeline Update Service: Distributes user activity updates to various timeline services or storage systems.</li> <li>News Feed Blender: Combines and ranks updates from various sources (e.g., friends, groups, pages) to create a personalized news feed for each user.</li> <li>Search Indexing Service: Distributes data updates to multiple search index shards or partitions for indexing.</li> <li>Notification Service: Broadcasts notifications or messages to multiple recipients or devices.</li> <li>Copy Service: Distributes data or content updates to multiple replicas or caching layers for redundancy and faster access.</li> </ol>"},{"location":"Technical/Distributed%20Systems%20Design/Fan%20Out.html#push-vs-pull-models","title":"Push vs. Pull Models","text":"<p>Fan-out services can operate using either a push or a pull model, or a combination of both:</p> <ol> <li> <p>Push Model: In the push model, the fan-out service actively pushes data or events to the consumers or destinations as soon as they become available. This model is suitable for scenarios that require real-time or near real-time delivery of updates.</p> </li> <li> <p>Pull Model: In the pull model, the consumers or destinations periodically poll or request data from the fan-out service. This model is suitable for scenarios where consumers can tolerate some delay in receiving updates.</p> </li> <li> <p>Hybrid Model: In some cases, a hybrid approach combining push and pull models can be used. For example, the fan-out service could push updates to a message queue, and consumers could pull from the queue at their own pace.</p> </li> </ol> <p>The choice between push, pull, or a hybrid model depends on the specific requirements of the system, such as latency, consistency, scalability, and resource constraints.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Interview%20Notes.html","title":"Interview Notes","text":""},{"location":"Technical/Distributed%20Systems%20Design/Interview%20Notes.html#components-to-know-about","title":"Components to know about","text":"<ul> <li>Domain Name System (DNS)</li> <li>Proxies and Reverse Proxies</li> <li>Load Balancers</li> <li>Relational and NoSQL Databases</li> <li>Object Storage</li> <li>Content Delivery Network (CDN)</li> <li>Servers</li> <li>Cache</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Interview%20Notes.html#concepts-to-know","title":"Concepts to know","text":"<ul> <li>CAP</li> <li>PACELC</li> <li>Distributed Systems Requirements</li> <li>Testing</li> <li>Consistent Hashing</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Interview%20Notes.html#data-structures-to-know","title":"Data Structures to Know","text":"<ul> <li>Trie - Search Autocomplete</li> <li>Quadtree - Location Based Indexing</li> <li>R-Tree - Location Based Indexing, sercing multi-dimension shapes, asuch as nearest neighbor lookups</li> <li>Geohash - Location Based Indexing</li> <li>Skiplist - in memory index type</li> <li>Hash Index - in-memory index type - common implementation of the \"Map\" datastructure</li> <li>SSTable - disk based \"Map\" datastructure</li> <li>LSM Tree - skiplist (memory) + sstable (disk) combination to provide high write throughput</li> <li>B-Tree - disk based index with consistent read/write performance - most popular index used in databases</li> <li>Inverted Index - used for document indexing in search</li> <li>Suffix Tree - used for string pattern search, such as string suffix match</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Interview%20Notes.html#algorithms-to-know","title":"Algorithms to Know","text":"<ul> <li>Consistent Hashing - Balancing load within a cluster of services</li> <li>Bloomfilter - Eliminate costly lookups</li> <li>Leaky Bucket - Rate limiter</li> <li>Token Bucket - Rate limited</li> <li>RSync - File transfers</li> <li>Raft/Paxos - Consensus algorithm</li> <li>Merkle Tree - Identify inconsistencies between nodes</li> <li>HyperLogLog - Count unique values fast</li> <li>Count-min Sketch - Estimate frequencies of items</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Interview%20Notes.html#key-topics","title":"Key Topics","text":""},{"location":"Technical/Distributed%20Systems%20Design/Interview%20Notes.html#system-requirements-analysis","title":"System Requirements Analysis","text":"<ul> <li>Understand the functional and non-functional requirements of the system.</li> <li>Discuss use cases, user expectations, and system constraints.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Interview%20Notes.html#system-architecture","title":"System Architecture","text":"<ul> <li>Design a high-level architecture that addresses the system's requirements.</li> <li>Discuss components, their responsibilities, and interactions.</li> <li>Consider microservices, service-oriented architecture (SOA), or serverless architecture based on the requirements.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Interview%20Notes.html#scalability","title":"Scalability","text":"<ul> <li>Discuss horizontal and vertical scaling strategies.</li> <li>Consider sharding, partitioning, and replication techniques.</li> <li>Discuss load balancing and auto-scaling mechanisms.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Interview%20Notes.html#availability-and-fault-tolerance","title":"Availability and Fault Tolerance","text":"<ul> <li>Discuss redundancy, failover, and disaster recovery strategies.</li> <li>Consider techniques such as replication, data mirroring, and distributed consensus algorithms.</li> <li>Discuss how to handle network partitions and node failures.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Interview%20Notes.html#consistency-and-concurrency","title":"Consistency and Concurrency","text":"<ul> <li>Discuss consistency models (e.g., eventual consistency, strong consistency) based on application requirements.</li> <li>Consider distributed locking, consensus algorithms (e.g., Paxos, Raft), and coordination services (e.g., ZooKeeper, etcd).</li> <li>Discuss isolation levels and transaction management.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Interview%20Notes.html#data-storage-and-retrieval","title":"Data Storage and Retrieval","text":"<ul> <li>Discuss database selection based on requirements (e.g., relational databases, NoSQL databases, distributed key-value stores).</li> <li>Consider data partitioning, indexing, and caching strategies.</li> <li>Discuss data replication, consistency, and durability guarantees.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Interview%20Notes.html#messaging-and-communication","title":"Messaging and Communication","text":"<ul> <li>Discuss message queuing, publish-subscribe patterns, and event-driven architectures.</li> <li>Consider message brokers (e.g., Kafka, RabbitMQ) and communication protocols (e.g., HTTP, gRPC).</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Interview%20Notes.html#security","title":"Security","text":"<ul> <li>Discuss authentication, authorization, encryption, and data privacy requirements.</li> <li>Consider network security, access control mechanisms, and secure communication protocols.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Interview%20Notes.html#monitoring-and-management","title":"Monitoring and Management","text":"<ul> <li>Discuss logging, monitoring, and alerting requirements.</li> <li>Consider metrics collection, distributed tracing, and centralized logging solutions.</li> <li>Discuss deployment strategies and continuous integration/continuous deployment (CI/CD) pipelines.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Interview%20Notes.html#cost-optimization","title":"Cost Optimization","text":"<ul> <li>Discuss resource utilization, cost-effective scaling strategies, and resource provisioning.</li> <li>Consider serverless computing, containerization, and cloud services pricing models.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Interview%20Notes.html#writing-service-interfaces","title":"Writing Service Interfaces","text":"<p>Write the outline of the service interface to easily describe the service behavior to the interviewer, by including:</p> <ol> <li>message data structures - describes the request and response messages that a service uses to communicate over a netwrk</li> <li>method signature - contains a methods name, return type, and parameter list</li> </ol> <pre><code>GetCourseRequest\n    int64 user_id\n    int64 timestamp\n    string course_name\n\nGetCourseResponse\n    int64 userid\n    repeated string lessons\n    int64 timestamp\n\n# CourseService has a method getCourse which accepts a request message GetCourseRequest and returns the response message GetCourseResponse\n\n# It accepts a single argument, the request, and returns a single object, the response\n\nCourseService\n    GetCourseResponse getCourse (GetCourseRequest request)\n</code></pre> <p>Reminder: the message data structure is different from the data model - e.g. song_names could be stored as rows that are aggregated into a sequential datastructure like an array in the course response.</p> <p>A system would have multiple services, and each service would have multiple methods. Making them descriptive makes the interview self-explanatory and self-documenting.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Load%20Balancing.html","title":"Load Balancing","text":"<p>A load balancer is a device or software component that distributes incoming network traffic across multiple servers or resources. It acts as a reverse proxy, intercepting client requests and forwarding them to the most suitable server based on predefined algorithms or rules. Load balancers are commonly used to improve the performance, availability, and reliability of applications and services.</p> <p>Impact on Availability and Reliability:</p> <ul> <li>Availability: Load balancers enhance availability by distributing traffic across multiple servers, preventing any single server from becoming overwhelmed. If one server fails or becomes unreachable, the load balancer redirects traffic to healthy servers, ensuring continuous service availability.</li> <li>Reliability: Load balancers contribute to reliability by minimizing the risk of service downtime due to server failures or maintenance. By distributing traffic evenly and efficiently, load balancers help maintain consistent performance and responsiveness, even under high loads or adverse conditions.</li> </ul> <p>In distributed systems, load balancers play a crucial role in achieving scalability, fault tolerance, and performance optimization. They are often deployed at the entry point of a distributed application or service, acting as the reverse proxy and to evenly distribute incoming requests among multiple instances or nodes. Load balancers can route traffic based on various factors such as server health, geographic location, or request type, ensuring efficient resource utilization and effective load distribution across the system.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Load%20Balancing.html#placement-of-load-balancers","title":"Placement of Load Balancers","text":"<p>In distributed systems, load balancers are typically placed at strategic points within the network to optimize traffic distribution and improve system performance. Some common placements for load balancers include:</p> <p>Frontend of the Application:</p> <ul> <li>Load balancers positioned at the entry point of the distributed application.</li> <li>Serve as the first point of contact for incoming client requests.</li> <li>Distribute traffic among multiple backend servers or instances before reaching application logic.</li> </ul> <p>Between Clients and Web Servers:</p> <ul> <li>Load balancers act as reverse proxies intercepting client requests.</li> <li>Positioned between clients (e.g., web browsers) and web servers hosting the application.</li> <li>Provide additional features such as SSL termination, caching, and request routing.</li> </ul> <p>Between Web Servers and Application Servers:</p> <ul> <li>Load balancers distribute traffic from the web layer to the application layer.</li> <li>Common in traditional multi-tier architectures where web servers handle incoming requests and application servers process application logic.</li> <li>Ensure efficient utilization of resources and optimal performance.</li> </ul> <p>Within Microservices Architecture:</p> <ul> <li>Load balancers used to distribute traffic between different microservices or service instances.</li> <li>Enable dynamic routing of requests based on load, health, or specific routing rules.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Load%20Balancing.html#load-balancer-types","title":"Load Balancer Types","text":"<p>Layer 4 Load Balancer / Session Level Load Balancing:</p> <ul> <li>Operates at the transport layer (Layer 4) of the OSI model.</li> <li>Directs traffic based on information available in network and transport layer protocols (e.g., IP addresses, TCP/UDP ports).</li> <li>Typically faster and more efficient than Layer 7 load balancers because they don't inspect application layer data.</li> <li>Suitable for scenarios where routing decisions can be made solely based on network and transport layer information.</li> <li>Examples include HAProxy and NGINX when configured as a Layer 4 load balancer.</li> </ul> <p>Layer 7 Load Balancer:</p> <ul> <li>Operates at the application layer (Layer 7) of the OSI model.</li> <li>Makes routing decisions based on application layer data, such as HTTP headers, URLs, cookies, or content.</li> <li>Offers more advanced routing and traffic management capabilities, including content-based routing, session persistence, and SSL termination.</li> <li>Provides greater flexibility and control over traffic routing but may introduce higher latency and overhead due to application layer inspection.</li> <li>Ideal for scenarios that require sophisticated routing logic or application-specific traffic management.</li> <li>Examples include F5 BIG-IP, AWS Application Load Balancer (ALB), and NGINX when configured as a Layer 7 load balancer.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Load%20Balancing.html#load-balancing-algorithms","title":"Load Balancing Algorithms","text":"<ul> <li> <p>Round Robin: Distributes incoming requests evenly across a pool of backend servers in a circular manner.</p> </li> <li> <p>Least Connections: Routes each new request to the backend server with the fewest active connections at the time.</p> </li> <li> <p>IP Hash: Uses a hash function to map the client's IP address to a specific backend server, ensuring session persistence.</p> </li> <li> <p>Weighted Round Robin: Assigns a weight to each backend server, determining the proportion of requests it receives relative to other servers.</p> </li> <li> <p>Least Response Time: Routes requests to the backend server with the fastest response time, based on historical performance metrics.</p> </li> <li> <p>Least Bandwidth: Directs requests to the backend server with the lowest current bandwidth usage, aiming to optimize resource utilization.</p> </li> <li> <p>Random Selection: Selects a backend server randomly from the pool without considering server load or capacity.</p> </li> <li> <p>Geographical Load Balancing: Directs clients to the nearest or most optimal server based on their geographic location, aiming to minimize latency and improve performance.</p> </li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Monitoring.html","title":"Monitoring","text":"<p>TODO: Add notes here - leaving blank for now.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Monolith%20vs.%20Microservices.html","title":"Monolith vs. Microservices","text":""},{"location":"Technical/Distributed%20Systems%20Design/Monolith%20vs.%20Microservices.html#monolith","title":"Monolith","text":"<p>Software that is built and deployed as a single unit - usually consisting of a user interface, a service-side application, and a database.</p> <p>Here different parts of the system are combined and launched as a single component.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Monolith%20vs.%20Microservices.html#advantages","title":"Advantages","text":"<ol> <li>Easy development and fast deployment</li> <li>Limited number of components need integration or communication, simplifying testing and monitoring</li> <li>Performant and low latency - requests are served from a singe unit, which means the request path contains few network hops and components</li> </ol>"},{"location":"Technical/Distributed%20Systems%20Design/Monolith%20vs.%20Microservices.html#disadvantages","title":"Disadvantages","text":"<ol> <li>All modules are scaled together, and resource allocation might be inefficient</li> <li>If one part of the monolith fails, the entire monolith can fail</li> <li>Often have tightly coupled components that make it difficult to incorporate new technology and other components</li> <li>Deployment becomes difficult at large scale, with multiple teams</li> </ol>"},{"location":"Technical/Distributed%20Systems%20Design/Monolith%20vs.%20Microservices.html#microservice","title":"Microservice","text":"<p>Software that is built and deployed as a collection of independent services.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Monolith%20vs.%20Microservices.html#advantages_1","title":"Advantages","text":"<ol> <li>Granular Scalability - independent services mean each service can be scaled independently instead of being scaled in conjunction with other services - reduces bottlenecks and allows for more efficient resource usage and cost savings</li> <li>Reusability - microservices can be used across different applications and systems in the same business unit or enterprise</li> <li>Reliability - microservices are more robust due to failure isolation, where the failure is isolated to a single service, node, or region. This failure does not result in system-wide outages. Traffic redirection can also happen.</li> <li>Technology agnostic - allows for use of different tech stacks and frameworks. The communication protocols are decoupled from the language and implementation of the service.</li> </ol>"},{"location":"Technical/Distributed%20Systems%20Design/Monolith%20vs.%20Microservices.html#disadvantages_1","title":"Disadvantages","text":"<ol> <li>Increased Complexity -- independent services need to communicate and coordinate over a network - ppotentially greater latency, and code/comm protocol overhead</li> <li>Difficult testing and debugging - end to end testing requires launching all services an testing interactions between them. Logging and debugging would also be spread around multiple files and executibles, making it difficult to understand interaction and integration behaviour.</li> <li>Increased design and deployment overhead - more services mean more design and deployment work, which is challenging for smaller teams who need to iterate quickly with low overhead.</li> </ol>"},{"location":"Technical/Distributed%20Systems%20Design/Networking.html","title":"Networking","text":"<p>Communication Protocols that standardize message formats and rules about how these messages should be sent and received are composed of layers. Each layer provides a specific function to the layer above it, but each layer is encapsulated; in other words, it does not need to know how the other layers work.</p> <p>Two major models describe these layers: the OSI (Open System Interconnect) model and the TCP/IP (Transmission Control Protocol/Internet Protocol) model.</p> <p>The TCP/IP model is a simplified implementation of the OSI model - and it has four layers (compared to seven in the OSI model). It defines how data is transmitted from one computer to another, and how a computer should be connected to the internet. Its design allows for high communication reliability through an unreliable network.</p> <p>Each layer in the TCP/IP model has protocols, which each describe the message formats and rules that pertain to the later.</p> <p></p> <p>Some definitions of the common protocols:</p> <ul> <li>IP - Internet Protocol - set of rules for routing packets of data across networks. Each device on the network is assigned an IP address which uniquely identifies the device so that other devices on the internet can find it. IPv4 is the fourth version of the protocol, and it defines an address sa a set of four numbers that range from <code>0.0.0.0</code> to <code>255.255.255.255</code>. Data is divided into smaller pieces, called packets, and sent through the internet. Each packet contains the IP information which directs routers and devices to send the packets to the correct destination. After the packets reach the correct IP destination, a transport protocol is then responsible for handling that data (e.g. TCP or UPD below).</li> <li>TCP - Transmission Control Protocol - implemented on top of IP, it is a connection oriented protocol that allows for reliable transmission of data on an unreliable network. A network can become unreliable because packets can be dropped, corrupted, or unordered. TCP solves these problems through retransmission, check-sum, and order checking.</li> <li>UDP - User Datagram Protocol - implemented on top of IP but does not have the reliability and guarantees of TCP. However, UDP has lower overhead and latency compared to TCP, and is useful for applications where lost packets are not essential e.g. video playback or streaming content.</li> <li>HTTP - Hyper Text Transfer Protocol - implemented on top of TCP with the purpose of communicating with a web server that hosts websites and establish a connection with the server. In HTTP, a client sends requests to a web server and receives HTML back as responses.</li> <li>TLS - Secure Sockets Layer - used to authenticate and encrypt connections in networks. Used for secure connections for messaging and email applications for example. When TLS is used on HTTP it is called HTTPS.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Object%20Storage.html","title":"Object Storage","text":"<p>Object storage, also known as blob storage, is a type of storage architecture that manages data as objects rather than as files or blocks. Each object typically includes the data itself, along with metadata and a unique identifier.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Object%20Storage.html#key-features","title":"Key Features","text":"<ul> <li>Scalability: Object storage systems are highly scalable and can store petabytes of data or more by distributing data across multiple nodes or storage devices.</li> <li>Durability: Object storage systems provide high durability by replicating data across multiple storage nodes or data centers, ensuring data availability even in the event of hardware failures.</li> <li>Accessibility: Objects in storage are typically accessed over the network using APIs (such as RESTful APIs) or protocols (such as HTTP or HTTPS), making them accessible from anywhere with an internet connection.</li> <li>Cost-Effectiveness: Object storage is often more cost-effective than traditional storage solutions, as it eliminates the need for expensive hardware-based storage arrays and can leverage commodity hardware.</li> <li>Metadata: Each object in storage includes metadata that provides additional information about the object, such as its content type, creation date, or custom attributes. This metadata enables efficient organization and retrieval of data.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Object%20Storage.html#use-cases","title":"Use Cases","text":"<ul> <li>Backup and Archiving: Object storage is well-suited for long-term backup and archival of data, as it provides high durability and scalability.</li> <li>Content Distribution: Object storage is commonly used for storing and delivering multimedia content (such as images, videos, and audio files) in content delivery networks (CDNs) due to its scalability and accessibility.</li> <li>Big Data Analytics: Object storage is used as a data lake for storing large volumes of unstructured data used in big data analytics and machine learning applications.</li> <li>Cloud-native Applications: Object storage is integral to cloud-native application development, providing scalable and durable storage for cloud-based applications and microservices.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Object%20Storage.html#examples","title":"Examples","text":"<ul> <li>Amazon S3 (Simple Storage Service): A popular cloud-based object storage service provided by Amazon Web Services (AWS).</li> <li>Microsoft Azure Blob Storage: Microsoft's object storage solution within the Azure cloud platform.</li> <li>Google Cloud Storage: Google's scalable object storage service for storing and accessing data in the cloud.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Object%20Storage.html#considerations","title":"Considerations","text":"<ul> <li>Data Security: Ensure appropriate access controls and encryption mechanisms are in place to protect sensitive data stored in object storage.</li> <li>Data Lifecycle Management: Implement policies for managing the lifecycle of objects, including data retention, deletion, and versioning.</li> <li>Performance: Consider performance requirements and latency characteristics when selecting an object storage solution, especially for latency-sensitive applications.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Orchestration%20vs.%20Choreography.html","title":"Orchestration vs. Choreography","text":"<p>There are two styles of communication between services to run workflows (e.g. sending an email confirmation of purchase to a user, notifying the seller, and assign for delivery):</p> <ol> <li>Synchronous - request/response pattern where the client initiates a request and waits for a response</li> <li>Asynchronous - event based pattern where a service emits an event that other services react to</li> </ol> <p>Synchronous request/reponse is associated with orchstration, where one service acts as the orchestator and handles communication between services. For example when services are invoked in a serial order with blocking calls. Such systems can become a distributed monolith with a single point of failure (the service orchestrator/controller). e.g. order service sends requests to the email service, notification service, and delivery service.</p> <p>Asynchronous event based patter is associated with choreography, where an event stream or message queue is used to hold events, and each service is a consumer and/or producer of events. Multiple services can process the same events simultaneously. This enables greater system throughput because services can execute requests async and in parallel. </p> <p>Both orchestration and choreography are useful for solving different problems, and can be used together in a hybrid architecture.</p> <p></p>"},{"location":"Technical/Distributed%20Systems%20Design/Orchestration%20vs.%20Choreography.html#benefits-of-orchestration","title":"Benefits of Orchestration","text":"<ol> <li>Reliability - has built-in transaction management and error handling, while choreography is point-to-point communications and the fault tolerance scenarios are much more complicated</li> <li>Scalability - when adding a new service, only the orchestrator needs to modify the interaction rules</li> </ol>"},{"location":"Technical/Distributed%20Systems%20Design/Orchestration%20vs.%20Choreography.html#limits-of-orchestration","title":"Limits of Orchestration","text":"<ol> <li>Performance - all services talk via a centralized orchestrator, so latecy is higher than it is with choreography. The throughput is bound to the capacity of the orchestrator.</li> <li>Single poit of failure - if the orchestrator goes down, no services can talk to each other. Needs to be highly avialable.</li> </ol>"},{"location":"Technical/Distributed%20Systems%20Design/Overview.html","title":"Overview","text":"<p>Data intensive applications are one where:</p> <ol> <li>the amount of data that is generated/uses increases quickly OR</li> <li>the complexity of data generated/used increases quickly OR</li> <li>the speed of change in data increases quickly</li> </ol> <p>Distributed systems are a group of processes that run on different machines and communicate through a network to provide services and functionality. Such systems are often used to serve data intensive use cases.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Overview.html#key-requirements-for-data-intensive-applications","title":"Key Requirements for Data Intensive Applications","text":"ReliabilityScalabilityMaintainability <ul> <li>Fault Tolerance - a single node failure does not result in system failure</li> <li>No un-authorized access</li> <li>Chaos testing</li> <li>Automating tests for bugs</li> <li>Staging/Testing environment</li> <li>Ability to quickly roll-back</li> </ul> <ul> <li>Horizontal - by adding more storage and compute nodes</li> <li>Vertically - using more powerful machines</li> <li>Latency/Throughput Tradeoff - Meeting traffic load with peak number of reads, writes and simultaneous users</li> <li>Requires capacity planning with anticipated Latency and Throughput estimates</li> <li>End user response is measured as the server response time + network response time</li> <li>90<sup>th</sup>, 95<sup>th</sup>, 99<sup>th</sup> Percentile Service Level Objectives (SLOs) / Service Level Agreements (SLAs) are established</li> </ul> <ul> <li>Operable: Configurable and Testable</li> <li>Simple: Easy to understand and ramp up</li> <li>Evolveable: Easy to change</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Overview.html#advantages-of-distributed-systems","title":"Advantages of Distributed Systems","text":"<ul> <li>Scalability - can horizontally scale by adding more storage and/or compute</li> <li>Fault Tolerance through Replication - a single node failure does not result in system failure - traffic can be rerouted to functioning nodes.</li> <li>Performance - have lower latency and higher throughput. Nodes of a system can be geographically spread out so that storage and compute is closer to the external clients/consumers.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Overview.html#disadvantages-of-distributed-systems-tradeoffs","title":"Disadvantages of Distributed Systems / Tradeoffs","text":"<ul> <li>Unreliable Network - networks are inherently unreliable, and communication between different machines can be disrupted by network faults and partitions.</li> <li>Need for data replication - replicating data across multiple nodes over an unreliable network can result in lag and write conflicts</li> <li>Loss of consistency - different versions of the data are located at each node, and reconciling data differences between nodes allow non-deterministic behavior to arise.</li> <li>Coordination Overhead - additional components such as load balancers and replicas are needed to handle the distirbuted nature of the system. Strategies like leader election and failover need to be devised during outages.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/PACELC%20Theorem.html","title":"PACELC Theorem","text":"<p>This theorem expands the CAP Theorem: during a <code>Network Partition (P)</code>, a distributed system chooses between <code>Availability (A)</code> and <code>Consistency (C)</code>. <code>Else (E)</code>, where there is no network partition, a distributed system chooses between either <code>Latency (L)</code> or <code>Consistency (C)</code>.</p> <p>Therefore an additional tradeoff is introduced when a distributed system is running normally, without network partitions, an operation can either have</p> <ul> <li>low latency but possibly inconsistent data</li> <li>consistent data by syncing between nodes at the cost of latency</li> </ul> <p></p> <p>Databases with AP characteristics are usually also EL systems and are known as <code>AP/EL</code> systems.</p> <p>Databases with CP characteristics are usually also EC systems and are known as CP/EC systems. These typically support ACID transactions and prioritize consistency during network partition.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Proxy%20and%20Reverse%20Proxy.html","title":"Proxy and Reverse Proxy","text":""},{"location":"Technical/Distributed%20Systems%20Design/Proxy%20and%20Reverse%20Proxy.html#proxy-forward-proxy","title":"Proxy / Forward Proxy","text":"<p>A proxy (also called a proxy server or forward proxy) is a server that acts as an intermediary between a client and a server. A proxy server can regulate traffic according to policies and security features. Usually implemented on the client side.</p> <p>Uses of a proxy include:</p> <ol> <li>enforcing security policies on a shared network</li> <li>hiding the origin of a client's data packets and requeusts by masking a client's IP address from the receiving server</li> </ol>"},{"location":"Technical/Distributed%20Systems%20Design/Proxy%20and%20Reverse%20Proxy.html#reverse-proxy","title":"Reverse Proxy","text":"<p>Accepts requests from clients and forwards them to other servers. Usually implemented on the server side and acts as a gateway to protect the internal server side system from being exposed to the outside world.</p> <p>A reverse proxy typically sits in front of web servers and forwards requests to those web servers, which then subsequently process those requests or forward them to backend servers. Additionally, security policies and features can be implemented at this single point to protect all web and backend servers, adding anonymity and security for the entire system without having to implement policies at all servers.</p> <p>Reverse proxies also perform load balancing, which is distributing traffic across multiple servers.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Rate%20Limiter.html","title":"Rate Limiter","text":"<p>TODO: Add notes here - leaving blank for now.</p>"},{"location":"Technical/Distributed%20Systems%20Design/References.html","title":"References","text":"<ul> <li>DonneMartin</li> <li>ByteMonk</li> <li>Tech Primers</li> <li>ByteByteGo</li> <li>Hacking the System Design Inteview</li> <li>Patterns of Distributed Systems</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/References.html#papers","title":"Papers","text":"<ul> <li>The RUM Conjencture</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/System%20Integration%20and%20Communication.html","title":"System Integration and Communication","text":"<p>The process through which system components communicate and share information through compatible interfaces and messages.</p>"},{"location":"Technical/Distributed%20Systems%20Design/System%20Integration%20and%20Communication.html#database-integration","title":"Database Integration","text":"<p>Using a database is used as the primary form of sharing information between components of the system. Using database integration, system components communicate with each other indirectly through a database; one component may write to the database, and another component reads from it.</p> <p>Using a database as the primary form of sharing information runs into significant problems as the system grows larger and more complex. Here are some key problems:</p> <ol> <li>External Exposure to Internal Data Models - the internal implementation of database models is exposed and visible to external users. If there are schema changes, this could easily break the way services access the database. Since there is no external interface, all services would need to understand the data structures and schemas to use the database.</li> <li>Tight Coupling - each component is tied to the specific platform and implementation of the database. If the database platform needs to be changed, each of the services will need to make corresponding code changes.</li> <li>Lack of Security and Controls - by exposing the data models externally, the system is unable to fully control how data structures are accessed, adding security and authorization problems.</li> <li>Low Cohesion - the same logic to access the data is repeated in many services, and different versions of this logic may arise.</li> <li>Scalability problems - scaling is now only possible through the database.</li> <li>Asynchronous/Orchestration problems - if a service makes a transaction to the database, it is difficult for other components to know when the transaction was reflected in the database resulting in possible race conditions.</li> </ol> <p>Thus databases should be avoided as the primary form of integration system. Instead access should be controlled through services for both external clients and other internal clients.</p>"},{"location":"Technical/Distributed%20Systems%20Design/System%20Integration%20and%20Communication.html#rest","title":"REST","text":"<p>Representational State Transfer (REST) is an architectural style that provides a design of communication between computers on a network.</p> <p>An API that follows the REST architectural style is called a REST API (RESTFul API) and can be used for system integration. The architecture focuses on <code>scalability, interfaces, loose coupling, and encapsulation</code>. Its standards and constraints are:</p> <ul> <li>Client-server pattern - client and server should be separate components, developed independently, and communicate through an interface. This improves portability across different platforms and separates the interface (client-side) from logic and data (server-side).</li> <li>Resource Representation - data and content should be represented as resources. A REST server is responsible for managing these resources and allows clients to access and modify these resources through operations.</li> <li>Interface Uniformity - service interfaces should support the same set of operations. Allows for loose coupling, where services and clients can be developed separately without impacting each other. The downside is that this uniformity forces all clients and services on the same set of operations, and there cannot be specialized methods.</li> <li>Statelessness - operations are stateless, such that each request should incllude all the required information needed to process the request. Requests cannot assume ore require the server to have any stored context or data. Rather, the client is responsible for managing and storing all of the session's data and state. That is the client does not need to know what state the server is in and vice versa. The overhead here is that there may be repeating data in requests.</li> </ul> <p>The opposite of statelessness is statefulness where the client request uses context stored on the server.</p>"},{"location":"Technical/Distributed%20Systems%20Design/System%20Integration%20and%20Communication.html#http","title":"HTTP","text":"<p>HTTP is an example of a protocol that follows REST principles.</p> <ul> <li>HTTP requests and responses have a client-server pattern (e.g. clients such as web browsers send requests to web servers). Both client-side and server-side components can be developed separately and can operate on different technology stacks.</li> <li>HTTP treats content such as HTML, CSS, Javascript, images, and video files as server side resources. Each file is accessed as a resource with a unique URL.</li> <li>HTTP operations follow a uniform interface because they use the same request methods. The four main HTTP methods used to send and receive requests are <code>POST, GET, PUT, and DELETE</code> which correspond to <code>CRUD (create, read, update, delete)</code> operations.</li> </ul> HTTP Method URL Path Description POST <code>/courses</code> Create a new course GET <code>/courses/{course_id}</code> Get an existing course GET <code>/courses/</code> Get all courses PUT <code>/courses/{course_id}</code> Updated an existing course DELETE <code>/courses/{course_id}</code> Delete an existing course <ul> <li>HTTP requests are stateless - each of the CRUD operations contain all of the information needed to process and form a response. The client manages all of the information and does not assume that the server has any stored context.</li> </ul> <p>A system can be RESTful without using HTTP, and likewise HTTP communication does not need to follow REST principles.</p>"},{"location":"Technical/Distributed%20Systems%20Design/System%20Integration%20and%20Communication.html#rpc","title":"RPC","text":"<p>Remote Procedure Call (RPC) is a form of inter-process communication in which a process invokes a local call that causes a routine to execute in another process that is running on a remote machine over a network. A call (procedure call, function call, or subroutine call) is the invoking of a function.</p> <p>Popular RPC frameworks include gRPC, Thrift, and JSON RPC.</p> <p>RPC Frameworks use stubs - these are compiled with the client and server code to communicate between the server and client.</p> <ul> <li>A client stub receives the local call from the client and is responsible for the conversion of parameters and data. It also performs serialization by transforming the request into a transmittable form that can be sent over a network.</li> <li>A server stub is responsible for the deserialization of the transmitted request and the deconversion of the parameters and data into a remote call to the server. It is also responsible for the conversion and serialization of the response back to the client.</li> </ul> <p>An RPC Runtime is the software that manages the transmission of the request and responses between client and server. This runtime is typically built on top of the networking protocols and is responsible for retransmission and encryption.</p> <p></p>"},{"location":"Technical/Distributed%20Systems%20Design/System%20Integration%20and%20Communication.html#interface-definition-language-idl","title":"Interface Definition Language (IDL)","text":"<p>Defines how software components written in different languages can communicate with one another through a common interface that is language-independent.</p> <p>For example, gRPC provides an IDL called Protocol Buffers (or \"protobug\") and defines messages and RPC methods that are language agnostic.</p> <pre><code>service CourseService {\n    // Retrieves a Course\n    rpc GetCourse (GetCourseRequest) returns (GetCourseResponse) {}\n}\n\n// The request message sent from the client\nmessage GetCourseRequest {\n    required int id = 1;\n}\n\n// The response message sent from the server\nmessage GetCourseResponse {\n    required int id = 1;\n    required string name = 2;\n    optional int enrollment = 3;\n    repeated string categories = 4;\n}\n</code></pre>"},{"location":"Technical/Distributed%20Systems%20Design/System%20Integration%20and%20Communication.html#benefits-of-rpc","title":"Benefits of RPC","text":"<ol> <li>Abstraction - applications and developers do not need to understand the inter-process network call mechanism</li> <li>RPC Frameworks - provide stubs in high-level languages, allowing applications to integrate with minimal efforts</li> <li>Method calls do not need to conform to a standard interface and can be highly optimized</li> <li>Allows for serialization in binary and customized formats, which typically means tight data packing and smaller packet payloads</li> </ol> <p>There just is no standardization - one downside compared to REST.</p>"},{"location":"Technical/Distributed%20Systems%20Design/System%20Integration%20and%20Communication.html#rest-vs-rpc","title":"REST vs. RPC","text":"<p>Generally integration with external clients use REST protocol (e.g. RESTful HTTP) but integration behind the reverse proxy for internal components uses an RPC framework. This pattern is common because REST calls have a standard interface so that most client-side applications can be developed without being tied to specific technologies. However RPCs are usually more performant than REST calls, and a system where a single RPC framework is enforced can achieve a more efficient integration.</p> <p></p> <p>REST APIs are centered around resources while RPC APIs are centered around actions.</p> REST Method + Resource RPC Action <code>POST /courses</code> <code>createNewCourse()</code> <code>GET /courses/{course_id}</code> <code>getCourse()</code> <code>PUT /course/{course_id}</code> <code>updateCourse()</code> <code>DELETE /course/{course_id}</code> <code>deleteCourse()</code> <p>RPC frameworks also add another application layer of protocol on top of existing Internet protocols unlike REST (which uses HTTP). The result is that an RPC framework is tightly coupled with its underlying technology stack, whereas REST implementations are more agnostic of the technology stack.</p> <p>A summary of the comparisons between REST and RPC:</p> REST RPC Resources representation of data Action based view of data Human readable message formats (e.g. JSON) Binary message format (e.g. protocol buffers) Usually uses the HTTP 1.1 protocol Customized network communication protocols or not widely used protocols Lower throughput performance and high latency Higher throughput performance and low latency Usually uses the standard HTTP methods, no need for additional libraries or code Custom developer defined methods that use stubds and an RPC runtime Self-documenting Separate documentation Stateless Stateless Communication through HTTP action verbs (CRUD) Communication is performed by invoking method calls and specifying params"},{"location":"Technical/Distributed%20Systems%20Design/System%20Integration%20and%20Communication.html#more-on-serialization","title":"More On Serialization","text":"<p>Naive approach to serialization is to use language specific frameworks like Python's pickle, Marshal for Ruby, Serializable for Java etc. However these are locked to a single language and deserialize data into arbitrary classes which lead to security vulnerabilities. Also there is not data versioning.</p> <p>Some common standardized encoding formats used are:</p> Feature JSON Protocol Buffers / Thrift Avro Parquet Data Representation Human-readable Binary Binary Binary Schema Support No Yes Yes Yes Schema Evolution Support No Limited Yes Limited Efficiency Less efficient due to text format Highly efficient binary format Efficient binary format Highly efficient for analytics Programming Language Support Widely supported Supports multiple languages Supports multiple languages Limited (primarily for analytics) Tagging of Fields No Yes No No Readability Human-readable Not human-readable Not human-readable Not human-readable Common Use Cases Interchange between systems Efficient data transmission between systems Data interchange, message serialization Efficient storage and querying for analytics Examples Web APIs, configuration files Google APIs, Apache Thrift Apache Kafka, Apache Avro, Hadoop ecosystem Apache Hadoop, Apache Spark"},{"location":"Technical/Distributed%20Systems%20Design/System%20Integration%20and%20Communication.html#realtime-updates","title":"Realtime Updates","text":"<p>When there is a need to provide real time updates from a database or server back to a client (e.g. in a chat app, ride sharing app, and notification services) there are a few options:</p> <ol> <li> <p>Long Polling</p> <ul> <li>Description<ul> <li>make a typical HTTP request to the server, and if the server does not have newdata it keeps the request open, and eventually reponds once new dta is present.</li> <li>once new data is recieved, the client issues another long polling request</li> <li>connection timeouts also results in client sending another request</li> </ul> </li> <li>Pros:</li> <li>Easy to implement</li> <li>Cons:</li> <li>one directional, and can cause excess load by constantly recreating HTTP connections</li> <li>possible race condition of multiple requests</li> </ul> </li> <li> <p>Websockets</p> <ul> <li>Description</li> <li>fully bidirectional communication channel between clients and servers</li> <li>websockets are registered to a port which means it can have ~65k active connections</li> <li>Pros:</li> <li>bidirectional communication in realtime</li> <li>lower network overhead as headers are only sent once</li> <li>Cons:</li> <li>less support for older browsers / clients</li> <li>may be complex for infrequent data changes</li> </ul> </li> <li> <p>Server Sent Events</p> <ul> <li>Description</li> <li>one way connection from servers to clients, cliet-side code registers for events from the server</li> <li>Pros:</li> <li>persistent HTTP connection (unlike long polling)</li> <li>connection restablished automatically upon failure (unlike websockets)</li> <li>Cons:</li> <li>single directional communication</li> <li>a lot of concurrent connections can add load on server</li> </ul> </li> </ol>"},{"location":"Technical/Distributed%20Systems%20Design/Testing.html","title":"Testing","text":"<p>There are different types of tests that serve different purposes and are often used collectively in modern systems.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Testing.html#functional-testing-tests-that-validate-correctness-of-a-system","title":"Functional Testing: Tests that Validate Correctness of a System","text":"<p>These tests are used to verify inputs and outputs of a system.</p> <p>The difference between these tess is scope of the test in terms of components. Usually there are many unit tests, fewer integration tests, and even fewer system tests.</p> <ul> <li> <p>Unit Testing - testing individual sofware units e.g. functions, methods, class, object etc.</p> </li> <li> <p>Integration Testing - process of testing software units as a group i.e. testing of an entire component or subsystem of a system. The goal is to verify that different software units are interacting as expected. For example software units could function correctly as individual units but incorrectly when they interact.</p> </li> <li> <p>System/End-to-end Test - process of testing an entire system from start to end, where often the external client application is used as the test entry point. The attempt is to simulate production behavior and verify the system's dependencies, network communication, and interfaces.</p> </li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Testing.html#non-functional-testing","title":"Non-Functional Testing","text":"<ul> <li> <p>Regression Testing - testing if recent code or configuration changes have negatively impacted existing system features. It can be functional or non-functional in practicality.</p> </li> <li> <p>Performance Testing - process of testing the latency, scalibility and reliability of a system or a subsystem.</p> </li> <li>Types of tests:<ul> <li>Load Testing - test behavior of a system or subsystem under anticipated workloads, discovering bottlenecks or slow performance</li> <li>Stress Testing - tests te limit of a system or subsystem uner heavy workloads, identify where the system would break down under the most extreme circumstances</li> <li>Endurance Testing - tests the stability of a system or subsystem over a long period, identifying if problems arise over time</li> </ul> </li> <li> <p>Key metrics:</p> <ul> <li>CPU Usage - the amount of CPU used to execute a workload</li> <li>Memory Usage - the amount of RAM used during a workload</li> <li>Disk Usage - the amount of hard drive or SSD used during a workload</li> <li>Bandwidth - the byte per second sent over the network interface during a workload</li> <li>Response Time - the time elapsed from a request is sent to when a response is recieved</li> <li>Throughput - the number of requests per second that are processed by the system</li> </ul> </li> <li> <p>Security Testing - testing risks, vulnerabilities, and weaknesses in a system or a subsystem. Involves scanning for vulnerabilities and discovering network and system weaknesses. This verifies that the system can protect itself against malicious attacks such as a denial-of-service attack (DoS) or a Distributed Denial-of-Service Attack (DDoS) where a malicious user attempts to overload a system by sending an overwhelming number of requests. </p> </li> <li> <p>A/B Testing - compare varients of the same component and measure performance through randomized experiments. See Experimentation Notes</p> </li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Testing.html#flakiness","title":"Flakiness","text":"<p>A flaky tet is a test that inconsistently passes - i.e. it is non-deterministic. Usually this can be attributed to:</p> <ol> <li>test order - if tests generate state, they may impact each other, and a test pass may rely on other tests to run in a particular order</li> <li>race conditions - threads and processes use synchronization mechanisms (e.g. semaphore or mutexes) where there may be race conditions that cause flakiness.</li> <li>assertion timing - asserts or checks are run when tests are in inconsistent states, resulting in flaky tests</li> </ol>"},{"location":"Technical/Distributed%20Systems%20Design/Unique%20ID%20Generators.html","title":"Unique ID Generators","text":"<p>A distributed unique ID generator is a system or service that generates unique identifiers across distributed systems in a scalable and efficient manner. These identifiers are crucial for various purposes, such as database primary keys, message correlation, and ensuring data consistency in distributed systems.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Unique%20ID%20Generators.html#guid-globally-unique-identifier","title":"GUID (Globally Unique Identifier)","text":"<p>GUID is a type of unique identifier defined by Microsoft, also known as UUID (Universally Unique Identifier). GUIDs are 128-bit numbers represented as 32 hexadecimal digits, typically generated using algorithms that guarantee uniqueness across space and time, although collisions are theoretically possible.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Unique%20ID%20Generators.html#cons-of-guid","title":"Cons of GUID:","text":"<ul> <li>Length: GUIDs are relatively long, comprising 32 hexadecimal digits, which can increase storage and bandwidth requirements, especially in systems with large datasets or high throughput.</li> <li>Performance Impact: Generating GUIDs can incur performance overhead, particularly in high-concurrency scenarios, due to the complexity of generating unique identifiers and the potential for collisions.</li> <li>Non-sequential: GUIDs are not inherently sequential or sortable by time, which can hinder efficient indexing and querying in databases, especially for time-based data analysis and processing.</li> <li>Uniqueness Guarantee: While GUIDs are designed to be globally unique, there's a small probability of collisions due to the finite size of the identifier space, which may be a concern in systems with extremely high data volumes or stringent uniqueness requirements.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Unique%20ID%20Generators.html#unique-id-service","title":"Unique ID Service","text":"<p>A unique ID service is a centralized or distributed service responsible for generating and managing unique identifiers within a system or across multiple systems. It provides APIs or interfaces for applications to request and obtain unique IDs, ensuring consistency and uniqueness in ID generation.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Unique%20ID%20Generators.html#cons-of-unique-id-service","title":"Cons of Unique ID Service:","text":"<ul> <li>Single Point of Failure: Centralized unique ID services can become single points of failure, causing system-wide outages if the service becomes unavailable or experiences downtime.</li> <li>Scalability Challenges: Centralized ID generation can pose scalability challenges as the system grows, leading to performance bottlenecks and limiting the system's ability to handle increasing request volumes.</li> <li>Latency: Accessing a remote unique ID service over a network can introduce latency, impacting application performance, especially in distributed or geographically dispersed environments.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Unique%20ID%20Generators.html#snowflakecomposite-id-twitter","title":"Snowflake/Composite ID (Twitter)","text":"<p>The Snowflake ID generation algorithm, introduced by Twitter, is a distributed unique ID generation system designed for scalability and efficiency. It generates 64-bit unique IDs consisting of a timestamp, a worker ID, and a sequence number, ensuring monotonicity and uniqueness within a distributed environment.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Unique%20ID%20Generators.html#components-of-snowflake-id","title":"Components of Snowflake ID:","text":"<ul> <li>Timestamp: The timestamp component ensures that IDs are sortable by time, facilitating efficient data indexing and retrieval.</li> <li>Worker ID: A unique identifier assigned to each worker node or machine in the distributed system, preventing ID collisions across nodes.</li> <li>Sequence Number: A counter that increments for each ID generated within the same millisecond to avoid collisions in high-concurrency scenarios.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Unique%20ID%20Generators.html#benefits-of-snowflakecomposite-id","title":"Benefits of Snowflake/Composite ID:","text":"<ul> <li>High Scalability: Snowflake IDs can be generated at high throughput across distributed systems without centralized coordination, enabling horizontal scaling.</li> <li>Efficiency: The use of timestamp, worker ID, and sequence number ensures efficient data retrieval and indexing while maintaining uniqueness.</li> <li>Monotonicity: IDs generated by Snowflake are guaranteed to be monotonically increasing or decreasing when sorted by time, facilitating chronological data analysis and processing.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Unique%20ID%20Generators.html#considerations","title":"Considerations:","text":"<ul> <li>Clock Synchronization: Snowflake relies on synchronized clocks to generate unique timestamps accurately, necessitating clock synchronization mechanisms in distributed environments.</li> <li>Worker ID Assignment: Careful assignment and management of worker IDs are essential to prevent collisions and ensure balanced load distribution across worker nodes.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Vertical%20and%20Horizontal%20Scaling.html","title":"Vertical and Horizontal Scaling","text":""},{"location":"Technical/Distributed%20Systems%20Design/Vertical%20and%20Horizontal%20Scaling.html#vertical-scaling","title":"Vertical Scaling","text":""},{"location":"Technical/Distributed%20Systems%20Design/Vertical%20and%20Horizontal%20Scaling.html#horizontal-scaling","title":"Horizontal Scaling","text":"<p>Distributing data over multiple nodes.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Vertical%20and%20Horizontal%20Scaling.html#partitioning-sharding","title":"Partitioning / Sharding","text":"<p>Splitting database or service across multiple nodes for scalability, such that each one has a subset of the whole data. To handle increased query rates and data amounts, we strive for balanced partitions and blaanced read/write load.</p> <p>Partitioning must be balanced so that each partition recieves about the same amount of data - if it is unbalanced a majority of the queries will fall into a few partitions and become heavily loaded system bottlenecks. </p>"},{"location":"Technical/Distributed%20Systems%20Design/Vertical%20and%20Horizontal%20Scaling.html#vertical-sharding","title":"Vertical Sharding","text":"<p>Distributing tables across multiple nodes, where separate databases are used for sets of tables (e.g. payments related tables in one database server, and user related tables in another database server).</p> <p>We need to be careful if there are joins between multiple tables, and keep them together on one shard. </p> <p>Often used to increase the speed of data retrieval from a table consisting of columns with very wide text or binary object data. In this case the column with the large text or blob is split into another table.</p> <p>This is usually more ameanable to manual sharding.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Vertical%20and%20Horizontal%20Scaling.html#horizontal-sharding","title":"Horizontal Sharding","text":"<p>When tables in the database become too big they also affect read/write latency - and horizontal sharding helps splitting the data row-wise and distributing each partition over database servers.</p>"},{"location":"Technical/Distributed%20Systems%20Design/Vertical%20and%20Horizontal%20Scaling.html#strategies","title":"Strategies","text":"<ol> <li> <p>Key-range based sharding</p> <ul> <li>Details:<ul> <li>each partition is assigned a continuous range of keys</li> <li>for tables in a relation (bound by foreign key) use the same partition, and tables that belong to the same partition key are colocated in the same partition</li> <li>a partition key table is used by applications to look for data in a specific shard</li> <li>a <code>created_at</code> column serves as the data consistency point, assuming clocks of nodes are synhronized</li> </ul> </li> <li>Pros:<ul> <li>easy to implement range queries as we know exactly which node/shard to look for a specific range of keys</li> </ul> </li> <li>Cons:<ul> <li>range queries on data other than partition key is hard</li> <li>some nodes may store more data due to uneven distribution</li> </ul> </li> </ul> </li> <li> <p>Hash-based sharding</p> <ul> <li>Details:<ul> <li>uses hash function on an attribute</li> <li>gives each partition a range of hashes</li> </ul> </li> <li>Pros:<ul> <li>uniform distribution of keys</li> </ul> </li> <li>Cons:<ul> <li>can't use to perform range queries</li> </ul> </li> </ul> </li> </ol>"},{"location":"Technical/Distributed%20Systems%20Design/Vertical%20and%20Horizontal%20Scaling.html#some-key-concepts","title":"Some Key Concepts","text":"<ol> <li> <p>Consistent Hashing</p> <ul> <li>Details:<ul> <li>assigns each server or item in a distributed has table a place on an abstract circle (hash ring) irrespective of the number of servers/shard on the table. </li> <li>this permits servers/shard to scale without compromising the system's overall performance i.e. minimal reshuffle due to adding or removing shards when using hash based sharding</li> </ul> </li> <li>Pros:<ul> <li>easy to scale horizontally</li> <li>increases throughput and improves latency</li> </ul> </li> <li>Cons:<ul> <li>randomly assigning nodes on the ring may cause non-uniformity</li> </ul> </li> </ul> </li> <li> <p>Rebalancing</p> <ul> <li>Details:<ul> <li>query load can get imbalanced across nodes due to:<ul> <li>distributionof data not being equal</li> <li>too much load on a single partition</li> <li>increase in query traffic</li> </ul> </li> <li>key strategies:</li> <li>avoid hash mod n (as changing number of shards will re-shuffle all data)</li> <li>fixed number of partitions</li> <li>dynamic partitioning</li> <li>partition proportionally to nodes</li> </ul> </li> </ul> </li> <li> <p>Partioning and Secondary Indexes</p> <ul> <li>strategies for managing secondary indexes in a partitioned database:<ul> <li>Partition by Document: Local indexes that are independent per partition.</li> <li>Partition by Term: Global indexes that span all partitions, suitable for read-heavy workloads but complicating writes.</li> </ul> </li> </ul> </li> <li> <p>Request Routing</p> <ul> <li>Ensuring clients connect to the correct data node:<ul> <li>Direct Routing: Clients have knowledge of the partitioning scheme.</li> <li>Routing Tier: A middleware layer that directs client requests based on partition information.</li> <li>ZooKeeper: Manages cluster state and routing information, notifying clients or routing layers of changes.</li> </ul> </li> </ul> </li> </ol>"},{"location":"Technical/Distributed%20Systems%20Design/Vertical%20and%20Horizontal%20Scaling.html#key-issues-to-look-out-for","title":"Key Issues to Look Out For","text":"<ul> <li>Hot spots / Skews</li> <li>Key Hash Based Partition</li> <li>Rebalancing strategies</li> <li>Routing Logic Placement</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Vertical%20and%20Horizontal%20Scaling.html#replication","title":"Replication","text":"<p>The process of keeping multiple copies of data and syncing data between databases nodes in a consistent way to achieve availability, scalability and performance.</p> <p>This is needed due to a few reasons:</p> <ul> <li>Resilience of service to machine failures</li> <li>Improving latency for global audience</li> <li>Offline/Network failures</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Vertical%20and%20Horizontal%20Scaling.html#types-of-replication-models","title":"Types of Replication Models","text":"<ul> <li> <p>Single Leader Replication</p> <ul> <li>Characteristics: One database (the leader) handles all write operations. Followers (slaves) replicate the leader's state and serve read operations.</li> <li>Advantages: Simplifies conflict resolution and write consistency. Improves read scalability.</li> <li>Disadvantages: Single point of failure at the leader. Potential for replication lag causing stale reads.</li> </ul> </li> <li> <p>Multi-Leader Replication</p> <ul> <li>Characteristics: Multiple databases act as leaders, each capable of handling writes. Writes are replicated across all leaders.</li> <li>Advantages: Eliminates single point of failure. Reduces write latency in geographically distributed setups.</li> <li>Disadvantages: Complex conflict resolution. Higher system configuration and maintenance complexity.</li> </ul> </li> <li> <p>Leaderless Replication</p> <ul> <li>Characteristics: Every node can accept both reads and writes, replicating data across several nodes without a central leader.</li> <li>Advantages: High availability and fault tolerance. Enhanced scalability and performance.</li> <li>Disadvantages: Complex conflict resolution mechanisms needed. Typically offers eventual consistency.</li> </ul> </li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Vertical%20and%20Horizontal%20Scaling.html#methods-to-capture-changes-for-replication","title":"Methods to Capture Changes for Replication","text":"<ul> <li> <p>Write-Ahead Logging (WAL)</p> <ul> <li>Mechanism: Before any changes are made to the database, they are logged. This log is then used to replicate changes to follower nodes.</li> <li>Used By: Primarily single leader replication models for ensuring data consistency and aiding in recovery processes.</li> <li>Advantages: Ensures data integrity and aids in crash recovery.</li> <li>Disadvantages: Can increase disk I/O overhead.</li> </ul> </li> <li> <p>Statement-Based Replication</p> <ul> <li>Mechanism: The SQL statements executed on the master (or leader) database are logged and then executed on the follower databases.</li> <li>Used By: Both single leader and, in some configurations, multi-leader replication models.</li> <li>Advantages: Simplicity in replicating changes.</li> <li>Disadvantages: Can lead to issues with non-deterministic functions or statements leading to inconsistent states.</li> </ul> </li> <li> <p>Row-Based Replication</p> <ul> <li>Mechanism: Instead of logging the SQL statements, the actual changes to the rows are logged and replicated.</li> <li>Used By: Often used in scenarios where statement-based replication might lead to inconsistencies due to non-deterministic statements.</li> <li>Advantages: Avoids problems with non-deterministic functions and ensures that exact changes are replicated.</li> <li>Disadvantages: Can result in larger amounts of data needing to be logged and replicated, especially for large transactions.</li> </ul> </li> <li> <p>Logical Replication</p> <ul> <li>Mechanism: Changes are captured and replicated at a higher logical level rather than the physical level, allowing for more flexibility.</li> <li>Used By: Multi-leader replication models or leaderless models, supporting heterogeneous database systems.</li> <li>Advantages: Flexibility to replicate between different database versions or different database systems.</li> <li>Disadvantages: Might require additional logic to handle schema or data type differences.</li> </ul> </li> <li> <p>Conflict Resolution Mechanisms</p> <ul> <li>Context: Necessary in multi-leader and leaderless replication models to resolve discrepancies when the same data is modified in different locations.</li> <li>Mechanisms: Include last write wins (LWW), vector clocks, or application-specific logic.</li> <li>Advantages: Ensures data consistency across replicated nodes.</li> <li>Disadvantages: Can add complexity to the replication system and might result in data loss or overwrite under certain conditions.</li> </ul> </li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Web%20Server.html","title":"Web Server","text":"<p>We distinguish between frontend server, that is part of the first layer of servers that a request reachers, and backend-server, that is not part of the first layer of servers and often contains services and controls resource access.</p> <p>Frontend servers are designed to be external facing, and handle unregulated and unfiltered requests to the system. Backend servers are designed to run services, perform computations, and access databases.</p> <p></p> <p>A web server is a frontend server that is stateless and responds to network requests over HTTP and other web protocols. It is commonly used to process and deliver webpages to clients such as web browsers. Its job is to sit between the external clients and the system's services. Its roles may include:</p> <ol> <li>Receiving Requests - Listens on a port for network requests (e.g., HTTP) to the web server.</li> <li>Request Routing - Directs incoming requests to appropriate endpoints or services based on predefined rules.</li> <li>Request Deduplication - Eliminates duplicate requests to prevent unnecessary processing and improve efficiency.</li> <li>Session Management - Tracks and maintains user sessions to enable stateful interactions with the server.</li> <li>Request Validation - Validates incoming requests to ensure they meet predefined criteria for security and integrity.</li> <li>Authentication - Verifies the identity of users or clients attempting to access the server or its resources.</li> <li>Authorization - Determines whether authenticated users have permission to access requested resources.</li> <li>Malicious Traffic Detection - Identifies and blocks or mitigates malicious requests or traffic aimed at compromising server security.</li> <li>Throttling - Limits the rate of incoming requests from clients to prevent overloading the server and ensure fair resource allocation.</li> <li>Load Shedding - Discards or delays requests when the server is under heavy load to prioritize essential operations and maintain stability.</li> <li>Response Caching - Stores frequently accessed responses to reduce server load and improve performance by serving cached content.</li> <li>TLS/SSL Termination - Decrypts incoming HTTPS requests, allowing the server to process them in plain text.</li> <li>TLS/SSL Encryption - Encrypts outgoing responses before transmission to ensure data confidentiality and integrity over the network.</li> <li>Server-side Encryption - Encrypts data stored on the server to protect it from unauthorized access or tampering.</li> <li>Usage Analytics and Data Collection - Gathers and analyzes data on server usage, user interactions, and performance metrics for monitoring and optimization purposes.</li> </ol>"},{"location":"Technical/Distributed%20Systems%20Design/Web%20Server.html#api-gateway","title":"API Gateway","text":"<p>Web Servers can often also be used as or be called API Gateways, which provides a single point of access to multiple services. This provides an external facing interface that masks the structure of the services in the system, providing better readability to external clients. Also backend services can be refactored or redesigned without impacting external client interfaces.</p> <p>An API Gateway would therefore have consolidated Read and Write APIs:</p> <p>Read API - the portion of the API Gateway that allows clients to perform read requests without directly communicating with respective services through single methods. It can stitch together results of multiple async calls to different services and return them as a single result.</p> <ul> <li>can be used by both internal and external clients</li> <li>often uses a cache to re-serve data without invoking repeated calls to services</li> <li>can track cache hit ratios and implement its own caching logic</li> </ul> <p>Write API - similar to read API but performs CUD (Create, Update and Delete) operations across services. It often trigger on-write actions such as:</p> <ul> <li>fan out on write</li> <li>copy files to object store</li> <li>copy files to CDN or cache</li> </ul> <p>The advantages of an API Gateway are listed below:</p> <ul> <li>Centralized Access Point: Provides a single entry point for all client requests, simplifying management and monitoring.</li> <li>Security Enforcement: Implements security measures like authentication, authorization, and encryption to protect backend services.</li> <li>Traffic Management: Offers features like rate limiting, throttling, and load balancing to ensure optimal performance and resource utilization.</li> <li>Protocol Translation: Translates incoming requests from various protocols to a standard format, facilitating communication between clients and backend services.</li> <li>Analytics and Monitoring: Collects data on API usage, performance metrics, and errors for monitoring, troubleshooting, and optimization.</li> <li>Service Composition: Allows for combining multiple microservices into a unified API, abstracting complexity for clients.</li> <li>Versioning and Lifecycle Management: Supports versioning of APIs and manages their lifecycle, enabling smooth updates and deprecations.</li> <li>Caching and Response Optimization: Implements caching strategies to improve latency and reduce load on backend services by serving cached responses.</li> <li>Cross-Origin Resource Sharing (CORS): Facilitates CORS policies to manage access control for web applications accessing APIs from different origins.</li> <li>Scalability and Flexibility: Scales horizontally to handle growing traffic and can be easily configured to adapt to changing business requirements.</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/Great%20Writeups/links.html","title":"Great Distributed System Writeups","text":"<ul> <li>Staying in the Zone: How DoorDash used a service mesh to manage  data transfer, reducing hops and cloud spend</li> </ul>"},{"location":"Technical/Distributed%20Systems%20Design/System%20Designs/Github.com.html","title":"Github.com","text":""},{"location":"Technical/Distributed%20Systems%20Design/System%20Designs/URL%20Forwarding%20or%20Object%20Sharing.html","title":"URL Forwarding or Object Sharing","text":""},{"location":"Technical/Experimentation/Basic%20Probability%20and%20Statistics.html","title":"Basic Probability and Statistics","text":"<p>P(A): probability of A P(A'): probability of not A (the complement) P(A, B): probability of A and B occuring together</p>"},{"location":"Technical/Experimentation/Basic%20Probability%20and%20Statistics.html#basic-principles","title":"Basic Principles","text":"<ul> <li>P(A) &lt;= 1</li> <li>P(A') = 1 - P(A)</li> <li>P(A or B) = P(A) + P(B) - P(A and B)</li> </ul>"},{"location":"Technical/Experimentation/Basic%20Probability%20and%20Statistics.html#conditional-probability","title":"Conditional Probability","text":"<p>P(A|B) = P(A and B) / P(B)</p> <p>Example: A: number of fatalities B: number of people whose age is between 40-49 P(A and B): number of fatalities for people whose age is between 40-49</p> <p>P(A|B) = P(A and B) / P(B)</p> <p>P(A|B) != P(A) as higher age groups tend to have higher fatality rate</p>"},{"location":"Technical/Experimentation/Basic%20Probability%20and%20Statistics.html#independence","title":"Independence","text":"<p>If A and B are independent (not correlated) then P(A and B) = P(A)P(B) and P(A|B) = P(A)</p>"},{"location":"Technical/Experimentation/Basic%20Probability%20and%20Statistics.html#conditional-independence","title":"Conditional Independence","text":"<p>[Typical Independence] Suppose Alice and Malice each toss separate die. A is Alice's outcome and B is Malice's outcome. Knowing Alice's outcome provides no information about Malice's outcome.</p> <p>However if Alice and Malice toss the same dice, and the dice is biased toward showing some numbers, then knowing Alice's outcome provide some information about Malice's outcome can be inferred.</p> <p>Independece does not hold A\u27c2B | C if P(A|B, C) = P(A|C)</p> <p>Given the information of C, B contributes no information about A.</p> <p>If P(C) is the distribution for outcomes for the shared dice, then B\u27c2A | C - i.e. knowing Alice's outcome A gives no additional informaiton about Malice's outcome B. Therefore P(B|(A, C)) = P(B|C).</p> <p>Another Example:</p> <pre><code>graph TD\n    A --&gt; B;\n    A --&gt; C;\n    C --&gt; E;\n    D --&gt; E;\n\n  subgraph \"Stats Knowledge\";\n    A[Statistics];\n    B[Causal Inference];\n    C[Machine Learning];\n  end\n\n  subgraph \"Job Requirement\";\n    C[Machine Learning];\n    D[SQL Skill];\n    E[Job Offer];\n  end</code></pre> <ul> <li>knowledge of statistics means you know more about causal inference and machine learning</li> <li>to get a job offer you need to know either machine learning or SQL</li> <li>B is not independent of C - if you are good at causal inference, you are probably good at statistics, and likely good at machine learning</li> <li>B is independent of C given/conditioned on A - if you know someones level of statistics, you can infer machine learning skills and knowledge of causal inference will not give any further information</li> <li>C is independent of D - knowing how good you are at machine learning tells me nothing about how good you are at SQL</li> <li>C is not conditionally independent of D given E - given you get a job offer, knowing how good you are at ML gives me an idea about your SQL skills</li> </ul>"},{"location":"Technical/Experimentation/Basic%20Probability%20and%20Statistics.html#simpsons-paradox","title":"Simpson's Paradox","text":"<p>It can be that for all age groups, fatality rate in Italy is lower than those in China, but the overall fatality rate in Italy is higher than in China.</p> <p>This is because Age is a confounding factor - causing the Simpson Paradox. Italian population is generally older than the Chinese population.</p> <pre><code>graph LR\n    Country --&gt; Age\n    Country --&gt; Fatality\n    Age --&gt; Fatality</code></pre>"},{"location":"Technical/Experimentation/Basic%20Probability%20and%20Statistics.html#potential-outcomes-framework","title":"Potential Outcomes Framework","text":"<p>For a set of i.i.d. subjects \\(i = 1, ..., n\\) we observe a tuple \\((X_i, Y_i, W_i)\\) comprised of:</p> <ul> <li>a feature vector \\(X_i\\)</li> <li>a response or potential outcomes \\(Y_i\\)</li> <li>a treatment assignment \\(T_i\\)</li> </ul> <p>The goal is to estimate the Causal Estimand or Causal Effect of Treatment. However we only get to observe one outcome for each \\(i\\) i.e. \\(Y_i = Y_i(T_i)\\) - you are either treated or not treated.</p> <pre><code>graph LR\n    A[Causal Estimand] --&gt; |Identificaiton| B[Statistical Estimand]\n    B[Statistical Estimand] --&gt; |Estimation| C[Estimate]</code></pre> <ul> <li>This is the \"missingness\" issue in causal inference</li> <li>Causal Estimand or Causal Effect of Treatment: \\(E[Y(1) - Y(0)]\\)</li> <li>Statistical Estimand: \\(E_X[E[Y|T=1, X] - E[Y|T=0, X]]\\)</li> <li>Estimate: \\(Average(Average(Y_i|T_i=1)) - Average(Y_i|T_i=0))\\)</li> </ul>"},{"location":"Technical/Experimentation/Basic%20Probability%20and%20Statistics.html#average-treatment-effect-ate","title":"Average Treatment Effect - ATE","text":"<p>The causal effect of treatment is the average treatment effect i.e. \\(E[Y(1) - Y(0)]\\).</p> <p>However we cannot observe treatment and control outcomes for all subjects \\(i\\).</p> <p>Approach 1: Pure Randomized Control Trial</p> <ul> <li>we assume that \\((Y_0, Y_1) \\bot T\\)</li> <li>the potential outcomes are independent of the treatment assignment</li> <li>in an RCT we get the following: \\(E[Y_i(1)] - E[Y_i(0)] = E[Y_i(1)|T_i=1] - E[Y_i(0)|T_i=0] = E[Y_i|T_i=1] - E[Y_i|T_i=0]\\)</li> </ul> <p>Approach 2: Conditional Average Treatment Effect</p> <ul> <li>used when we expect that the treatment assignment is confounded by pre-treatment covariates \\(X\\)</li> <li>this is commonly seen in observed data</li> <li>we assume that \\((Y_0, Y_1) \\bot T | X\\)</li> <li>controlling for X is enough if treatment is as good as random conditional on \\(X\\)</li> <li>this is also known as unconfoundedness</li> <li>this means that the ATE is calculated as \\(E[Y_i(1) - Y_i(0)] =E[Y_i(1)-Y_i(0)|X] = E[E[Y_i|X_i, T_i=1] - E[Y_i|X_i, T_i=0]\\)</li> <li>note that \\(\\text{CATE}\\) and individual treatment effect can be considered the same</li> </ul>"},{"location":"Technical/Experimentation/Basic%20Probability%20and%20Statistics.html#choice-of-estimator","title":"Choice of Estimator","text":"<p>A good estimator is:</p> <ul> <li>Unbiased</li> <li>Consistent</li> <li>Efficient</li> <li>Robust</li> </ul>"},{"location":"Technical/Experimentation/Basic%20Probability%20and%20Statistics.html#example","title":"Example","text":"<pre><code>graph LR\n    cx[&lt;b&gt;Alice&lt;/b&gt;&lt;br&gt;Age: 25&lt;br&gt;Cohort Age: 5&lt;br&gt;Region: City&lt;br&gt;Device: iOS] --&gt; t0[T=0&lt;br&gt;Promotion]\n    cx --&gt; t1[T=1&lt;br&gt;No promotion]\n    t0 --&gt; o1[\"LTV=?&lt;br&gt;Y(0)\"]\n    t1 --&gt; o2[\"LTV=?&lt;br&gt;Y(1)\"]\n    style cx text-align: left\n    style t0 text-align: left\n    style t1 text-align: left\n    style o1 text-align: left</code></pre> X (age, cohort_age, region, device) Promo (Y1) No Promo (Y0) Observed Y CATE (25, 5, city, iOS) 60 55 60 5 (35, 1, suburb, iOS) 70 65 65 5 (25, 5, city, android) 70 60 70 10 (27, 0, city, iOS) 90 80 80 10 (36, 2, city, android) 85 80 80 5 (17, 2, suburb, iOS) 75 70 75 5 (21, 7, suburb, iOS) 100 90 90 10 (36, 2, city, android) 80 70 80 10 E(Y) 71.25 78.75 7.5 ATE 7.5 <p>The bold values represent actual observed outcomes.</p> <p>If we assumed RCT then ATE = -7.5 = E[Y|T=1] - E[Y|T=0].</p> <p>However in this case we can see that the actual treatment effect is 7.5 - and a perfect CATE strategy would estimate that.</p>"},{"location":"Technical/Experimentation/Basic%20Probability%20and%20Statistics.html#assumptions","title":"Assumptions","text":""},{"location":"Technical/Experimentation/Basic%20Probability%20and%20Statistics.html#ignorability","title":"Ignorability","text":"<p>Basically we are saying that we control for everything possible to make sure our treatment assignment is not biased.</p> <ol> <li>The causal structure when treatment assignment mechanism is ignorable - i.e. not    arrow from X to T - no confounding</li> </ol> <pre><code>graph LR\n  T ---&gt; Y\n  X ---&gt; Y</code></pre> <ol> <li>Causal structure of X confounding the effect of T on Y. The confounding is depicted    as the red dashed line.</li> </ol> <pre><code>graph LR\n  T --&gt; Y\n  X --&gt; T\n  X --&gt; Y</code></pre>"},{"location":"Technical/Experimentation/Basic%20Probability%20and%20Statistics.html#positivity-aka-overlap","title":"Positivity (a.k.a Overlap)","text":"<p>Two ways to look at positivity a.k.a overlap:</p> <ol> <li>For all values of covariates \\(x\\) present in the population of subjects, \\(0 &lt; P(T=1|X=x) &lt; 1\\). That is the probability of treatment assignment for all values of covariate \\(x\\) should be non-zero.</li> <li>As overlap between the conditional distributions \\(P(X|T=0)\\) and \\(P(X|T=1)\\) is present.</li> </ol> <p>An example is:</p> <ul> <li>Data on previous promos were only applied to Cx that are more than 2 years since activation.</li> <li>This means you can\u2019t estimate reliably the outcome under T = 1 for Cx with &lt; 2 cohort age.</li> </ul>"},{"location":"Technical/Experimentation/Basic%20Probability%20and%20Statistics.html#the-positivity-unconfoundedness-tradeoff","title":"The Positivity - Unconfoundedness Tradeoff","text":"<p>Similar to curse of dimensionality, as you increase the number of covariates you condition on the degree of overlap decreases.</p> <p>Therefore even if you can guarantee ignorability by controling for many confounders, you may still have a problem with overlap or positivity.</p>"},{"location":"Technical/Experimentation/Outline.html","title":"Outline","text":"<p>Causal Inference</p> <ul> <li>Bayesian vs. Frequentist</li> </ul> <p>Basic Probability and Statistics</p> <ul> <li>Probability Theory</li> <li>Potential Outcomes Framework</li> <li>Graphical Models</li> <li>False Positives and False Negatives</li> <li>P-Values</li> <li>Power</li> <li>Confidence Interval</li> <li>CTR Variance</li> </ul> <p>Designing Experiments</p> <ul> <li>Why experiment?</li> <li>Landscape</li> <li>Product Development Cycle</li> <li>Measuring Impact</li> <li>What does the data look like</li> <li>How to randomize</li> <li>What could go wrong</li> <li>What to measure</li> <li>How to measure</li> <li>Compare</li> <li>Design and Monitor</li> <li>Interpret and Recommend Action</li> <li>A/A Test</li> </ul> <p>Overall Evaluation Criteria (OEC)</p> <p>Multiple Testing</p> <ul> <li>False Positives and Multiple Testing</li> <li>Omnibus test</li> </ul> <p>Tiered Metrics</p> <p>Central Limit Theorem and Handling Violated Assumptions</p> <ul> <li>When not to A/B Test</li> <li>What to do when you cannot A/B test</li> <li>How to check assumptions</li> <li>Bootstrap and Delta Method</li> <li>One-sided and Two Sided tests</li> <li>High Skew Remedies</li> <li>Small Samples</li> <li>Data Quality Checks</li> </ul> <p>Sequential Tests &amp; Variance Reduction</p> <ul> <li>Peeking and Sequential Testing</li> <li>Getting Results Faster</li> <li>Variance Reduction</li> <li>CUPED (vs Normal AB) and Crossover</li> <li>Fast Surrogates</li> <li>Pre-experiment Imbalance</li> </ul> <p>Multi-armed Bandits</p> <ul> <li>vs. AB</li> <li>vs. Contextual Bandits</li> </ul> <p>Marketplace and Network Effects</p>"},{"location":"Technical/Machine%20Learning/references.html","title":"References","text":""},{"location":"Technical/Machine%20Learning/references.html#some-references","title":"Some References","text":"<ol> <li> <p>He Wang, Math 7339 Machine Learning and Statistical Learning Theory 1 https://hewang.sites.northeastern.edu/math7243/\u00a0\u21a9</p> </li> <li> <p>He Wang, Math 7243 Machine Learning and Statistical Learning Theory 2 https://hewang.sites.northeastern.edu/math7339/\u00a0\u21a9</p> </li> <li> <p>Anand Avati, CS 229 Machine Learning - Summer Edition https://hewang.sites.northeastern.edu/math4570/\u00a0\u21a9</p> </li> </ol>"},{"location":"Technical/Math/Linear%20Algebra/Numpy%20and%20PyTorch%20Tensors%20Guide.html","title":"Numpy and PyTorch Tensors Guide","text":"<p>Two things we need to be able to do with data:</p> <ol> <li>acquire</li> <li>process</li> </ol> <p>Acquiring data also requires us to store it, and the most convenient tool we have at our disposal are <code>tensors</code> or <code>n-dimensional arrays</code>.</p>"},{"location":"Technical/Math/Linear%20Algebra/Numpy%20and%20PyTorch%20Tensors%20Guide.html#initializing","title":"Initializing","text":"<pre><code>import numpy as np\nimport torch\n\n# Tensor: multi-dimensional array of numerical values\n# k = 1 axes is a vector\nnp_vector = np.random.rand(12)\ntorch_vector = torch.rand(12, dtype=torch.float32)\nprint(np_vector.shape)\nprint(torch_vector.shape)\n# k = 2 axes is a matrix\nnp_matrix = np.random.rand(12,2)\ntorch_matrix = torch.rand((12,2), dtype=torch.float32)\nprint(np_matrix.shape)\nprint(torch_matrix.shape)\n# k &gt; 2 axes is a kth order tensor\nnp_tensor = np.random.rand(12,2,1)\ntorch_tensor = torch.rand((12,2,1), dtype=torch.float32)\nprint(np_tensor.shape)\nprint(torch_tensor.shape)\n</code></pre> <p>Other ways to initialize:</p> <pre><code>torch.ones((2,3,4))\ntorch.zeros((2,3,4))\ntorch.randn((2,3,4)) # sample from normal distribution vs. uniform in rand\n</code></pre>"},{"location":"Technical/Math/Linear%20Algebra/Numpy%20and%20PyTorch%20Tensors%20Guide.html#reshaping","title":"Reshaping","text":"<p>Change the shape of a tensor by either not changing:</p> <ol> <li>the number of elements</li> <li>the values of elements</li> </ol> <pre><code>np_tensor = np_tensor.reshape(6,4)\ntorch_tensor = torch_tensor.reshape(6,4)\n</code></pre> <p>Defining all dimensions is uneccessary - we only need to define n - 1 dims, the remaining is inferred.</p> <pre><code>np_tensor = np_tensor.reshape(6,-1)\ntorch_tensor = torch_tensor.reshape(6,-1)\n</code></pre>"},{"location":"Technical/Math/Linear%20Algebra/Numpy%20and%20PyTorch%20Tensors%20Guide.html#operations","title":"Operations","text":""},{"location":"Technical/Math/Linear%20Algebra/Numpy%20and%20PyTorch%20Tensors%20Guide.html#elementwise-operations","title":"Elementwise operations","text":"<p>Apply a standard scalar operation to each element of on array, or for two tensor inputs apply elementwise operations on each pair of elements.</p> <p>Thse include standard arithmetic operations (+, -, *, / and **).</p> <pre><code>x = torch.tensor([1.0, 2, 4, 8])\ny = torch.tensor([2, 2, 2, 2])\nx + y, x - y, x * y, x / y, x ** y\n</code></pre>"},{"location":"Technical/Math/Linear%20Algebra/Numpy%20and%20PyTorch%20Tensors%20Guide.html#hadamard-product-elementwise-multiplication-of-two-matrices","title":"Hadamard product (Elementwise multiplication of two matrices)","text":"<p>Specifically, elementwise multiplication of two matrices is called their Hadamard product (math notation \\(\\odot\\)). Consider matrix \\(\\mathbf{B} \\in \\mathbb{R}^{m \\times n}\\) whose element of row $i` and column \\(j\\) is \\(b_{ij}\\). The Hadamard product of matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\)</p> \\[    \\mathbf{A} \\odot \\mathbf{B} =    \\begin{bmatrix}        a_{11}  b_{11} &amp; a_{12}  b_{12} &amp; \\dots  &amp; a_{1n}  b_{1n} \\\\        a_{21}  b_{21} &amp; a_{22}  b_{22} &amp; \\dots  &amp; a_{2n}  b_{2n} \\\\        \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\        a_{m1}  b_{m1} &amp; a_{m2}  b_{m2} &amp; \\dots  &amp; a_{mn}  b_{mn}    \\end{bmatrix}. \\]"},{"location":"Technical/Math/Linear%20Algebra/Numpy%20and%20PyTorch%20Tensors%20Guide.html#linear-algebra-operations","title":"Linear Algebra Operations","text":""},{"location":"Technical/Math/Linear%20Algebra/Numpy%20and%20PyTorch%20Tensors%20Guide.html#transpose","title":"Transpose","text":"<pre><code>A = np.arange(20).reshape(5, 4)\nA\nA.T\n</code></pre> <p>As a special type of the square matrix, a symmetric matrix \\(\\mathbf{A}\\) is equal to its transpose: \\(\\mathbf{A} = \\mathbf{A}^\\top\\). Here we define a symmetric matrix <code>B</code>.</p> <pre><code>B = np.array([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\nB\nB == B.T\n</code></pre>"},{"location":"Technical/Math/Linear%20Algebra/Numpy%20and%20PyTorch%20Tensors%20Guide.html#vector-dot-products","title":"Vector Dot Products","text":"<p>Given two vectors \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d\\), their dot product \\(\\mathbf{x}^\\top \\mathbf{y}\\) (or \\(\\langle \\mathbf{x}, \\mathbf{y} \\rangle\\)) is a sum over the products of the elements at the same position: \\(\\mathbf{x}^\\top \\mathbf{y} = \\sum_{i=1}^{d} x_i y_i\\).</p> <pre><code>x = torch.arange(4, dtype=torch.float32)\ny = torch.ones(4, dtype = torch.float32)\nx, y, torch.dot(x, y)\n</code></pre> <p>Dot products are useful in a wide range of contexts. For example, given some set of values, denoted by a vector \\(\\mathbf{x} \\in \\mathbb{R}^d\\) and a set of weights denoted by \\(\\mathbf{w} \\in \\mathbb{R}^d\\), the weighted sum of the values in \\(\\mathbf{x}\\) according to the weights \\(\\mathbf{w}\\) could be expressed as the dot product \\(\\mathbf{x}^\\top \\mathbf{w}\\). When the weights are non-negative and sum to one (i.e., \\(\\left(\\sum_{i=1}^{d} {w_i} = 1\\right)\\)), the dot product expresses a weighted average. After normalizing two vectors to have the unit length, the dot products express the cosine of the angle between them. We will formally introduce this notion of length later in this section.</p>"},{"location":"Technical/Math/Linear%20Algebra/Numpy%20and%20PyTorch%20Tensors%20Guide.html#matrix-multiplications","title":"Matrix Multiplications","text":""},{"location":"Technical/Math/Linear%20Algebra/Numpy%20and%20PyTorch%20Tensors%20Guide.html#concatentation-and-stacking","title":"Concatentation and Stacking","text":"<p>Provide the list of tensors and axis to concatenate against.</p> <pre><code>X = torch.arange(12, dtype=torch.float32).reshape((3,4))\nY = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\ntorch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)\n</code></pre>"},{"location":"Technical/Math/Linear%20Algebra/Numpy%20and%20PyTorch%20Tensors%20Guide.html#summation","title":"Summation","text":"<p>Summing all the elements in the tensor yields a tensor with only one element. You can also sum along just a given axis.</p> <pre><code>X = torch.sum(X, 1) # sum along dim 1\nX = torch.sum() # sum all elements\n</code></pre>"},{"location":"Technical/Math/Linear%20Algebra/Numpy%20and%20PyTorch%20Tensors%20Guide.html#non-reduction-sum","title":"Non-Reduction Sum","text":"<p>However, sometimes it can be useful to keep the number of axes unchanged when invoking the function for calculating the sum or mean.</p> <pre><code>sum_A = A.sum(axis=1, keepdims=True)\nsum_A\nA / sum_A\n</code></pre>"},{"location":"Technical/Math/Linear%20Algebra/Numpy%20and%20PyTorch%20Tensors%20Guide.html#cumulative-sum","title":"Cumulative Sum","text":"<p>If we want to calculate the cumulative sum of elements of A along some axis, say axis=0 (row by row), we can call the cumsum function. This function will not reduce the input tensor along any axis.</p> <pre><code>A.cumsum(axis=0)\n</code></pre>"},{"location":"Technical/Math/Linear%20Algebra/Numpy%20and%20PyTorch%20Tensors%20Guide.html#logical-operations","title":"Logical Operations","text":"<p>Sometimes, we want to construct a binary tensor via logical statements. Take X == Y as an example. For each position, if X and Y are equal at that position, the corresponding entry in the new tensor takes a value of 1, meaning that the logical statement X == Y is true at that position; otherwise that position takes 0.</p> <pre><code>X == Y\n</code></pre>"},{"location":"Technical/Math/Linear%20Algebra/Numpy%20and%20PyTorch%20Tensors%20Guide.html#broadcasting","title":"Broadcasting","text":"<p>Under certain conditions, even when shapes differ, we can still perform elementwise operations by invoking the broadcasting mechanism. This mechanism works in the following way: First, expand one or both arrays by copying elements appropriately so that after this transformation, the two tensors have the same shape. Second, carry out the elementwise operations on the resulting arrays.</p> <pre><code>a = torch.arange(3).reshape((3, 1))\nb = torch.arange(2).reshape((1, 2))\na, b\n</code></pre> <p>Since <code>a</code> and <code>b</code> are \\(3\\times1\\) and \\(1\\times2\\) matrices respectively, their shapes do not match up if we want to add them. We broadcast the entries of both matrices into a larger \\(3\\times2\\) matrix as follows: for matrix <code>a</code> it replicates the columns and for matrix <code>b</code> it replicates the rows before adding up both elementwise.</p> <pre><code>a + b\n</code></pre>"},{"location":"Technical/Math/Linear%20Algebra/Numpy%20and%20PyTorch%20Tensors%20Guide.html#indexing-an-slicing","title":"Indexing an Slicing","text":"<p>Just as in any other Python array, elements in a tensor can be accessed by index. As in any Python array, the first element has index 0 and ranges are specified to include the first but before the last element. As in standard Python lists, we can access elements according to their relative position to the end of the list by using negative indices.</p> <p>Thus, [-1] selects the last element and [1:3] selects the second and the third elements as follows:</p> <pre><code>X[-1], X[1:3]\n</code></pre> <p>Beyond reading, we can also write elements of a matrix by specifying indices.</p> <pre><code>X[1, 2] = 9\nX\n\nX[0:2, :] = 12\nX\n</code></pre>"},{"location":"Technical/Math/Linear%20Algebra/Numpy%20and%20PyTorch%20Tensors%20Guide.html#saving-memory","title":"Saving Memory","text":"<p>Running operations can cause new memory to be allocated to host results. For example, if we write Y = X + Y, we will dereference the tensor that Y used to point to and instead point Y at the newly allocated memory. In the following example, we demonstrate this with Python\u2019s id() function, which gives us the exact address of the referenced object in memory. After running Y = Y + X, we will find that id(Y) points to a different location. That is because Python first evaluates Y + X, allocating new memory for the result and then makes Y point to this new location in memory.</p> <pre><code>before = id(Y)\nY = Y + X\nid(Y) == before\n</code></pre> <p>This might be undesirable for two reasons. First, we do not want to run around allocating memory unnecessarily all the time. In machine learning, we might have hundreds of megabytes of parameters and update all of them multiple times per second. Typically, we will want to perform these updates in place. Second, we might point at the same parameters from multiple variables. If we do not update in place, other references will still point to the old memory location, making it possible for parts of our code to inadvertently reference stale parameters.</p> <p>Fortunately, performing in-place operations is easy. We can assign the result of an operation to a previously allocated array with slice notation, e.g., Y[:] = . To illustrate this concept, we first create a new matrix Z with the same shape as another Y, using zeros_like to allocate a block of 0 entries. <pre><code>Z = torch.zeros_like(Y)\nprint('id(Z):', id(Z))\nZ[:] = X + Y\nprint('id(Z):', id(Z))\n</code></pre>"},{"location":"Technical/Math/Linear%20Algebra/basics.html","title":"Basics","text":"<p>TODO: Add notes here - leaving blank for now.</p>"},{"location":"Technical/Math/Linear%20Algebra/basics.html#vector-equations","title":"Vector Equations","text":"<p>Equation of linear combination of vectors with unknown coefficients. Essentially defines a linear system. For example, \\(\\cos(x) + \\sin(y) = 0\\) is NOT linear.</p> <p>There three ways to think of them:</p> <ul> <li>System of linear equations</li> </ul> \\[ \\begin{matrix} 2x_1 + 3x_2-3x_3=7\\\\ x_1-x_2-3x_3=5 \\end{matrix} \\] <ul> <li>Augmented Matrix: Representing the equation as a matrix and allowing elimination operations easily when done by hand.<ul> <li>Algorithms: Gaussian Elimination and Row Reduction.  </li> <li>To solve: All of them are methods to get augmented matrix in reduced row echelon form.  </li> <li>Reduced: Where all pivots are equal to 1, and are the only non-zero entry in the column.</li> </ul> </li> </ul> \\[ \\left[ \\begin{array}{ccc|c} 2 &amp; 3 &amp; -2 &amp; 7 \\\\ 1 &amp; -1 &amp; -3 &amp; 5 \\end{array} \\right] \\] <ul> <li>Vector Equation</li> </ul> \\[ x_1 \\begin{pmatrix} 2\\\\1 \\end{pmatrix} + x_2 \\begin{pmatrix} 3\\\\-1 \\end{pmatrix} + x_3 \\begin{pmatrix} -2\\\\-3 \\end{pmatrix} = \\begin{pmatrix} 7\\\\5 \\end{pmatrix} \\] <p>A system of equation is consistent if it has a solution. If there are no solutions, it is inconsistent.</p>"},{"location":"Technical/Math/Linear%20Algebra/basics.html#great-references","title":"Great References","text":"<ol> <li> <p>He Wang, Math 2331 Linear Algebra, Department of Mathematics Northeastern University. https://hewang.sites.northeastern.edu/math2331/\u00a0\u21a9</p> </li> <li> <p>He Wang, Math 4570 Matrix Methods in Data Analysis and Machine Learning https://hewang.sites.northeastern.edu/math4570/\u00a0\u21a9</p> </li> </ol>"},{"location":"Technical/Math/Linear%20Algebra/matrix%20decomposition.html","title":"Matrix Decomposition Methods","text":""},{"location":"Technical/Math/Linear%20Algebra/matrix%20decomposition.html#basics","title":"Basics","text":""},{"location":"Technical/Math/Linear%20Algebra/matrix%20decomposition.html#what-are-eigenvalues-and-eigenvectors","title":"What are Eigenvalues and Eigenvectors?","text":"<p>For a given square matrix \\(A\\), an eigenvector is a non-zero vector \\(\\mathbf{v}\\) that, when multiplied by \\(A\\), yields a scalar multiple of itself. This scalar is known as an eigenvalue. Mathematically, this relationship is defined as:</p> \\[ A\\mathbf{v} = \\lambda\\mathbf{v} \\] <p>where: - \\(A\\) is a square matrix, - \\(\\mathbf{v}\\) is an eigenvector of \\(A\\), - \\(\\lambda\\) is the corresponding eigenvalue of \\(\\mathbf{v}\\).</p>"},{"location":"Technical/Math/Linear%20Algebra/matrix%20decomposition.html#intuitive-understanding","title":"Intuitive Understanding","text":"<p>Intuitively, eigenvectors point in a direction that is unchanged by the application of \\(A\\), while eigenvalues scale the eigenvector in that direction. This concept is crucial in understanding transformations represented by \\(A\\).</p>"},{"location":"Technical/Math/Linear%20Algebra/matrix%20decomposition.html#common-properties","title":"Common Properties","text":"<ul> <li>Characteristic Equation: The eigenvalues of \\(A\\) can be found by solving the characteristic equation \\(\\det(A - \\lambda I) = 0\\), where \\(I\\) is the identity matrix of the same size as \\(A\\).</li> <li>Multiplicity: An eigenvalue's multiplicity is the number of times it is a root of the characteristic equation. It can have more than one corresponding eigenvector.</li> </ul>"},{"location":"Technical/Math/Linear%20Algebra/matrix%20decomposition.html#diagonalization","title":"Diagonalization","text":"<p>A matrix \\(A\\) can be diagonalized if there exists an invertible matrix \\(P\\) and a diagonal matrix \\(D\\) such that:</p> \\[ A = PDP^{-1} \\] <p>Here, \\(D\\) contains the eigenvalues of \\(A\\), and the columns of \\(P\\) are the corresponding eigenvectors. For \\(A\\) to be diagonalizable, it must have \\(n\\) linearly independent eigenvectors.</p>"},{"location":"Technical/Math/Linear%20Algebra/matrix%20decomposition.html#eigenvalues-and-eigenvectors-of-symmetric-matrices","title":"Eigenvalues and Eigenvectors of Symmetric Matrices","text":"<p>Symmetric matrices (\\(A = A^T\\)) have several special properties:</p> <ul> <li>All eigenvalues are real.</li> <li>Eigenvectors corresponding to different eigenvalues are orthogonal.</li> <li>The matrix can be diagonalized using an orthogonal matrix \\(Q\\) (i.e., \\(A = Q\\Lambda Q^T\\)), where \\(\\Lambda\\) is a diagonal matrix of eigenvalues and \\(Q\\) is an orthogonal matrix of eigenvectors.</li> </ul>"},{"location":"Technical/Math/Linear%20Algebra/matrix%20decomposition.html#applications","title":"Applications","text":"<ul> <li>Rotation Matrix: Describes rotation in a space, with eigenvectors indicating invariant directions of rotation.</li> <li>Scaling Matrix: Diagonal elements (eigenvalues) represent scaling factors along principal axes.</li> <li>Inverse and Easy Inversion: If \\(A\\) is symmetric and diagonalizable, its inverse (if it exists) is easily computed via its eigen decomposition.</li> </ul>"},{"location":"Technical/Math/Linear%20Algebra/matrix%20decomposition.html#checking-positive-definiteness","title":"Checking Positive Definiteness","text":"<p>A symmetric matrix is positive definite if all its eigenvalues are positive. This property is crucial for optimizing quadratic forms and ensuring the existence of unique solutions in various problems.</p>"},{"location":"Technical/Math/Linear%20Algebra/matrix%20decomposition.html#singular-value-decomposition-svd","title":"Singular Value Decomposition (SVD)","text":"<p>SVD decomposes a matrix \\(A\\) into three matrices:</p> \\[ A = U\\Sigma V^T \\] <ul> <li>\\(U\\) and \\(V\\) are orthogonal matrices containing the left and right singular vectors of \\(A\\).</li> <li>\\(\\Sigma\\) is a diagonal matrix containing the singular values of \\(A\\).</li> </ul> <p>SVD is applicable to any \\(m \\times n\\) matrix and is particularly useful in solving problems that involve non-square matrices.</p>"},{"location":"Technical/Math/Linear%20Algebra/matrix%20decomposition.html#connection-between-svd-and-evd","title":"Connection Between SVD and EVD","text":"<ul> <li>For square matrices, EVD focuses on decomposing a matrix into its eigenvectors and eigenvalues, while SVD generalizes this concept to any matrix through singular values and vectors.</li> <li>If a matrix \\(A\\) is symmetric, its SVD and EVD coincide, with singular values corresponding to the absolute values of the eigenvalues of \\(A\\).</li> </ul>"},{"location":"Technical/Math/Linear%20Algebra/matrix%20decomposition.html#other-decomposition-methods","title":"Other Decomposition Methods","text":""},{"location":"Technical/Math/Linear%20Algebra/matrix%20decomposition.html#lu-factorization","title":"LU Factorization","text":"<p>LU Factorization decomposes a matrix \\(A\\) into the product of a lower triangular matrix \\(L\\) and an upper triangular matrix \\(U\\):</p> \\[ A = LU \\] <p>This decomposition is useful for solving linear systems, calculating determinants, and inverting matrices. The process involves Gaussian elimination.</p>"},{"location":"Technical/Math/Linear%20Algebra/matrix%20decomposition.html#qr-decomposition","title":"QR Decomposition","text":"<p>QR Decomposition decomposes a matrix \\(A\\) into the product of an orthogonal matrix \\(Q\\) and an upper triangular matrix \\(R\\):</p> \\[ A = QR \\] <p>QR Decomposition is widely used in solving linear least squares problems and for eigenvalue computations in the QR algorithm.</p>"},{"location":"Technical/Math/Linear%20Algebra/matrix%20decomposition.html#cholesky-decomposition","title":"Cholesky Decomposition","text":"<p>For a positive definite symmetric matrix \\(A\\), Cholesky Decomposition is a factorization into the product of a lower triangular matrix \\(L\\) and its transpose:</p> \\[ A = LL^T \\] <p>This method is more efficient than LU decomposition for solving systems of equations when \\(A\\) is symmetric and positive definite.</p>"},{"location":"Technical/Math/Linear%20Algebra/matrix%20decomposition.html#application-to-multivariate-normal-distribution-sampling","title":"Application to Multivariate Normal Distribution Sampling","text":"<p>Cholesky Decomposition can be applied to sample from a Multivariate Normal distribution. Given a covariance matrix \\(\\Sigma\\), which is symmetric and positive definite, we can decompose \\(\\Sigma\\) as \\(\\Sigma = LL^T\\) using Cholesky Decomposition.</p> <p>To sample a vector \\(\\mathbf{x}\\) from a Multivariate Normal distribution with mean \\(\\mu\\) and covariance \\(\\Sigma\\), we can first sample a vector \\(\\mathbf{z}\\) from a standard Multivariate Normal distribution (mean \\(\\mathbf{0}\\) and covariance \\(I\\)), and then transform \\(\\mathbf{z}\\) using the Cholesky factor \\(L\\):</p> \\[ \\mathbf{x} = \\mu + L\\mathbf{z} \\] <p>This method leverages the property that linear transformations of normally distributed variables are also normally distributed, with the transformation defining the new mean and covariance.</p>"},{"location":"Technical/Math/Probability/map%20of%20univariate%20distributions.html","title":"Map of Univariate Distributions","text":"Univariate Disribution Relationships (Source Link)"},{"location":"Technical/Object%20Oriented%20Programming/abstract%20base%20class.html","title":"Abstract Base Class","text":"<p>A formal mechanism to designate the relationships between different implementations of the same abstractionis through the definition of one class that serves as an abstract base class, via inheritance, for one or more concrete classes.</p> <p>The abstract base class may not have any concrete methods implemented, but still define expectations of methods that should exit as abstract methods. The subclasses that inherit the abstract base class are responsible for overriding abstract methods to provide a working implementation for each behaviour. </p>"},{"location":"Technical/Object%20Oriented%20Programming/adapter_pattern.html","title":"Adapter pattern","text":"<pre><code>class Empty(Exception):\n    \"\"\"Error attempting to access an element from an empty container.\"\"\"\n    pass\n\nclass ArrayStack:\n    \"\"\"LIFO stack implementation using Python list as underlying storage\"\"\"\n\n    def __init__(self):\n        self._data = []\n\n    def __len__(self):\n        return len(self._data)\n\n    def is_empty(self):\n        return len(self._data) == 0\n\n    def push(self, e):\n        self._data.append(e)\n\n    def top(self):\n        if self.is_empty():\n            raise Empty('Stack is Empty')\n        return self._data[-1]\n\n    def pop(self):\n        if self.is_empty():\n            raise Empty('Stack is Empty')\n        return self._data.pop()\n</code></pre>"},{"location":"Technical/Object%20Oriented%20Programming/adapter_pattern.html#adapter-pattern","title":"Adapter Pattern","text":"<p>The adapter design pattern applies to any context where we effectively want to modify an existing class so that its methods match those a related, but different, class or interface. </p> <p>One general way to apply the adapter pattern is to define a new class in such a way that it contains an instance of the existing class as a hidden field, and then to implement each method of the new class using methods of this hidden instance variable. </p> <p>By applying the adapter pattern in this way, we have created a new class that performs some of the same functions as an existing class, but repacked in a more convenient way. </p> <p>An example is to implement an Array based Stack using a Python <code>list</code> - with a simple example shown below.</p>"},{"location":"Technical/Object%20Oriented%20Programming/composition%20pattern.html","title":"Composition Pattern","text":"<p>Define an object that is composed of two or more other objects.</p> <p>TODO: Add notes here - leaving blank for now.</p>"},{"location":"Technical/Object%20Oriented%20Programming/factory.html","title":"Factory","text":"<p>TODO: Add notes here - leaving blank for now.</p>"},{"location":"Technical/Object%20Oriented%20Programming/goals.html","title":"Goals and Principles","text":"<p>TODO: Add notes here - leaving blank for now.</p>"},{"location":"Technical/Object%20Oriented%20Programming/goals.html#design","title":"Design","text":""},{"location":"Technical/Object%20Oriented%20Programming/goals.html#design-patterns","title":"Design Patterns","text":"<ol> <li>Iterator</li> <li>Adapter</li> <li>Position</li> <li>Composition</li> <li>Template Method</li> <li>Locator</li> <li>Factory Method</li> </ol>"},{"location":"Technical/Object%20Oriented%20Programming/inheritance.html","title":"Inheritance","text":"<p>TODO: Add notes here - leaving blank for now.</p>"},{"location":"Technical/Object%20Oriented%20Programming/interface.html","title":"Interface","text":"<p>TODO: Add notes here - leaving blank for now.</p>"},{"location":"Technical/Object%20Oriented%20Programming/observer.html","title":"Observer","text":"<p>TODO: Add notes here - leaving blank for now.</p>"},{"location":"Technical/Object%20Oriented%20Programming/template%20method%20pattern.html","title":"Template Method","text":"<p>The template method pattern is a generic computation mechanism that can be specialized for a particular application by redefining certain steps. </p> <p>To allow customization, the primary algorithm calls auxiliary functions known as hooks at designated steps of the process. These hooks can be overridden for specialized use cases.</p> <p>An example is the Euler Tour Tree Traverl</p>"},{"location":"Technical/Tools/React/current%20build%20stack.html","title":"My Go To Stack for Projects","text":"<p>Whenever I have an idea this is my go to stack to get started.</p> <p>Front End: - UI Library: React - Animations: React-Spring - CSS: TailwindCSS - Component Library: Shadcn</p> <p>Backend: - Logic: NodeJS + ExpressJS - Auth: Supabase Auth - Database: Postgres</p> <p>AI: - LangChain.js - LlamaCPP - WebAssembly</p> <p>General: - Typescript - Type Library: Zod - Runtime: NodeJS - NextJS</p>"},{"location":"Technical/Tools/React/current%20build%20stack.html#key-features-of-nextjs-to-use","title":"Key Features of NextJS to use","text":"<ul> <li>Loading templates</li> <li>Route Caching</li> </ul>"},{"location":"Technical/Tools/Vim/useful%20commands.html","title":"Useful Vim Commands","text":"<ul> <li><code>gg</code>              : Move to the first line of the file</li> <li><code>G</code>               : Move to the last line</li> <li><code>zc</code>              : Close current fold</li> <li><code>zo</code>              : Open current fold</li> <li><code>za</code>              : Toggle current fold</li> <li><code>zi</code>              : Toggle folding entirely</li> <li><code>vab</code>             : Select content within parenthesis () along with itself</li> <li><code>va{</code>             : Select content within curly brackets {} along with itself etc. (you can replace the character with any bracket type)</li> <li><code>vib</code>             : Select content inside parenthesis () only</li> <li><code>vi{</code>             : Select content inside curly brackets {} only etc.</li> <li><code>%</code>               : Jump between matching brackets, tags etc.</li> <li><code>vit</code>             : Select content between tags</li> <li><code>cit</code>             : Jump into the tag</li> <li><code>D</code>               : Deletes from cursor position to the end of the line</li> <li><code>Y</code>               : Copies till the end of the line</li> <li><code>gg=G</code>            : Reindent the whole file</li> <li><code>gv</code>              : Reselect the last visual selection</li> <li><code>`&lt;</code>          : Jump to the beginning of the last visual selection</li> <li><code>`&gt;</code>          : Jump to the end of the last visual selection</li> <li><code>^</code>               : Move to the first non-blank character of the line</li> <li><code>g_</code>              : Move to the last non-blank character of the line (but you remove trailing whitespace, right)</li> <li><code>g_lD</code>            : Delete all the trailing whitespace on the line</li> <li><code>ea</code>              : Append to the end of the current word</li> <li><code>gf</code>              : Jump to the file name under the cursor</li> <li><code>xp</code>              : Swap character forward</li> <li><code>Xp</code>              : Swap character backward</li> <li><code>yyp</code>             : Duplicate the current line</li> <li><code>yapP</code>            : Duplicate the current paragraph</li> <li><code>dat</code>             : Delete around an HTML tag, including the tag</li> <li><code>dit</code>             : Delete inside an HTML tag, excluding the tag</li> <li><code>w</code>               : Move one word to the right</li> <li><code>b</code>               : Move one word to the left</li> <li><code>dd</code>              : Delete the current line</li> <li><code>&lt;&lt;</code>              : Outdent current line</li> <li><code>&gt;&gt;</code>              : Indent current line</li> <li><code>z=</code>              : Show spelling corrections</li> <li><code>zg</code>              : Add to spelling dictionary</li> <li><code>zw</code>              : Remove from spelling dictionary</li> <li><code>~</code>               : Toggle the case of the current character</li> <li><code>gUw</code>             : Uppercase until the end of the word (u for lower, ~ to toggle)</li> <li><code>gUiw</code>            : Uppercase entire word (u for lower, ~ to toggle)</li> <li><code>gUU</code>             : Uppercase entire line</li> <li><code>gu$</code>             : Lowercase until the end of the line</li> <li><code>da\"</code>             : Delete the next double-quoted string</li> <li><code>+</code>               : Move to the first non-whitespace character of the next line</li> <li><code>S</code>               : Delete current line and go into insert mode</li> <li><code>I</code>               : Insert at the beginning of the line</li> <li><code>ci\"</code>             : Change what\u2019s inside the next double-quoted string</li> <li><code>ca{</code>             : Change inside the curly braces (try [, (, etc.)</li> <li><code>vaw</code>             : Visually select word</li> <li><code>dap</code>             : Delete the whole paragraph</li> <li><code>r</code>               : Replace a character</li> <li><code>`[</code>          : Jump to the beginning of the last yanked text</li> <li><code>`]</code>          : Jump to the end of the last yanked text</li> <li><code>g;</code>              : Jump to the last change you made</li> <li><code>g,</code>              : Jump back forward through the change list</li> <li><code>&amp;</code>               : Repeat the last substitution on the current line</li> <li><code>g&amp;</code>              : Repeat the last substitution on all lines</li> <li><code>ZZ</code>              : Save the current file and close it</li> </ul>"}]}